#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»“æ„åŒ–GitHub Trendingçˆ¬è™«
æŒ‰æ—¶é—´ç»´åº¦åˆ†ç±»çˆ¬å–ï¼Œå‚è€ƒarXivæ¶æ„å®ç°ç»“æ„åŒ–å­˜å‚¨
"""

import sys
import os
import time
import json
import hashlib
import asyncio
import aiohttp
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set
from bs4 import BeautifulSoup

# å¯¼å…¥é…ç½®
from github_config import GITHUB_CONFIG, get_api_headers, get_trending_url, get_repo_api_url

# æ·»åŠ å…±äº«æ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'shared'))
from utils import save_json, save_markdown, clean_text


class StructuredGitHubSpider:
    """ç»“æ„åŒ–GitHubçˆ¬è™«ï¼Œæ”¯æŒæ—¶é—´ç»´åº¦åˆ†ç±»å’ŒarXivå¼å­˜å‚¨"""
    
    def __init__(self, base_output_dir: str = "crawled_data"):
        self.config = GITHUB_CONFIG
        self.api_headers = get_api_headers()
        self.base_output_dir = Path(base_output_dir)
        
        # åˆ›å»ºåŸºç¡€ç›®å½•ç»“æ„
        self._setup_directory_structure()
        
        # å»é‡é›†åˆ - è·¨æ—¶é—´ç»´åº¦å»é‡
        self.processed_repos: Set[str] = set()
        self.load_processed_repos()
        
        # è¯·æ±‚ç»Ÿè®¡
        self.api_requests_count = 0
        self.web_requests_count = 0
        self.start_time = time.time()
        
        print("ğŸš€ ç»“æ„åŒ–GitHubçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ è¾“å‡ºç›®å½•: {self.base_output_dir}")
        print(f"   ğŸ”„ å·²å¤„ç†é¡¹ç›®: {len(self.processed_repos)} ä¸ª")
    
    def _setup_directory_structure(self):
        """åˆ›å»ºç›®å½•ç»“æ„ï¼Œå‚è€ƒarXivæ¶æ„"""
        directories = [
            # ä¸»æ•°æ®ç›®å½•
            self.base_output_dir,
            
            # æŒ‰æ—¶é—´ç»´åº¦åˆ†ç±»çš„å·¥å…·ç›®å½•
            self.base_output_dir / "tools" / "daily",
            self.base_output_dir / "tools" / "weekly", 
            self.base_output_dir / "tools" / "monthly",
            
            # èšåˆæ•°æ®ç›®å½•
            self.base_output_dir / "data" / "daily",
            self.base_output_dir / "data" / "weekly",
            self.base_output_dir / "data" / "monthly",
            
            # æ’è¡Œæ¦œç›®å½•
            self.base_output_dir / "rankings" / "daily",
            self.base_output_dir / "rankings" / "weekly", 
            self.base_output_dir / "rankings" / "monthly",
            
            # å»é‡è®°å½•ç›®å½•
            self.base_output_dir / "metadata"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
        
        print("ğŸ“ ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")
    
    def load_processed_repos(self):
        """åŠ è½½å·²å¤„ç†çš„ä»“åº“è®°å½•"""
        processed_file = self.base_output_dir / "metadata" / "processed_repos.json"
        
        if processed_file.exists():
            try:
                with open(processed_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_repos = set(data.get('processed_repos', []))
                print(f"ğŸ“‹ åŠ è½½å·²å¤„ç†ä»“åº“: {len(self.processed_repos)} ä¸ª")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½å·²å¤„ç†ä»“åº“å¤±è´¥: {e}")
                self.processed_repos = set()
        else:
            self.processed_repos = set()
    
    def save_processed_repos(self):
        """ä¿å­˜å·²å¤„ç†çš„ä»“åº“è®°å½•"""
        processed_file = self.base_output_dir / "metadata" / "processed_repos.json"
        
        data = {
            'processed_repos': list(self.processed_repos),
            'last_updated': datetime.now().isoformat(),
            'total_count': len(self.processed_repos)
        }
        
        try:
            with open(processed_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å·²å¤„ç†ä»“åº“å¤±è´¥: {e}")
    
    async def crawl_all_time_ranges(self, languages: List[str] = None) -> Dict[str, List[Dict]]:
        """çˆ¬å–æ‰€æœ‰æ—¶é—´ç»´åº¦çš„Trendingæ•°æ®"""
        # åªçˆ¬å–æ‰€æœ‰è¯­è¨€ï¼ˆä¸æŒ‰è¯­è¨€åˆ†ç±»ï¼‰
        languages = [None]  # Noneè¡¨ç¤ºæ‰€æœ‰è¯­è¨€
        
        time_ranges = ["daily", "weekly", "monthly"]
        all_results = {}
        
        for time_range in time_ranges:
            print(f"\nğŸ¯ å¼€å§‹çˆ¬å– {time_range} trending (æ‰€æœ‰è¯­è¨€)...")
            time_results = []
            
            try:
                # çˆ¬å–æ‰€æœ‰è¯­è¨€çš„trendingä»“åº“
                repos = await self.crawl_trending_repos(None, time_range)
                
                # è¿‡æ»¤å’Œå¤„ç†
                processed_repos = await self.process_and_filter_repos(repos, time_range)
                
                time_results.extend(processed_repos)
                
                print(f"  âœ… è·å¾—AIå·¥å…·: {len(processed_repos)} ä¸ª")
                
            except Exception as e:
                print(f"  âŒ {time_range} çˆ¬å–å¤±è´¥: {e}")
                import traceback
                print(f"  ğŸ“‹ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                continue
            
            # ä¿å­˜è¯¥æ—¶é—´ç»´åº¦çš„ç»“æœ
            all_results[time_range] = time_results
            await self.save_time_range_results(time_results, time_range)
            
            print(f"ğŸ‰ {time_range} çˆ¬å–å®Œæˆ: {len(time_results)} ä¸ªAIå·¥å…·")
            
            # æ—¶é—´ç»´åº¦é—´å»¶è¿Ÿ
            if time_range != time_ranges[-1]:
                print("â³ ç­‰å¾…5ç§’åç»§ç»­ä¸‹ä¸€ä¸ªæ—¶é—´ç»´åº¦...")
                await asyncio.sleep(5)
        
        # ç”Ÿæˆè·¨æ—¶é—´ç»´åº¦çš„æ±‡æ€»æŠ¥å‘Š
        await self.generate_comprehensive_report(all_results)
        
        # ä¿å­˜å»é‡è®°å½•
        self.save_processed_repos()
        
        return all_results
    
    async def crawl_trending_repos(self, language: str = None, since: str = "daily") -> List[Dict]:
        """çˆ¬å–å•ä¸ªæ—¶é—´ç»´åº¦çš„trendingä»“åº“"""
        # è·å–trendingé¡µé¢æ•°æ®
        trending_repos = await self._get_trending_from_web(language, since)
        
        # ä½¿ç”¨APIè·å–è¯¦ç»†ä¿¡æ¯
        detailed_repos = []
        for repo in trending_repos:
            try:
                # è·å–APIè¯¦ç»†ä¿¡æ¯
                api_data = await self._get_repo_details_from_api(repo)
                
                if api_data:
                    # åˆå¹¶æ•°æ®
                    enhanced_repo = {**repo, **api_data}
                    enhanced_repo['time_range'] = since
                    enhanced_repo['language_filter'] = language
                    detailed_repos.append(enhanced_repo)
                
                # é¢‘ç‡æ§åˆ¶
                await asyncio.sleep(self.config['crawl_config']['request_delay'])
                
            except Exception as e:
                print(f"  âŒ å¤„ç†ä»“åº“å¤±è´¥: {e}")
                continue
        
        return detailed_repos
    
    async def process_and_filter_repos(self, repos: List[Dict], time_range: str) -> List[Dict]:
        """å¤„ç†å’Œè¿‡æ»¤ä»“åº“æ•°æ®"""
        processed_repos = []
        
        for i, repo in enumerate(repos):
            try:
                # æ˜¾ç¤ºå¤„ç†è¿›åº¦
                if i % 10 == 0:
                    print(f"    ğŸ”„ å¤„ç†è¿›åº¦: {i}/{len(repos)}")
                
                # åŸºç¡€æ•°æ®éªŒè¯
                if not repo or not isinstance(repo, dict):
                    print(f"    âš ï¸ æ— æ•ˆä»“åº“æ•°æ®: {type(repo)}")
                    continue
                
                repo_name = repo.get('name', 'Unknown')
                if not repo_name or repo_name == 'Unknown':
                    print(f"    âš ï¸ ç¼ºå°‘ä»“åº“åç§°: {repo}")
                    continue
                
                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
                repo_id = self._generate_repo_id(repo)
                if not repo_id:
                    print(f"    âš ï¸ æ— æ³•ç”Ÿæˆä»“åº“ID: {repo_name}")
                    continue
                
                # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†ï¼ˆå»é‡ï¼‰
                if repo_id in self.processed_repos:
                    print(f"    â© è·³è¿‡é‡å¤é¡¹ç›®: {repo_name}")
                    continue
                
                # AIç›¸å…³æ€§æ£€æŸ¥
                if not self._is_ai_related(repo):
                    continue
                
                # è´¨é‡è¯„ä¼°
                try:
                    repo['quality_score'] = self._calculate_quality_score(repo)
                except Exception as e:
                    print(f"    âš ï¸ è´¨é‡è¯„åˆ†å¤±è´¥: {repo_name} - {e}")
                    repo['quality_score'] = 0
                
                # æ·»åŠ å¤„ç†ä¿¡æ¯
                repo['repo_id'] = repo_id
                repo['crawl_timestamp'] = datetime.now().isoformat()
                repo['time_range'] = time_range
                
                # ä¸ºè¯¥å·¥å…·åˆ›å»ºå•ç‹¬çš„å­˜å‚¨ç›®å½•
                try:
                    await self._create_individual_tool_directory(repo, time_range)
                except Exception as e:
                    print(f"    âš ï¸ åˆ›å»ºç›®å½•å¤±è´¥: {repo_name} - {e}")
                    continue
                
                processed_repos.append(repo)
                
                # æ ‡è®°ä¸ºå·²å¤„ç†
                self.processed_repos.add(repo_id)
                
                print(f"    âœ… æ–°AIå·¥å…·: {repo_name} (è´¨é‡åˆ†: {repo['quality_score']:.1f})")
                
            except Exception as e:
                print(f"    âŒ å¤„ç†ä»“åº“å¼‚å¸¸: {repo.get('name', 'Unknown')} - {e}")
                import traceback
                print(f"    ğŸ“‹ å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
                continue
        
        print(f"  ğŸ“Š å¤„ç†å®Œæˆ: {len(processed_repos)}/{len(repos)} ä¸ªæœ‰æ•ˆAIå·¥å…·")
        return processed_repos
    
    def _generate_repo_id(self, repo: Dict) -> str:
        """ç”Ÿæˆä»“åº“çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
        try:
            # ä½¿ç”¨ä»“åº“å…¨åä½œä¸ºå”¯ä¸€æ ‡è¯†
            full_name = repo.get('full_name') or repo.get('name', '')
            
            if not full_name:
                # å¦‚æœæ²¡æœ‰å…¨åï¼Œä½¿ç”¨URLç”Ÿæˆ
                url = repo.get('url', '')
                if url:
                    # ä»URLæå–ä»“åº“ä¿¡æ¯
                    url_clean = url.replace('https://github.com/', '').replace('http://github.com/', '')
                    parts = url_clean.split('/')
                    if len(parts) >= 2:
                        full_name = f"{parts[0]}/{parts[1]}"
            
            if not full_name:
                # æœ€åå°è¯•ä»nameå­—æ®µæå–
                name = repo.get('name', '')
                if '/' in name:
                    full_name = name
                else:
                    # ä½¿ç”¨ownerå’Œrepo_nameç»„åˆ
                    owner = repo.get('owner', '')
                    repo_name = repo.get('repo_name', '')
                    if owner and repo_name:
                        full_name = f"{owner}/{repo_name}"
            
            if not full_name:
                return ""
            
            # æ ‡å‡†åŒ–å¤„ç†
            repo_id = full_name.lower().replace('/', '_').replace('-', '_').replace(' ', '_')
            
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            import re
            repo_id = re.sub(r'[^\w_]', '', repo_id)
            
            return repo_id
            
        except Exception as e:
            print(f"    âš ï¸ ç”Ÿæˆä»“åº“IDå¼‚å¸¸: {e}")
            return ""
    
    async def _create_individual_tool_directory(self, repo: Dict, time_range: str):
        """ä¸ºæ¯ä¸ªå·¥å…·åˆ›å»ºå•ç‹¬çš„å­˜å‚¨ç›®å½•ï¼Œå‚è€ƒarXivæ¶æ„"""
        try:
            repo_id = repo.get('repo_id', 'unknown')
            repo_name = repo.get('name', 'unknown')
            
            # å®‰å…¨å¤„ç†ä»“åº“åç§°
            safe_repo_name = repo_name.replace('/', '_').replace('\\', '_').replace(':', '_')
            safe_repo_name = ''.join(c for c in safe_repo_name if c.isalnum() or c in '_-')[:50]  # é™åˆ¶é•¿åº¦
            
            # åˆ›å»ºç›®å½•åï¼šrepo_id_ç®€åŒ–åç§°
            dir_name = f"{repo_id}_{safe_repo_name}"
            tool_dir = self.base_output_dir / "tools" / time_range / dir_name
            
            # ç¡®ä¿ç›®å½•åˆ›å»ºæˆåŠŸ
            try:
                tool_dir.mkdir(parents=True, exist_ok=True)
            except Exception as e:
                print(f"      âŒ åˆ›å»ºç›®å½•å¤±è´¥: {dir_name} - {e}")
                return
            
            # ç”Ÿæˆcontent.mdæ–‡ä»¶
            try:
                content_md = self._generate_tool_content_md(repo)
                content_file = tool_dir / "content.md"
                
                with open(content_file, 'w', encoding='utf-8') as f:
                    f.write(content_md)
            except Exception as e:
                print(f"      âš ï¸ ç”Ÿæˆcontent.mdå¤±è´¥: {dir_name} - {e}")
            
            # ç”Ÿæˆmetadata.jsonæ–‡ä»¶
            try:
                metadata = self._extract_metadata(repo)
                metadata_file = tool_dir / "metadata.json"
                
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"      âš ï¸ ç”Ÿæˆmetadata.jsonå¤±è´¥: {dir_name} - {e}")
            
            print(f"      ğŸ“ åˆ›å»ºå·¥å…·ç›®å½•: {dir_name}")
            
        except Exception as e:
            print(f"      âŒ åˆ›å»ºå·¥å…·ç›®å½•å¼‚å¸¸: {e}")
            import traceback
            print(f"      ğŸ“‹ å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
    
    def _generate_tool_content_md(self, repo: Dict) -> str:
        """ç”Ÿæˆå·¥å…·çš„Markdownå†…å®¹ï¼Œå‚è€ƒarXivæ ¼å¼"""
        name = repo.get('name', 'Unknown Tool')
        description = repo.get('description', 'æš‚æ— æè¿°')
        url = repo.get('url', '#')
        stars = repo.get('stars', 0)
        forks = repo.get('forks', 0)
        language = repo.get('language', 'Unknown')
        license_name = repo.get('license', 'Unknown')
        topics = repo.get('topics', [])
        created_at = repo.get('created_at', '')
        updated_at = repo.get('updated_at', '')
        quality_score = repo.get('quality_score', 0)
        time_range = repo.get('time_range', 'daily')
        readme_content = repo.get('readme_content', '')
        
        # å¤„ç†READMEå†…å®¹
        readme_section = 'æš‚æ— READMEæ–‡ä»¶'
        if readme_content:
            # æ¸…ç†å’Œæ ¼å¼åŒ–READMEå†…å®¹
            readme_lines = readme_content.strip().split('\n')
            formatted_readme = []
            
            for line in readme_lines:
                # ç§»é™¤è¿‡å¤šçš„ç©ºè¡Œ
                if line.strip() or (formatted_readme and formatted_readme[-1].strip()):
                    formatted_readme.append(line)
            
            if formatted_readme:
                readme_section = '\n'.join(formatted_readme)
            else:
                readme_section = 'æ— æœ‰æ•ˆREADMEå†…å®¹'
        
        # æ ¼å¼åŒ–æ—¶é—´
        created_date = ''
        updated_date = ''
        try:
            if created_at:
                from dateutil import parser
                dt = parser.parse(created_at)
                created_date = dt.strftime('%Y-%m-%d')
        except Exception as e:
            print(f"    âš ï¸ åˆ›å»ºæ—¶é—´è§£æå¤±è´¥: {e}")
            created_date = 'æœªçŸ¥'
        
        try:
            if updated_at:
                from dateutil import parser
                dt = parser.parse(updated_at)
                updated_date = dt.strftime('%Y-%m-%d')
        except Exception as e:
            print(f"    âš ï¸ æ›´æ–°æ—¶é—´è§£æå¤±è´¥: {e}")
            updated_date = 'æœªçŸ¥'
        
        content = f"""# {name}

## åŸºæœ¬ä¿¡æ¯
- **é¡¹ç›®åç§°**: {name}
- **GitHub ID**: {repo.get('repo_id', 'unknown')}
- **åˆ›å»ºæ—¥æœŸ**: {created_date}
- **æœ€åæ›´æ–°**: {updated_date}
- **ä¸»è¦è¯­è¨€**: {language}
- **å¼€æºè®¸å¯**: {license_name}
- **è¶‹åŠ¿èŒƒå›´**: {time_range}

## é“¾æ¥
- **GitHubé“¾æ¥**: {url}
- **Starsæ•°é‡**: {stars:,}
- **Forksæ•°é‡**: {forks:,}

## é¡¹ç›®æè¿°
{description}

## ğŸ“ README è¯¦æƒ…
{readme_section}

## æŠ€æœ¯æ ‡ç­¾
{', '.join(topics) if topics else 'æš‚æ— æ ‡ç­¾'}

## é¡¹ç›®ç»Ÿè®¡
- **â­ Stars**: {stars:,}
- **ğŸ´ Forks**: {forks:,}
- **ğŸ‘ï¸ Watchers**: {repo.get('watchers', 0):,}
- **ğŸ“‚ Issues**: {repo.get('open_issues', 0)}
- **ğŸ“¦ å¤§å°**: {repo.get('size', 0)} KB

## é¡¹ç›®ç‰¹ç‚¹
"""
        
        # æ·»åŠ é¡¹ç›®ç‰¹ç‚¹
        features = []
        if repo.get('has_wiki'):
            features.append("ğŸ“š åŒ…å«Wikiæ–‡æ¡£")
        if repo.get('has_pages'):
            features.append("ğŸ“– åŒ…å«GitHub Pages")
        if repo.get('archived'):
            features.append("ğŸ“¦ é¡¹ç›®å·²å½’æ¡£")
        if repo.get('topics'):
            features.append(f"ğŸ·ï¸ åŒ…å« {len(repo['topics'])} ä¸ªæŠ€æœ¯æ ‡ç­¾")
        
        if features:
            content += '\n'.join(f"- {feature}" for feature in features)
        else:
            content += "- æš‚æ— ç‰¹æ®Šç‰¹ç‚¹"
        
        content += f"""

## è´¨é‡è¯„ä¼°
- **è´¨é‡è¯„åˆ†**: {quality_score:.1f}/100
- **æ´»è·ƒåº¦**: {'é«˜' if self._is_active_repo(repo) else 'ä¸­ç­‰'}
- **ç¤¾åŒºçƒ­åº¦**: {'é«˜' if stars > 1000 else 'ä¸­ç­‰' if stars > 100 else 'ä¸€èˆ¬'}

## å¤„ç†ä¿¡æ¯
- **çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æ•°æ®æ¥æº**: GitHub Trending
- **æ–‡ä»¶å“ˆå¸Œ**: {hashlib.md5(name.encode()).hexdigest()[:8]}
"""
        
        return content
    
    def _is_active_repo(self, repo: Dict) -> bool:
        """åˆ¤æ–­ä»“åº“æ˜¯å¦æ´»è·ƒï¼ˆæœ€è¿‘30å¤©æœ‰æ›´æ–°ï¼‰"""
        updated_at = repo.get('updated_at')
        if not updated_at:
            return False
        
        try:
            from dateutil import parser
            from datetime import timezone
            
            update_date = parser.parse(updated_at)
            
            # ç»Ÿä¸€æ—¶åŒºå¤„ç†
            if update_date.tzinfo is None:
                update_date = update_date.replace(tzinfo=timezone.utc)
            
            current_time = datetime.now(update_date.tzinfo)
            days_ago = (current_time - update_date).days
            
            return days_ago < 30
        except Exception:
            return False
    
    def _extract_metadata(self, repo: Dict) -> Dict:
        """æå–ä»“åº“å…ƒæ•°æ®"""
        return {
            "id": repo.get('repo_id'),
            "name": repo.get('name'),
            "full_name": repo.get('full_name'),
            "description": repo.get('description'),
            "url": repo.get('url'),
            "language": repo.get('language'),
            "stars": repo.get('stars', 0),
            "forks": repo.get('forks', 0),
            "watchers": repo.get('watchers', 0),
            "open_issues": repo.get('open_issues', 0),
            "topics": repo.get('topics', []),
            "license": repo.get('license'),
            "created_at": repo.get('created_at'),
            "updated_at": repo.get('updated_at'),
            "quality_score": repo.get('quality_score', 0),
            "time_range": repo.get('time_range'),
            "crawl_timestamp": repo.get('crawl_timestamp'),
            "ai_related": True,
            "source": "github_trending"
        }
    
    async def save_time_range_results(self, repos: List[Dict], time_range: str):
        """ä¿å­˜ç‰¹å®šæ—¶é—´èŒƒå›´çš„ç»“æœ"""
        if not repos:
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜èšåˆæ•°æ®JSON
        data_file = self.base_output_dir / "data" / time_range / f"github_trending_{time_range}_{timestamp}.json"
        
        aggregated_data = {
            'crawl_info': {
                'timestamp': timestamp,
                'time_range': time_range,
                'total_tools': len(repos),
                'api_requests': self.api_requests_count,
                'web_requests': self.web_requests_count
            },
            'tools': repos
        }
        
        save_json(aggregated_data, str(data_file))
        
        # ç”Ÿæˆæ’è¡Œæ¦œ
        await self._generate_ranking(repos, time_range, timestamp)
        
        print(f"    ğŸ’¾ ä¿å­˜ {time_range} æ•°æ®: {len(repos)} ä¸ªå·¥å…·")
    
    async def _generate_ranking(self, repos: List[Dict], time_range: str, timestamp: str):
        """ç”Ÿæˆçƒ­åº¦æ’è¡Œæ¦œ"""
        if not repos:
            return
        
        # æŒ‰è´¨é‡åˆ†æ’åº
        sorted_repos = sorted(repos, key=lambda x: x.get('quality_score', 0), reverse=True)
        
        ranking_md = f"""# GitHub Trending AIå·¥å…·æ’è¡Œæ¦œ - {time_range.title()}

## ğŸ“Š æ’è¡Œæ¦œä¿¡æ¯
- **æ—¶é—´èŒƒå›´**: {time_range}
- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **å·¥å…·æ€»æ•°**: {len(repos)}

## ğŸ† Top 20 AIå·¥å…·æ’è¡Œæ¦œ

"""
        
        for i, repo in enumerate(sorted_repos[:20], 1):
            name = repo.get('name', 'Unknown')
            desc = repo.get('description', 'æ— æè¿°')[:100]
            stars = repo.get('stars', 0)
            quality = repo.get('quality_score', 0)
            url = repo.get('url', '#')
            language = repo.get('language', 'Unknown')
            
            # æ·»åŠ å¥–ç‰Œè¡¨æƒ…
            medal = ""
            if i == 1:
                medal = "ğŸ¥‡ "
            elif i == 2:
                medal = "ğŸ¥ˆ "
            elif i == 3:
                medal = "ğŸ¥‰ "
            else:
                medal = f"{i}. "
            
            ranking_md += f"""### {medal}[{name}]({url})

**æè¿°**: {desc}{'...' if len(repo.get('description', '')) > 100 else ''}

**æŠ€æœ¯ä¿¡æ¯**:
- ğŸ’« Stars: {stars:,}
- ğŸ’» è¯­è¨€: {language}
- ğŸ¯ è´¨é‡è¯„åˆ†: {quality:.1f}/100

---

"""
        
        # ä¿å­˜æ’è¡Œæ¦œ
        ranking_file = self.base_output_dir / "rankings" / time_range / f"ranking_{time_range}_{timestamp}.md"
        save_markdown(ranking_md, str(ranking_file))
        
        print(f"    ğŸ† ç”Ÿæˆ {time_range} æ’è¡Œæ¦œ")
    
    async def generate_comprehensive_report(self, all_results: Dict[str, List[Dict]]):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_tools = sum(len(repos) for repos in all_results.values())
        daily_count = len(all_results.get('daily', []))
        weekly_count = len(all_results.get('weekly', []))
        monthly_count = len(all_results.get('monthly', []))
        
        report_md = f"""# GitHub Trending AIå·¥å…·ç»¼åˆæŠ¥å‘Š

## ğŸ“Š æ€»ä½“ç»Ÿè®¡
- **æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **AIå·¥å…·æ€»æ•°**: {total_tools}
- **å»é‡åå”¯ä¸€å·¥å…·**: {len(self.processed_repos)}
- **APIè¯·æ±‚æ€»æ•°**: {self.api_requests_count}
- **ç½‘é¡µè¯·æ±‚æ€»æ•°**: {self.web_requests_count}

## ğŸ“ˆ æ—¶é—´ç»´åº¦åˆ†å¸ƒ
- **ğŸ“… Daily Trending**: {daily_count} ä¸ªå·¥å…·
- **ğŸ“… Weekly Trending**: {weekly_count} ä¸ªå·¥å…·  
- **ğŸ“… Monthly Trending**: {monthly_count} ä¸ªå·¥å…·

## ğŸ† å„æ—¶é—´ç»´åº¦Top 5

"""
        
        for time_range, repos in all_results.items():
            if not repos:
                continue
            
            sorted_repos = sorted(repos, key=lambda x: x.get('quality_score', 0), reverse=True)
            
            report_md += f"""### {time_range.title()} Top 5

"""
            
            for i, repo in enumerate(sorted_repos[:5], 1):
                name = repo.get('name', 'Unknown')
                stars = repo.get('stars', 0)
                quality = repo.get('quality_score', 0)
                url = repo.get('url', '#')
                
                report_md += f"{i}. [{name}]({url}) - {stars:,} â­ (è´¨é‡åˆ†: {quality:.1f})\n"
            
            report_md += "\n"
        
        # è¯­è¨€åˆ†å¸ƒç»Ÿè®¡
        language_stats = {}
        for repos in all_results.values():
            for repo in repos:
                lang = repo.get('language', 'Unknown')
                language_stats[lang] = language_stats.get(lang, 0) + 1
        
        report_md += """## ğŸ’» ç¼–ç¨‹è¯­è¨€åˆ†å¸ƒ

"""
        
        sorted_languages = sorted(language_stats.items(), key=lambda x: x[1], reverse=True)
        for lang, count in sorted_languages[:10]:
            report_md += f"- **{lang}**: {count} ä¸ªå·¥å…·\n"
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Š
        report_file = self.base_output_dir / f"comprehensive_report_{timestamp}.md"
        save_markdown(report_md, str(report_file))
        
        print(f"ğŸ“‹ ç”Ÿæˆç»¼åˆæŠ¥å‘Š: {report_file.name}")
    
    async def _get_trending_from_web(self, language: str = None, since: str = "daily") -> List[Dict]:
        """ä»Trendingé¡µé¢è·å–åŸºç¡€ä»“åº“åˆ—è¡¨"""
        trending_url = get_trending_url(language, since)
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(trending_url, timeout=30) as response:
                    if response.status == 200:
                        html = await response.text()
                        self.web_requests_count += 1
                        
                        soup = BeautifulSoup(html, 'html.parser')
                        repos = []
                        
                        # è§£æTrendingé¡µé¢
                        articles = soup.find_all('article', class_='Box-row')
                        
                        for article in articles:
                            repo = self._parse_trending_repo(article)
                            if repo:
                                repos.append(repo)
                        
                        return repos
                    else:
                        print(f"âŒ è·å–Trendingé¡µé¢å¤±è´¥: {response.status}")
                        return []
                        
        except Exception as e:
            print(f"âŒ è·å–Trendingé¡µé¢å¼‚å¸¸: {e}")
            return []
    
    def _parse_trending_repo(self, article) -> Optional[Dict]:
        """è§£æTrendingé¡µé¢çš„ä»“åº“ä¿¡æ¯"""
        try:
            repo = {}
            
            # ä»“åº“åç§°å’Œé“¾æ¥
            title_elem = article.find('h2', class_='h3')
            if title_elem:
                link_elem = title_elem.find('a')
                if link_elem:
                    href = link_elem.get('href', '')
                    repo['name'] = href.strip('/')
                    repo['url'] = f"https://github.com{href}"
                    
                    # æå–ownerå’Œrepoå
                    parts = href.strip('/').split('/')
                    if len(parts) >= 2:
                        repo['owner'] = parts[0]
                        repo['repo_name'] = parts[1]
            
            # æè¿°
            desc_elem = article.find('p', class_='col-9')
            if desc_elem:
                repo['description'] = clean_text(desc_elem.get_text())
            
            # ç¼–ç¨‹è¯­è¨€
            lang_elem = article.find('span', {'itemprop': 'programmingLanguage'})
            if lang_elem:
                repo['language'] = lang_elem.get_text().strip()
            
            # Stars
            star_elem = article.find('a', href=lambda x: x and '/stargazers' in x)
            if star_elem:
                star_text = star_elem.get_text().strip()
                repo['stars_trending'] = self._parse_number(star_text)
            
            # ä»Šæ—¥Stars
            today_elem = article.find('span', string=lambda x: x and 'stars today' in x)
            if today_elem:
                today_text = today_elem.get_text()
                import re
                match = re.search(r'(\d+)', today_text)
                if match:
                    repo['stars_today'] = int(match.group(1))
            
            return repo if repo.get('name') else None
            
        except Exception as e:
            return None
    
    async def _get_repo_details_from_api(self, repo: Dict) -> Optional[Dict]:
        """ä½¿ç”¨GitHub APIè·å–è¯¦ç»†ä»“åº“ä¿¡æ¯"""
        try:
            # éªŒè¯å¿…è¦å­—æ®µ
            owner = repo.get('owner')
            repo_name = repo.get('repo_name')
            
            if not owner or not repo_name:
                # å°è¯•ä»å…¶ä»–å­—æ®µæå–
                full_name = repo.get('full_name') or repo.get('name', '')
                if '/' in full_name:
                    parts = full_name.split('/')
                    owner = parts[0]
                    repo_name = parts[1] if len(parts) > 1 else parts[0]
                else:
                    print(f"    âš ï¸ ç¼ºå°‘owneræˆ–repo_name: {repo.get('name', 'Unknown')}")
                    return None
            
            api_url = get_repo_api_url(owner, repo_name)
            
            async with aiohttp.ClientSession() as session:
                async with session.get(api_url, headers=self.api_headers) as response:
                    self.api_requests_count += 1
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        # æå–å…³é”®ä¿¡æ¯
                        result = {
                            'full_name': data.get('full_name'),
                            'stars': data.get('stargazers_count', 0),
                            'forks': data.get('forks_count', 0),
                            'watchers': data.get('watchers_count', 0),
                            'open_issues': data.get('open_issues_count', 0),
                            'size': data.get('size', 0),
                            'created_at': data.get('created_at'),
                            'updated_at': data.get('updated_at'),
                            'license': data.get('license', {}).get('name') if data.get('license') else None,
                            'topics': data.get('topics', []),
                            'has_wiki': data.get('has_wiki', False),
                            'has_pages': data.get('has_pages', False),
                            'archived': data.get('archived', False),
                        }
                        
                        # è·å–READMEå†…å®¹
                        readme_content = await self._get_readme_content(session, owner, repo_name)
                        if readme_content:
                            result['readme_content'] = readme_content
                        
                        return result
                        
                    elif response.status == 403:
                        print(f"    âš ï¸ APIé™åˆ¶ (403): {owner}/{repo_name}")
                        return None
                    elif response.status == 404:
                        print(f"    âš ï¸ ä»“åº“ä¸å­˜åœ¨ (404): {owner}/{repo_name}")
                        return None
                    else:
                        print(f"    âš ï¸ APIè¯·æ±‚å¤±è´¥ ({response.status}): {owner}/{repo_name}")
                        return None
                        
        except Exception as e:
            print(f"    âŒ APIè¯·æ±‚å¼‚å¸¸: {repo.get('name', 'Unknown')} - {e}")
            return None
    
    async def _get_readme_content(self, session: aiohttp.ClientSession, owner: str, repo_name: str) -> Optional[str]:
        """è·å–ä»“åº“çš„READMEå†…å®¹"""
        try:
            # GitHub API endpoint for README
            readme_url = f"https://api.github.com/repos/{owner}/{repo_name}/readme"
            
            async with session.get(readme_url, headers=self.api_headers) as response:
                self.api_requests_count += 1
                
                if response.status == 200:
                    data = await response.json()
                    
                    # READMEå†…å®¹æ˜¯base64ç¼–ç çš„
                    import base64
                    content = data.get('content', '')
                    if content:
                        try:
                            # è§£ç base64å†…å®¹
                            decoded_content = base64.b64decode(content).decode('utf-8')
                            
                            # ç®€åŒ–READMEå†…å®¹ï¼Œå–å‰1500å­—ç¬¦
                            if len(decoded_content) > 1500:
                                decoded_content = decoded_content[:1500] + "\n\n... (å†…å®¹è¿‡é•¿ï¼Œå·²æˆªæ–­)"
                            
                            return decoded_content
                        except Exception as decode_error:
                            print(f"    âš ï¸ READMEè§£ç å¤±è´¥: {decode_error}")
                            return None
                elif response.status == 404:
                    # æ²¡æœ‰READMEæ–‡ä»¶
                    return None
                else:
                    print(f"    âš ï¸ READMEè·å–å¤±è´¥ ({response.status}): {owner}/{repo_name}")
                    return None
                    
        except Exception as e:
            print(f"    âš ï¸ READMEè¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    def _is_ai_related(self, repo: Dict) -> bool:
        """åˆ¤æ–­ä»“åº“æ˜¯å¦ä¸AIç›¸å…³"""
        text_to_check = " ".join([
            repo.get('name', ''),
            repo.get('description', ''),
            " ".join(repo.get('topics', []))
        ]).lower()
        
        for keyword in self.config['ai_keywords']:
            if keyword.lower() in text_to_check:
                return True
        return False
    
    def _calculate_quality_score(self, repo: Dict) -> float:
        """è®¡ç®—ä»“åº“è´¨é‡åˆ†æ•°ï¼ˆ0-100ï¼‰"""
        score = 0
        
        # Starsæƒé‡ (40%)
        stars = repo.get('stars', 0)
        if stars >= 10000:
            score += 40
        elif stars >= 1000:
            score += 30
        elif stars >= 100:
            score += 20
        elif stars >= 10:
            score += 10
        
        # æ´»è·ƒåº¦æƒé‡ (25%)
        updated_at = repo.get('updated_at')
        if updated_at:
            try:
                from dateutil import parser
                update_date = parser.parse(updated_at)
                
                # ç»Ÿä¸€æ—¶åŒºå¤„ç†
                if update_date.tzinfo is None:
                    # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå‡è®¾ä¸ºUTC
                    from datetime import timezone
                    update_date = update_date.replace(tzinfo=timezone.utc)
                
                current_time = datetime.now(update_date.tzinfo)
                days_ago = (current_time - update_date).days
                
                if days_ago <= 7:
                    score += 25
                elif days_ago <= 30:
                    score += 20
                elif days_ago <= 90:
                    score += 15
                elif days_ago <= 365:
                    score += 10
            except Exception as e:
                print(f"    âš ï¸ æ—¶é—´è§£æå¤±è´¥: {e}")
                pass
        
        # ç¤¾åŒºå‚ä¸åº¦æƒé‡ (20%)
        forks = repo.get('forks', 0)
        watchers = repo.get('watchers', 0)
        community_score = min((forks * 0.5 + watchers * 0.3) / 10, 20)
        score += community_score
        
        # é¡¹ç›®å®Œæ•´æ€§æƒé‡ (15%)
        if repo.get('license'):
            score += 5
        if repo.get('has_wiki'):
            score += 3
        if repo.get('topics'):
            score += min(len(repo['topics']) * 1, 7)
        
        return min(score, 100)
    
    def _parse_number(self, text: str) -> int:
        """è§£ææ•°å­—ï¼ˆå¤„ç†k, mç­‰å•ä½ï¼‰"""
        try:
            text = text.strip().lower().replace(',', '')
            if 'k' in text:
                return int(float(text.replace('k', '')) * 1000)
            elif 'm' in text:
                return int(float(text.replace('m', '')) * 1000000)
            else:
                import re
                numbers = re.findall(r'\d+', text)
                return int(numbers[0]) if numbers else 0
        except:
            return 0


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç»“æ„åŒ–GitHub Trendingçˆ¬è™«å¯åŠ¨")
    print("=" * 60)
    
    spider = StructuredGitHubSpider()
    
    try:
        # çˆ¬å–æ‰€æœ‰æ—¶é—´ç»´åº¦çš„æ•°æ®
        all_results = await spider.crawl_all_time_ranges()
        
        print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        print(f"   ğŸ“Š Daily: {len(all_results.get('daily', []))} ä¸ªå·¥å…·")
        print(f"   ğŸ“Š Weekly: {len(all_results.get('weekly', []))} ä¸ªå·¥å…·")
        print(f"   ğŸ“Š Monthly: {len(all_results.get('monthly', []))} ä¸ªå·¥å…·")
        print(f"   ğŸ”„ å»é‡åæ€»è®¡: {len(spider.processed_repos)} ä¸ªå”¯ä¸€å·¥å…·")
        print(f"   ğŸŒ æ€»APIè¯·æ±‚: {spider.api_requests_count}")
        print(f"   ğŸ“„ æ€»ç½‘é¡µè¯·æ±‚: {spider.web_requests_count}")
        
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
