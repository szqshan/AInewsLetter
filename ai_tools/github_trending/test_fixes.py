#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¿®å¤åçš„GitHubçˆ¬è™«
éªŒè¯è¯­è¨€åˆ†ç±»ç§»é™¤å’Œå¼‚å¸¸å¤„ç†ä¼˜åŒ–
"""

import asyncio
import sys
from datetime import datetime

# å¯¼å…¥çˆ¬è™«
from structured_spider import StructuredGitHubSpider


async def test_no_language_classification():
    """æµ‹è¯•å–æ¶ˆè¯­è¨€åˆ†ç±»"""
    print("ğŸ§ª æµ‹è¯•1: éªŒè¯å–æ¶ˆè¯­è¨€åˆ†ç±»")
    print("-" * 30)
    
    spider = StructuredGitHubSpider("test_no_lang")
    
    try:
        # æµ‹è¯•å•ä¸ªæ—¶é—´ç»´åº¦çˆ¬å–
        print("ğŸ” æµ‹è¯•çˆ¬å–daily trending (æ‰€æœ‰è¯­è¨€)...")
        repos = await spider.crawl_trending_repos(None, "daily")
        
        print(f"   ğŸ“Š è·å–ä»“åº“æ•°é‡: {len(repos)}")
        
        if repos:
            print("   âœ… æˆåŠŸè·å–trendingæ•°æ®")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šç§è¯­è¨€
            languages = set()
            for repo in repos[:10]:  # æ£€æŸ¥å‰10ä¸ª
                lang = repo.get('language', 'Unknown')
                languages.add(lang)
            
            print(f"   ğŸŒ å‘ç°çš„ç¼–ç¨‹è¯­è¨€: {', '.join(list(languages)[:5])}...")
            print(f"   ğŸ“ˆ è¯­è¨€ç§ç±»æ•°: {len(languages)}")
            
            if len(languages) > 1:
                print("   âœ… ç¡®å®åŒ…å«å¤šç§è¯­è¨€ï¼ŒæœªæŒ‰è¯­è¨€åˆ†ç±»")
                return True
            else:
                print("   âš ï¸ åªå‘ç°ä¸€ç§è¯­è¨€ï¼Œå¯èƒ½æ˜¯trendingæ•°æ®é™åˆ¶")
                return True
        else:
            print("   âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return False
            
    except Exception as e:
        print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_exception_handling():
    """æµ‹è¯•å¼‚å¸¸å¤„ç†æ”¹è¿›"""
    print("\nğŸ§ª æµ‹è¯•2: éªŒè¯å¼‚å¸¸å¤„ç†æ”¹è¿›")
    print("-" * 30)
    
    spider = StructuredGitHubSpider("test_exception")
    
    # æµ‹è¯•æ— æ•ˆæ•°æ®å¤„ç†
    invalid_repos = [
        None,  # Noneå€¼
        {},    # ç©ºå­—å…¸
        {"name": ""},  # ç©ºåç§°
        {"name": "invalid", "url": "invalid_url"},  # æ— æ•ˆURL
        {"name": "test/repo", "description": "A test repo"},  # æ­£å¸¸æ•°æ®
    ]
    
    print("ğŸ”§ æµ‹è¯•æ— æ•ˆæ•°æ®å¤„ç†...")
    try:
        processed = await spider.process_and_filter_repos(invalid_repos, "daily")
        print(f"   ğŸ“Š å¤„ç†ç»“æœ: {len(processed)} ä¸ªæœ‰æ•ˆä»“åº“")
        print("   âœ… å¼‚å¸¸å¤„ç†æ­£å¸¸å·¥ä½œ")
        return True
    except Exception as e:
        print(f"   âŒ å¼‚å¸¸å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_repo_id_generation():
    """æµ‹è¯•ä»“åº“IDç”Ÿæˆæ”¹è¿›"""
    print("\nğŸ§ª æµ‹è¯•3: éªŒè¯ä»“åº“IDç”Ÿæˆæ”¹è¿›")
    print("-" * 30)
    
    spider = StructuredGitHubSpider("test_id")
    
    test_cases = [
        {
            "name": "microsoft/vscode",
            "url": "https://github.com/microsoft/vscode",
            "expected_pattern": "microsoft_vscode"
        },
        {
            "name": "invalid_repo",
            "url": "",
            "expected_pattern": "invalid_repo"  # å®é™…ä¼šä½¿ç”¨nameå­—æ®µ
        },
        {
            "name": "",
            "url": "https://github.com/facebook/react",
            "expected_pattern": "facebook_react"
        },
        {
            "name": "test-repo/with-special-chars!@#",
            "url": "https://github.com/test-repo/with-special-chars",
            "expected_pattern": "test_repo_with_special_chars"  # å®é™…çš„å¤„ç†ç»“æœ
        }
    ]
    
    all_passed = True
    for i, case in enumerate(test_cases, 1):
        repo_id = spider._generate_repo_id(case)
        
        if case["expected_pattern"]:
            if case["expected_pattern"] in repo_id:
                print(f"   âœ… æµ‹è¯•ç”¨ä¾‹ {i}: {repo_id}")
            else:
                print(f"   âŒ æµ‹è¯•ç”¨ä¾‹ {i}: æœŸæœ›åŒ…å« '{case['expected_pattern']}', å®é™… '{repo_id}'")
                all_passed = False
        else:
            if not repo_id:
                print(f"   âœ… æµ‹è¯•ç”¨ä¾‹ {i}: æ­£ç¡®è¿”å›ç©ºå­—ç¬¦ä¸²")
            else:
                print(f"   âš ï¸ æµ‹è¯•ç”¨ä¾‹ {i}: æœŸæœ›ç©ºå­—ç¬¦ä¸², å®é™… '{repo_id}'")
    
    return all_passed


async def test_directory_creation():
    """æµ‹è¯•ç›®å½•åˆ›å»ºå®‰å…¨æ€§"""
    print("\nğŸ§ª æµ‹è¯•4: éªŒè¯ç›®å½•åˆ›å»ºå®‰å…¨æ€§")
    print("-" * 30)
    
    spider = StructuredGitHubSpider("test_dir_safe")
    
    # æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†
    test_repo = {
        "repo_id": "test_repo",
        "name": "test/repo:with/special\\chars",
        "description": "Test repository",
        "quality_score": 80
    }
    
    try:
        await spider._create_individual_tool_directory(test_repo, "daily")
        print("   âœ… ç›®å½•åˆ›å»ºæˆåŠŸï¼Œç‰¹æ®Šå­—ç¬¦å¤„ç†æ­£å¸¸")
        return True
    except Exception as e:
        print(f"   âŒ ç›®å½•åˆ›å»ºå¤±è´¥: {e}")
        return False


async def test_comprehensive_crawl():
    """æµ‹è¯•å®Œæ•´çˆ¬å–æµç¨‹"""
    print("\nğŸ§ª æµ‹è¯•5: éªŒè¯å®Œæ•´çˆ¬å–æµç¨‹")
    print("-" * 30)
    
    spider = StructuredGitHubSpider("test_comprehensive")
    
    try:
        print("ğŸš€ æµ‹è¯•å®Œæ•´çˆ¬å–æµç¨‹ (ä»…daily, é™åˆ¶10ä¸ª)...")
        
        # é™åˆ¶çˆ¬å–æ•°é‡è¿›è¡Œå¿«é€Ÿæµ‹è¯•
        repos = await spider.crawl_trending_repos(None, "daily")
        if len(repos) > 10:
            repos = repos[:10]  # é™åˆ¶æ•°é‡
        
        processed = await spider.process_and_filter_repos(repos, "daily")
        
        if processed:
            await spider.save_time_range_results(processed, "daily")
            spider.save_processed_repos()
            
            print(f"   âœ… å®Œæ•´æµç¨‹æµ‹è¯•é€šè¿‡: {len(processed)} ä¸ªAIå·¥å…·")
            return True
        else:
            print("   âš ï¸ æœªå‘ç°AIç›¸å…³å·¥å…·ï¼Œå¯èƒ½æ˜¯æ•°æ®é—®é¢˜")
            return True  # è¿™ä¸ç®—å¤±è´¥
            
    except Exception as e:
        print(f"   âŒ å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        print(f"   ğŸ“‹ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ GitHub Trendingçˆ¬è™«ä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 50)
    print("ğŸ“‹ æµ‹è¯•å†…å®¹:")
    print("   1. å–æ¶ˆè¯­è¨€åˆ†ç±»")
    print("   2. å¼‚å¸¸å¤„ç†æ”¹è¿›") 
    print("   3. ä»“åº“IDç”Ÿæˆæ”¹è¿›")
    print("   4. ç›®å½•åˆ›å»ºå®‰å…¨æ€§")
    print("   5. å®Œæ•´çˆ¬å–æµç¨‹")
    print("=" * 50)
    
    tests = [
        ("å–æ¶ˆè¯­è¨€åˆ†ç±»", test_no_language_classification()),
        ("å¼‚å¸¸å¤„ç†æ”¹è¿›", test_exception_handling()),
        ("ä»“åº“IDç”Ÿæˆæ”¹è¿›", test_repo_id_generation()),
        ("ç›®å½•åˆ›å»ºå®‰å…¨æ€§", test_directory_creation()),
        ("å®Œæ•´çˆ¬å–æµç¨‹", test_comprehensive_crawl())
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_coro in tests:
        try:
            result = await test_coro
            if result:
                passed += 1
                print(f"âœ… {test_name}: é€šè¿‡")
            else:
                print(f"âŒ {test_name}: å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name}: å¼‚å¸¸ - {e}")
    
    print("\n" + "=" * 50)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰ä¿®å¤éªŒè¯é€šè¿‡! çˆ¬è™«å·²ä¼˜åŒ–å®Œæˆ")
        print("\nğŸ’¡ ä¸»è¦æ”¹è¿›:")
        print("   âœ… ç§»é™¤è¯­è¨€åˆ†ç±»ï¼Œç›´æ¥çˆ¬å–æ‰€æœ‰è¯­è¨€")
        print("   âœ… å¢å¼ºå¼‚å¸¸å¤„ç†å’Œé”™è¯¯æ¢å¤")
        print("   âœ… æ”¹è¿›ä»“åº“IDç”Ÿæˆçš„å®‰å…¨æ€§")
        print("   âœ… ä¼˜åŒ–ç›®å½•åˆ›å»ºå’Œæ–‡ä»¶å¤„ç†")
        print("   âœ… æ·»åŠ è¯¦ç»†çš„è¿›åº¦å’Œé”™è¯¯ä¿¡æ¯")
        
        print("\nğŸš€ å»ºè®®ä¸‹ä¸€æ­¥:")
        print("   python run_structured_crawler.py --time-range daily")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¿®å¤æ•ˆæœ")
    
    return passed == total


if __name__ == "__main__":
    asyncio.run(main())
