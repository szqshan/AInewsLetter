#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆGitHub Trendingçˆ¬è™«
ç»“åˆç½‘é¡µçˆ¬è™«å’ŒGitHub APIï¼Œæä¾›æ›´ä¸°å¯Œçš„æ•°æ®
"""

import sys
import os
import time
import json
import requests
import asyncio
import aiohttp
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
from bs4 import BeautifulSoup

# å¯¼å…¥é…ç½®
from github_config import GITHUB_CONFIG, get_api_headers, get_trending_url, get_repo_api_url

# æ·»åŠ å…±äº«æ¨¡å—è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'shared'))
from utils import save_json, save_markdown, clean_text


class EnhancedGitHubSpider:
    """å¢å¼ºç‰ˆGitHubçˆ¬è™«ï¼Œæ”¯æŒAPI Token"""
    
    def __init__(self):
        self.config = GITHUB_CONFIG
        self.api_headers = get_api_headers()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.config['headers']['User-Agent']
        })
        
        # è¯·æ±‚ç»Ÿè®¡
        self.api_requests_count = 0
        self.web_requests_count = 0
        self.start_time = time.time()
        
        # æ•°æ®å­˜å‚¨
        self.output_dir = Path("crawled_data")
        self.output_dir.mkdir(exist_ok=True)
        
        print("ğŸš€ GitHubå¢å¼ºçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        print(f"   API Token: {self.config['api_token'][:20]}...")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    async def crawl_trending_repos(self, language: str = None, since: str = "daily") -> List[Dict]:
        """çˆ¬å–Trendingä»“åº“ï¼ˆç½‘é¡µ + APIæ··åˆæ¨¡å¼ï¼‰"""
        print(f"\nğŸ” çˆ¬å–Trendingä»“åº“: {language or 'all'} languages, {since}")
        
        # æ­¥éª¤1: ä»Trendingé¡µé¢è·å–åŸºç¡€åˆ—è¡¨
        trending_repos = await self._get_trending_from_web(language, since)
        print(f"ğŸ“Š ä»Trendingé¡µé¢è·å–: {len(trending_repos)} ä¸ªä»“åº“")
        
        # æ­¥éª¤2: ä½¿ç”¨APIè·å–è¯¦ç»†ä¿¡æ¯
        enhanced_repos = []
        for i, repo in enumerate(trending_repos):
            try:
                print(f"ğŸ”„ å¤„ç†ä»“åº“ {i+1}/{len(trending_repos)}: {repo.get('name', 'Unknown')}")
                
                # è·å–APIè¯¦ç»†ä¿¡æ¯
                api_data = await self._get_repo_details_from_api(repo)
                
                if api_data:
                    # åˆå¹¶æ•°æ®
                    enhanced_repo = {**repo, **api_data}
                    
                    # AIç›¸å…³æ€§æ£€æŸ¥
                    if self._is_ai_related(enhanced_repo):
                        # è´¨é‡è¯„ä¼°
                        enhanced_repo['quality_score'] = self._calculate_quality_score(enhanced_repo)
                        enhanced_repos.append(enhanced_repo)
                        print(f"  âœ… AIç›¸å…³ä»“åº“ï¼Œè´¨é‡åˆ†: {enhanced_repo['quality_score']:.1f}")
                    else:
                        print(f"  â© éAIç›¸å…³ï¼Œè·³è¿‡")
                else:
                    print(f"  âŒ APIè·å–å¤±è´¥")
                
                # é¢‘ç‡æ§åˆ¶
                await asyncio.sleep(self.config['crawl_config']['request_delay'])
                
            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ‰ æœ€ç»ˆè·å¾— {len(enhanced_repos)} ä¸ªAIç›¸å…³ä»“åº“")
        return enhanced_repos
    
    async def _get_trending_from_web(self, language: str = None, since: str = "daily") -> List[Dict]:
        """ä»Trendingé¡µé¢è·å–åŸºç¡€ä»“åº“åˆ—è¡¨"""
        trending_url = get_trending_url(language, since)
        
        try:
            response = self.session.get(trending_url, timeout=30)
            response.raise_for_status()
            self.web_requests_count += 1
            
            soup = BeautifulSoup(response.content, 'html.parser')
            repos = []
            
            # è§£æTrendingé¡µé¢
            articles = soup.find_all('article', class_='Box-row')
            
            for article in articles:
                repo = self._parse_trending_repo(article)
                if repo:
                    repos.append(repo)
            
            return repos
            
        except Exception as e:
            print(f"âŒ è·å–Trendingé¡µé¢å¤±è´¥: {e}")
            return []
    
    def _parse_trending_repo(self, article) -> Optional[Dict]:
        """è§£æTrendingé¡µé¢çš„ä»“åº“ä¿¡æ¯"""
        try:
            repo = {}
            
            # ä»“åº“åç§°å’Œé“¾æ¥
            title_elem = article.find('h2', class_='h3')
            if title_elem:
                link_elem = title_elem.find('a')
                if link_elem:
                    href = link_elem.get('href', '')
                    repo['name'] = href.strip('/')
                    repo['url'] = f"https://github.com{href}"
                    
                    # æå–ownerå’Œrepoå
                    parts = href.strip('/').split('/')
                    if len(parts) >= 2:
                        repo['owner'] = parts[0]
                        repo['repo_name'] = parts[1]
            
            # æè¿°
            desc_elem = article.find('p', class_='col-9')
            if desc_elem:
                repo['description'] = clean_text(desc_elem.get_text())
            
            # ç¼–ç¨‹è¯­è¨€
            lang_elem = article.find('span', {'itemprop': 'programmingLanguage'})
            if lang_elem:
                repo['language'] = lang_elem.get_text().strip()
            
            # Stars
            star_elem = article.find('a', href=lambda x: x and '/stargazers' in x)
            if star_elem:
                star_text = star_elem.get_text().strip()
                repo['stars_trending'] = self._parse_number(star_text)
            
            # ä»Šæ—¥Stars
            today_elem = article.find('span', string=lambda x: x and 'stars today' in x)
            if today_elem:
                today_text = today_elem.get_text()
                import re
                match = re.search(r'(\d+)', today_text)
                if match:
                    repo['stars_today'] = int(match.group(1))
            
            # æ·»åŠ å…ƒæ•°æ®
            repo['crawl_source'] = 'trending_page'
            repo['crawl_timestamp'] = datetime.now().isoformat()
            
            return repo if repo.get('name') else None
            
        except Exception as e:
            print(f"âŒ è§£æä»“åº“å¤±è´¥: {e}")
            return None
    
    async def _get_repo_details_from_api(self, repo: Dict) -> Optional[Dict]:
        """ä½¿ç”¨GitHub APIè·å–è¯¦ç»†ä»“åº“ä¿¡æ¯"""
        if not repo.get('owner') or not repo.get('repo_name'):
            return None
        
        api_url = get_repo_api_url(repo['owner'], repo['repo_name'])
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(api_url, headers=self.api_headers) as response:
                    self.api_requests_count += 1
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        # æå–å…³é”®ä¿¡æ¯
                        api_info = {
                            'full_name': data.get('full_name'),
                            'stars': data.get('stargazers_count', 0),
                            'forks': data.get('forks_count', 0),
                            'watchers': data.get('watchers_count', 0),
                            'open_issues': data.get('open_issues_count', 0),
                            'size': data.get('size', 0),
                            'created_at': data.get('created_at'),
                            'updated_at': data.get('updated_at'),
                            'pushed_at': data.get('pushed_at'),
                            'license': data.get('license', {}).get('name') if data.get('license') else None,
                            'topics': data.get('topics', []),
                            'has_wiki': data.get('has_wiki', False),
                            'has_pages': data.get('has_pages', False),
                            'archived': data.get('archived', False),
                            'disabled': data.get('disabled', False),
                            'default_branch': data.get('default_branch'),
                            'subscribers_count': data.get('subscribers_count', 0),
                            'network_count': data.get('network_count', 0),
                            'api_source': True
                        }
                        
                        return api_info
                        
                    elif response.status == 403:
                        print(f"âš ï¸ APIé™åˆ¶ï¼Œè·³è¿‡: {repo['name']}")
                        return None
                    else:
                        print(f"âŒ APIè¯·æ±‚å¤±è´¥ {response.status}: {repo['name']}")
                        return None
                        
        except Exception as e:
            print(f"âŒ APIè¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    def _is_ai_related(self, repo: Dict) -> bool:
        """åˆ¤æ–­ä»“åº“æ˜¯å¦ä¸AIç›¸å…³"""
        # æ£€æŸ¥æ–‡æœ¬å†…å®¹
        text_to_check = " ".join([
            repo.get('name', ''),
            repo.get('description', ''),
            " ".join(repo.get('topics', []))
        ]).lower()
        
        # å…³é”®è¯åŒ¹é…
        for keyword in self.config['ai_keywords']:
            if keyword.lower() in text_to_check:
                return True
        
        return False
    
    def _calculate_quality_score(self, repo: Dict) -> float:
        """è®¡ç®—ä»“åº“è´¨é‡åˆ†æ•°ï¼ˆ0-100ï¼‰"""
        score = 0
        
        # Starsæƒé‡ (40%)
        stars = repo.get('stars', 0)
        if stars >= 10000:
            score += 40
        elif stars >= 1000:
            score += 30
        elif stars >= 100:
            score += 20
        elif stars >= 10:
            score += 10
        
        # æ´»è·ƒåº¦æƒé‡ (25%)
        updated_at = repo.get('updated_at')
        if updated_at:
            try:
                from dateutil import parser
                update_date = parser.parse(updated_at)
                days_ago = (datetime.now(update_date.tzinfo) - update_date).days
                
                if days_ago <= 7:
                    score += 25
                elif days_ago <= 30:
                    score += 20
                elif days_ago <= 90:
                    score += 15
                elif days_ago <= 365:
                    score += 10
            except:
                pass
        
        # ç¤¾åŒºå‚ä¸åº¦æƒé‡ (20%)
        forks = repo.get('forks', 0)
        watchers = repo.get('watchers', 0)
        
        community_score = min((forks * 0.5 + watchers * 0.3) / 10, 20)
        score += community_score
        
        # é¡¹ç›®å®Œæ•´æ€§æƒé‡ (15%)
        if repo.get('license'):
            score += 5
        if repo.get('has_wiki'):
            score += 3
        if repo.get('topics'):
            score += min(len(repo['topics']) * 1, 7)
        
        return min(score, 100)
    
    def _parse_number(self, text: str) -> int:
        """è§£ææ•°å­—ï¼ˆå¤„ç†k, mç­‰å•ä½ï¼‰"""
        try:
            text = text.strip().lower().replace(',', '')
            if 'k' in text:
                return int(float(text.replace('k', '')) * 1000)
            elif 'm' in text:
                return int(float(text.replace('m', '')) * 1000000)
            else:
                import re
                numbers = re.findall(r'\d+', text)
                return int(numbers[0]) if numbers else 0
        except:
            return 0
    
    async def save_results(self, repos: List[Dict], language: str = None, since: str = "daily"):
        """ä¿å­˜çˆ¬å–ç»“æœ"""
        if not repos:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        lang_suffix = f"_{language}" if language else "_all"
        
        # ä¿å­˜JSON
        json_file = self.output_dir / f"github_trending_{since}{lang_suffix}_{timestamp}.json"
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        metadata = {
            'crawl_info': {
                'timestamp': timestamp,
                'language': language,
                'since': since,
                'total_repos': len(repos),
                'api_requests': self.api_requests_count,
                'web_requests': self.web_requests_count,
                'duration_seconds': time.time() - self.start_time
            },
            'repos': repos
        }
        
        save_json(metadata, str(json_file))
        
        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_file = self.output_dir / f"github_trending_{since}{lang_suffix}_{timestamp}.md"
        markdown_content = self._generate_markdown_report(repos, language, since, metadata['crawl_info'])
        save_markdown(markdown_content, str(md_file))
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
        print(f"   ğŸ“„ JSON: {json_file}")
        print(f"   ğŸ“ Markdown: {md_file}")
    
    def _generate_markdown_report(self, repos: List[Dict], language: str, since: str, crawl_info: Dict) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        lang_name = language or "All Languages"
        
        content = f"""# GitHub Trending AI Tools - {lang_name} ({since.title()})

## ğŸ“Š çˆ¬å–ä¿¡æ¯
- **æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **è¯­è¨€**: {lang_name}
- **æ—¶é—´èŒƒå›´**: {since}
- **æ€»ä»“åº“æ•°**: {crawl_info['total_repos']}
- **APIè¯·æ±‚**: {crawl_info['api_requests']} æ¬¡
- **ç½‘é¡µè¯·æ±‚**: {crawl_info['web_requests']} æ¬¡
- **è€—æ—¶**: {crawl_info['duration_seconds']:.1f} ç§’

## ğŸ† é«˜è´¨é‡AIå·¥å…· (æŒ‰è´¨é‡åˆ†æ’åº)

"""
        
        # æŒ‰è´¨é‡åˆ†æ’åº
        sorted_repos = sorted(repos, key=lambda x: x.get('quality_score', 0), reverse=True)
        
        for i, repo in enumerate(sorted_repos, 1):
            name = repo.get('name', 'Unknown')
            desc = repo.get('description', 'æ— æè¿°')
            stars = repo.get('stars', 0)
            stars_today = repo.get('stars_today', 0)
            forks = repo.get('forks', 0)
            language = repo.get('language', 'Unknown')
            quality = repo.get('quality_score', 0)
            url = repo.get('url', '#')
            topics = repo.get('topics', [])
            license_name = repo.get('license', 'No License')
            
            content += f"""### {i}. [{name}]({url})

**æè¿°**: {desc}

**æŠ€æœ¯ä¿¡æ¯**:
- ğŸ’« Stars: {stars:,} (+{stars_today} ä»Šæ—¥)
- ğŸ´ Forks: {forks:,}
- ğŸ’» è¯­è¨€: {language}
- ğŸ“œ è®¸å¯è¯: {license_name}
- ğŸ¯ è´¨é‡è¯„åˆ†: {quality:.1f}/100

"""
            
            if topics:
                content += f"**æ ‡ç­¾**: {', '.join(topics[:10])}\n\n"
            
            # é¡¹ç›®ç‰¹ç‚¹
            features = []
            if repo.get('has_wiki'):
                features.append("ğŸ“š Wiki")
            if repo.get('has_pages'):
                features.append("ğŸ“– Pages")
            if repo.get('archived'):
                features.append("ğŸ“¦ å·²å½’æ¡£")
            
            if features:
                content += f"**ç‰¹ç‚¹**: {' | '.join(features)}\n\n"
            
            content += "---\n\n"
        
        # æ·»åŠ ç»Ÿè®¡æ‘˜è¦
        if repos:
            total_stars = sum(repo.get('stars', 0) for repo in repos)
            avg_stars = total_stars / len(repos)
            top_languages = {}
            
            for repo in repos:
                lang = repo.get('language', 'Unknown')
                top_languages[lang] = top_languages.get(lang, 0) + 1
            
            content += f"""## ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦

- **æ€»Starsæ•°**: {total_stars:,}
- **å¹³å‡Stars**: {avg_stars:.0f}
- **æœ€çƒ­é¡¹ç›®**: {sorted_repos[0].get('name', 'Unknown')} ({sorted_repos[0].get('stars', 0):,} stars)

### ğŸ”¤ ç¼–ç¨‹è¯­è¨€åˆ†å¸ƒ
"""
            
            for lang, count in sorted(top_languages.items(), key=lambda x: x[1], reverse=True):
                content += f"- **{lang}**: {count} ä¸ªé¡¹ç›®\n"
        
        return content


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GitHubå¢å¼ºçˆ¬è™«å¯åŠ¨")
    print("=" * 50)
    
    spider = EnhancedGitHubSpider()
    
    # é…ç½®çˆ¬å–å‚æ•°
    languages = ["python", "javascript", None]  # Noneè¡¨ç¤ºæ‰€æœ‰è¯­è¨€
    time_ranges = ["daily"]
    
    for time_range in time_ranges:
        for language in languages:
            try:
                lang_name = language or "all"
                print(f"\nğŸ¯ å¼€å§‹çˆ¬å–: {lang_name} - {time_range}")
                
                # çˆ¬å–ä»“åº“
                repos = await spider.crawl_trending_repos(language, time_range)
                
                # ä¿å­˜ç»“æœ
                await spider.save_results(repos, language, time_range)
                
                print(f"âœ… å®Œæˆ: {lang_name} - {len(repos)} ä¸ªAIä»“åº“")
                
                # è¯­è¨€é—´å»¶è¿Ÿ
                if language != languages[-1]:
                    print("â³ ç­‰å¾…10ç§’åç»§ç»­...")
                    await asyncio.sleep(10)
                    
            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
                continue
    
    print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
    print(f"   ğŸ“Š æ€»APIè¯·æ±‚: {spider.api_requests_count}")
    print(f"   ğŸŒ æ€»ç½‘é¡µè¯·æ±‚: {spider.web_requests_count}")


if __name__ == "__main__":
    asyncio.run(main())
