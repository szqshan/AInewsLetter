#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»“æ„åŒ–GitHubçˆ¬è™«å¯åŠ¨è„šæœ¬
æ”¯æŒæ—¶é—´ç»´åº¦åˆ†ç±»ã€å»é‡ã€ç»“æ„åŒ–å­˜å‚¨
"""

import argparse
import asyncio
import sys
from pathlib import Path
from datetime import datetime

# å¯¼å…¥ç»“æ„åŒ–çˆ¬è™«
from structured_spider import StructuredGitHubSpider


def print_banner():
    """æ‰“å°å¯åŠ¨æ¨ªå¹…"""
    print("""
ğŸš€ GitHub Trending ç»“æ„åŒ–çˆ¬è™«
================================
âœ¨ åŠŸèƒ½ç‰¹ç‚¹:
  ğŸ“… æ”¯æŒ daily/weekly/monthly ä¸‰ç§æ—¶é—´ç»´åº¦
  ğŸ”„ æ™ºèƒ½å»é‡ï¼Œé¿å…é‡å¤çˆ¬å–
  ğŸ“ ä¸ºæ¯ä¸ªå·¥å…·åˆ›å»ºå•ç‹¬ç›®å½•
  ğŸ† è‡ªåŠ¨ç”Ÿæˆçƒ­åº¦æ’è¡Œæ¦œ
  ğŸ“Š å‚è€ƒarXivæ¶æ„çš„ç»“æ„åŒ–å­˜å‚¨
================================
    """)


async def run_full_crawl(output_dir: str = "crawled_data"):
    """è¿è¡Œå®Œæ•´çš„åˆ†æ—¶é—´ç»´åº¦çˆ¬å–"""
    print_banner()
    
    start_time = datetime.now()
    
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        spider = StructuredGitHubSpider(output_dir)
        
        print("ğŸ¯ å¼€å§‹å®Œæ•´çš„GitHub Trendingçˆ¬å–...")
        print("   ğŸ“… å°†çˆ¬å– dailyã€weeklyã€monthly ä¸‰ä¸ªæ—¶é—´ç»´åº¦")
        print("   ğŸ” å°†çˆ¬å–æ‰€æœ‰è¯­è¨€çš„AIå·¥å…·ï¼ˆä¸æŒ‰è¯­è¨€åˆ†ç±»ï¼‰")
        print("   â±ï¸ é¢„è®¡è€—æ—¶: 10-20åˆ†é’Ÿ")
        print("-" * 60)
        
        # æ‰§è¡Œå®Œæ•´çˆ¬å–
        results = await spider.crawl_all_time_ranges()
        
        # ç»Ÿè®¡ç»“æœ
        total_tools = sum(len(repos) for repos in results.values())
        unique_tools = len(spider.processed_repos)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print("\n" + "=" * 60)
        print("ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆ!")
        print(f"   â±ï¸ æ€»è€—æ—¶: {duration}")
        print(f"   ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        print(f"      ğŸ“… Daily: {len(results.get('daily', []))} ä¸ªAIå·¥å…·")
        print(f"      ğŸ“… Weekly: {len(results.get('weekly', []))} ä¸ªAIå·¥å…·") 
        print(f"      ğŸ“… Monthly: {len(results.get('monthly', []))} ä¸ªAIå·¥å…·")
        print(f"   ğŸ”„ å»é‡ç»Ÿè®¡:")
        print(f"      ğŸ“š æ€»çˆ¬å–æ•°: {total_tools}")
        print(f"      ğŸ¯ å”¯ä¸€å·¥å…·: {unique_tools}")
        print(f"   ğŸ“¡ è¯·æ±‚ç»Ÿè®¡:")
        print(f"      ğŸŒ ç½‘é¡µè¯·æ±‚: {spider.web_requests_count}")
        print(f"      ğŸ”§ APIè¯·æ±‚: {spider.api_requests_count}")
        
        print(f"\nğŸ“ è¾“å‡ºç›®å½•ç»“æ„:")
        print(f"   ğŸ“‚ {output_dir}/")
        print(f"   â”œâ”€â”€ ğŸ“ tools/           # æ¯ä¸ªå·¥å…·çš„å•ç‹¬ç›®å½•")
        print(f"   â”‚   â”œâ”€â”€ ğŸ“ daily/")
        print(f"   â”‚   â”œâ”€â”€ ğŸ“ weekly/")
        print(f"   â”‚   â””â”€â”€ ğŸ“ monthly/")
        print(f"   â”œâ”€â”€ ğŸ“ data/            # èšåˆæ•°æ®JSON")
        print(f"   â”œâ”€â”€ ğŸ“ rankings/        # çƒ­åº¦æ’è¡Œæ¦œ")
        print(f"   â””â”€â”€ ğŸ“ metadata/        # å»é‡è®°å½•")
        
        return results
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        return None
    except Exception as e:
        print(f"\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


async def run_single_time_range(time_range: str, output_dir: str = "crawled_data"):
    """è¿è¡Œå•ä¸€æ—¶é—´ç»´åº¦çˆ¬å–"""
    print_banner()
    
    if time_range not in ["daily", "weekly", "monthly"]:
        print(f"âŒ æ— æ•ˆçš„æ—¶é—´èŒƒå›´: {time_range}")
        print("   æ”¯æŒçš„æ—¶é—´èŒƒå›´: daily, weekly, monthly")
        return None
    
    start_time = datetime.now()
    
    try:
        spider = StructuredGitHubSpider(output_dir)
        
        print(f"ğŸ¯ å¼€å§‹çˆ¬å– {time_range} GitHub Trending...")
        print("-" * 40)
        
        # çˆ¬å–å•ä¸€æ—¶é—´ç»´åº¦ï¼ˆæ‰€æœ‰è¯­è¨€ï¼‰
        print(f"\nğŸ” çˆ¬å–æ‰€æœ‰è¯­è¨€çš„AIå·¥å…·...")
        
        repos = await spider.crawl_trending_repos(None, time_range)
        all_repos = await spider.process_and_filter_repos(repos, time_range)
        
        # ä¿å­˜ç»“æœ
        await spider.save_time_range_results(all_repos, time_range)
        spider.save_processed_repos()
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"\nâœ… {time_range} çˆ¬å–å®Œæˆ!")
        print(f"   â±ï¸ è€—æ—¶: {duration}")
        print(f"   ğŸ“Š AIå·¥å…·æ•°é‡: {len(all_repos)}")
        print(f"   ğŸ”„ å»é‡åå”¯ä¸€å·¥å…·: {len(spider.processed_repos)}")
        
        return all_repos
        
    except Exception as e:
        print(f"\nâŒ çˆ¬å–å¤±è´¥: {e}")
        return None


def show_directory_structure(output_dir: str):
    """æ˜¾ç¤ºè¾“å‡ºç›®å½•ç»“æ„"""
    output_path = Path(output_dir)
    
    if not output_path.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {output_dir}")
        return
    
    print(f"ğŸ“ ç›®å½•ç»“æ„: {output_dir}")
    print("=" * 40)
    
    # ç»Ÿè®¡å„ç±»æ–‡ä»¶æ•°é‡
    tools_dirs = list((output_path / "tools").rglob("*/")) if (output_path / "tools").exists() else []
    data_files = list((output_path / "data").rglob("*.json")) if (output_path / "data").exists() else []
    ranking_files = list((output_path / "rankings").rglob("*.md")) if (output_path / "rankings").exists() else []
    
    print(f"ğŸ“‚ å·¥å…·ç›®å½•: {len(tools_dirs)} ä¸ª")
    print(f"ğŸ“„ æ•°æ®æ–‡ä»¶: {len(data_files)} ä¸ª")
    print(f"ğŸ† æ’è¡Œæ¦œæ–‡ä»¶: {len(ranking_files)} ä¸ª")
    
    # æ˜¾ç¤ºæœ€è¿‘çš„å‡ ä¸ªå·¥å…·ç›®å½•
    if tools_dirs:
        print("\nğŸ“ æœ€è¿‘çš„å·¥å…·ç›®å½• (å‰5ä¸ª):")
        for tool_dir in sorted(tools_dirs, key=lambda x: x.stat().st_mtime, reverse=True)[:5]:
            rel_path = tool_dir.relative_to(output_path)
            content_file = tool_dir / "content.md"
            metadata_file = tool_dir / "metadata.json"
            
            status = "âœ…" if content_file.exists() and metadata_file.exists() else "âš ï¸"
            print(f"   {status} {rel_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="GitHub Trending ç»“æ„åŒ–çˆ¬è™«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å®Œæ•´çˆ¬å–æ‰€æœ‰æ—¶é—´ç»´åº¦
  python run_structured_crawler.py

  # åªçˆ¬å–æ¯æ—¥trending
  python run_structured_crawler.py --time-range daily

  # è‡ªå®šä¹‰è¾“å‡ºç›®å½•
  python run_structured_crawler.py --output my_data

  # æ˜¾ç¤ºç›®å½•ç»“æ„
  python run_structured_crawler.py --show-structure

ç‰¹æ€§è¯´æ˜:
  ğŸ“… æ—¶é—´ç»´åº¦: daily, weekly, monthly
  ğŸ” ç¼–ç¨‹è¯­è¨€: python, javascript, typescript, all
  ğŸ”„ è‡ªåŠ¨å»é‡: é¿å…é‡å¤çˆ¬å–ç›¸åŒé¡¹ç›®
  ğŸ“ ç»“æ„åŒ–å­˜å‚¨: æ¯ä¸ªå·¥å…·å•ç‹¬ç›®å½•
  ğŸ† æ’è¡Œæ¦œ: æŒ‰è´¨é‡åˆ†æ•°æ’åº
        """
    )
    
    parser.add_argument(
        '--time-range', '-t',
        type=str,
        choices=['daily', 'weekly', 'monthly', 'all'],
        default='all',
        help='æ—¶é—´èŒƒå›´ (é»˜è®¤: all - çˆ¬å–æ‰€æœ‰æ—¶é—´ç»´åº¦)'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='crawled_data',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: crawled_data)'
    )
    
    parser.add_argument(
        '--show-structure', '-s',
        action='store_true',
        help='æ˜¾ç¤ºè¾“å‡ºç›®å½•ç»“æ„'
    )
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºç›®å½•ç»“æ„
    if args.show_structure:
        show_directory_structure(args.output)
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # æ‰§è¡Œçˆ¬å–ä»»åŠ¡
    try:
        if args.time_range == 'all':
            # å®Œæ•´çˆ¬å–
            results = asyncio.run(run_full_crawl(args.output))
        else:
            # å•ä¸€æ—¶é—´ç»´åº¦çˆ¬å–
            results = asyncio.run(run_single_time_range(args.time_range, args.output))
        
        if results:
            print(f"\nğŸŠ ä»»åŠ¡å®Œæˆ! æ•°æ®å·²ä¿å­˜åˆ°: {output_path.absolute()}")
            print("\nğŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:")
            print(f"   æŸ¥çœ‹ç›®å½•ç»“æ„: python {sys.argv[0]} --show-structure")
            print(f"   æŸ¥çœ‹æ’è¡Œæ¦œ: ls {args.output}/rankings/")
            print(f"   æŸ¥çœ‹å·¥å…·è¯¦æƒ…: ls {args.output}/tools/")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
