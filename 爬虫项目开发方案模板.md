# 🕷️ 网站爬虫项目开发方案模板

> 基于Claude Newsroom爬虫项目总结的标准化开发流程模板，适用于各类网站爬虫项目

## 📋 模板说明

本模板基于实际成功案例（Claude Newsroom爬虫）制定，包含从需求分析到代码实现的完整开发流程。遵循此模板可确保项目的标准化、可维护性和可扩展性。

---

## 🎯 第一阶段：需求分析与方案制定

### 1.1 需求澄清
- **目标网站**: {网站URL}
- **数据类型**: {新闻/文章/产品/数据等}
- **核心功能**: 
  - [ ] 数据爬取
  - [ ] 媒体文件下载
  - [ ] 本地存储
  - [ ] 上传到存储系统
  - [ ] 定时调度
- **数据量级**: {预估数据量}
- **更新频率**: {每日/每周/实时}

### 1.2 技术选型决策
```yaml
爬虫技术:
  - HTML解析: BeautifulSoup4 + requests (稳定性优先)
  - JS渲染: Playwright (复杂页面)
  - 异步处理: asyncio (高性能需求)

数据存储:
  - 本地存储: 标准化目录结构
  - 对象存储: MinIO集成
  - 数据库: PostgreSQL元数据
  - 搜索引擎: Elasticsearch索引

定时调度:
  - Linux: Cron
  - Windows: 任务计划程序
  - 高级: APScheduler
```

### 1.3 目录结构设计
```
{项目名}/
├── 📄 spider.py                    # 主爬虫脚本
├── 📄 uploader.py                  # 上传脚本
├── 📄 run_crawler.py               # 一键运行脚本
├── 📄 config.json                  # 配置文件
├── 📄 requirements.txt             # 依赖包
├── 📄 README.md                    # 说明文档
└── 📊 crawled_data/                # 本地数据存储
    └── {数据类型}/                 # 数据分类目录
        └── {唯一标识}/             # 按ID/slug组织
            ├── content.md          # Markdown内容
            ├── metadata.json       # 元数据JSON
            └── media/              # 媒体文件目录
```

---

## 🔍 第二阶段：目标网站分析

### 2.1 页面结构分析工具

#### 2.1.1 使用Playwright分析
```python
# 1. 导航到目标页面
browser.navigate(target_url)

# 2. 获取页面快照
browser.snapshot()

# 3. 分析页面元素
browser.evaluate("页面分析JavaScript")

# 4. 检查网络请求
browser.network_requests()
```

#### 2.1.2 关键信息提取
```yaml
页面架构分析:
  - 列表页面: {URL模式和选择器}
  - 详情页面: {URL模式和内容结构}
  - 分页机制: {翻页方式}
  - 媒体文件: {图片/视频等资源}

技术架构识别:
  - 前端框架: {React/Vue/原生等}
  - 渲染方式: {SSR/CSR/静态}
  - API接口: {REST/GraphQL/无}
  - 反爬措施: {验证码/限频/JS检测}
```

### 2.2 数据提取策略
```yaml
CSS选择器规划:
  列表页面:
    - 条目链接: "a[href*='/article/']"
    - 标题: ".title"
    - 分类: ".category"
    - 日期: ".date"
    - 摘要: ".summary"
  
  详情页面:
    - 标题: "h1.title"
    - 正文: ".content"
    - 图片: "img"
    - 元数据: ".meta"
```

---

## 🛠️ 第三阶段：核心代码开发

### 3.1 爬虫主脚本 (spider.py)

#### 3.1.1 基础架构
```python
class {WebsiteName}Spider:
    def __init__(self, config_file="config.json"):
        """初始化爬虫"""
        self.base_url = "{网站基础URL}"
        self.target_url = "{目标页面URL}"
        self.session = requests.Session()
        self.config = self.load_config(config_file)
        self.setup_directories()
        
    def setup_directories(self):
        """创建必要目录"""
        
    def get_article_list(self):
        """获取文章列表"""
        
    def crawl_article(self, article_url):
        """爬取单篇文章"""
        
    def download_media(self, media_url, save_path):
        """下载媒体文件"""
        
    def save_article(self, data, article_id):
        """保存文章数据"""
        
    def run(self, max_articles=None):
        """执行爬虫"""
```

#### 3.1.2 核心功能实现
```python
# 数据提取
def extract_data(self, soup, url):
    """提取页面数据"""
    return {
        'title': soup.select_one('h1').get_text().strip(),
        'content': self.clean_content(soup.select_one('.content')),
        'date': self.parse_date(soup.select_one('.date')),
        'category': soup.select_one('.category').get_text().strip(),
        'images': self.extract_images(soup),
        'url': url
    }

# 增量更新检查
def is_article_exists(self, article_id):
    """检查文章是否已存在"""
    return Path(f"crawled_data/{data_type}/{article_id}").exists()

# 错误处理和重试
def safe_request(self, url, max_retries=3):
    """安全的HTTP请求"""
```

### 3.2 上传脚本 (uploader.py)

#### 3.2.1 存储集成架构
```python
class {WebsiteName}Uploader:
    def __init__(self, config_file="config.json"):
        """初始化上传器"""
        
    def upload_to_minio(self, file_path, object_name):
        """上传到MinIO对象存储"""
        
    def save_to_postgresql(self, metadata):
        """保存元数据到PostgreSQL"""
        
    def index_to_elasticsearch(self, data):
        """建立Elasticsearch索引"""
        
    def process_directory(self, data_dir):
        """批量处理目录"""
```

### 3.3 配置文件 (config.json)
```json
{
  "crawler": {
    "delay": 2,
    "timeout": 30,
    "max_retries": 3,
    "max_articles": 50,
    "user_agent": "标准UserAgent"
  },
  "media": {
    "download_images": true,
    "download_videos": false,
    "image_timeout": 15,
    "max_file_size": 10485760,
    "allowed_extensions": [".jpg", ".png", ".gif"]
  },
  "storage": {
    "data_dir": "crawled_data",
    "data_type": "{数据类型名称}",
    "create_subdirs": true
  },
  "filter": {
    "skip_duplicates": true,
    "min_content_length": 100,
    "categories": []
  },
  "minio": {
    "bucket_name": "{项目名称}",
    "api_endpoint": "localhost:9011"
  }
}
```

---

## 📊 第四阶段：数据格式标准化

### 4.1 Markdown内容格式 (content.md)
```markdown
# {文章标题}

**分类**: {分类}  
**发布日期**: {日期}  
**来源**: {原始URL}  
**爬取时间**: {时间戳}

---

{正文内容}

## 相关媒体

![{描述}](media/{文件名})
```

### 4.2 元数据格式 (metadata.json)
```json
{
  "url": "原始URL",
  "title": "文章标题",
  "category": "分类",
  "date": "发布日期",
  "content": "正文内容",
  "images": [
    {
      "url": "原始图片URL",
      "alt": "图片描述",
      "filename": "本地文件名",
      "local_path": "本地路径"
    }
  ],
  "crawl_time": "爬取时间戳",
  "slug": "文章标识符",
  "word_count": 字数统计,
  "content_hash": "内容哈希"
}
```

---

## 🚀 第五阶段：部署与运维

### 5.1 依赖管理 (requirements.txt)
```
# 核心爬虫依赖
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0

# 数据处理
python-dateutil>=2.8.0

# 可选高级功能
# playwright>=1.40.0  # 复杂页面
# aiohttp>=3.9.0      # 异步请求
```

### 5.2 运行脚本模板

#### 5.2.1 一键运行脚本 (run_crawler.py)
```python
#!/usr/bin/env python3
def main():
    parser = argparse.ArgumentParser(description='{项目名称} 一键爬虫')
    parser.add_argument('--crawl-only', action='store_true')
    parser.add_argument('--upload-only', action='store_true')
    parser.add_argument('--max', type=int, default=20)
    parser.add_argument('--force', action='store_true')
    
    args = parser.parse_args()
    
    if not args.upload_only:
        # 执行爬取
        pass
        
    if not args.crawl_only:
        # 执行上传
        pass
```

#### 5.2.2 定时任务配置
```bash
# Linux Cron
0 8 * * * cd /path/to/project && python run_crawler.py --max 20

# Windows 批处理
cd /d "D:\path\to\project"
python run_crawler.py --max 20
```

### 5.3 README文档模板

#### 5.3.1 文档结构
```markdown
# 🤖 {网站名称} 爬虫

## 📋 项目概述
- 🎯 核心功能
- 🌐 数据源
- ✅ 已验证功能

## 🗂️ 项目目录结构
{标准目录树}

## 🚀 使用方式
- 快速开始
- 基础命令
- 高级参数
- 实际运行示例

## ⚙️ 配置文件
{完整配置说明}

## 📊 数据格式
{数据格式示例}

## 🔄 定时任务设置
{定时任务配置}

## 🔧 存储架构集成
{MinIO集成说明}

## 🛠️ 技术特点
{技术优势说明}

## 🐛 常见问题
{FAQ列表}
```

---

## ✅ 第六阶段：测试与验证

### 6.1 功能测试清单
- [ ] **基础爬取**: 能否成功访问目标网站
- [ ] **数据提取**: 各字段是否正确提取
- [ ] **媒体下载**: 图片等文件是否正常下载
- [ ] **增量更新**: 重复运行是否跳过已有数据
- [ ] **错误处理**: 网络异常时是否优雅处理
- [ ] **数据格式**: 生成的文件格式是否标准
- [ ] **存储集成**: 能否正常上传到MinIO系统

### 6.2 测试命令
```bash
# 小规模测试
python spider.py --max 3

# 功能验证
python run_crawler.py --crawl-only --max 5

# 上传测试
python uploader.py

# 完整流程测试
python run_crawler.py --max 10
```

---

## 🎯 第七阶段：项目总结

### 7.1 项目状态文档
```yaml
开发状态: ✅ 完成/🔄 进行中/❌ 待开始
核心功能: ✅ 已实现并验证
存储集成: ✅ 已集成MinIO架构
数据质量: ✅ 格式标准化
部署状态: ✅ 支持定时任务
文档完整性: ✅ 用户手册齐全
```

### 7.2 成果交付清单
- [ ] **代码文件**: spider.py, uploader.py, run_crawler.py
- [ ] **配置文件**: config.json, requirements.txt
- [ ] **文档**: README.md, 使用说明
- [ ] **测试数据**: 验证爬取的示例数据
- [ ] **运行日志**: 成功运行的日志记录

---

## 🔧 开发最佳实践

### 8.1 代码规范
```python
# 类命名: 网站名称 + Spider/Uploader
class TwitterSpider:
class RedditUploader:

# 方法命名: 动词开头，功能明确
def get_article_list():
def crawl_article():
def save_to_database():

# 配置管理: 统一config.json
config = self.load_config(config_file)
```

### 8.2 错误处理模式
```python
# 网络请求重试
@retry(max_attempts=3, delay=2)
def safe_request(self, url):
    try:
        response = self.session.get(url, timeout=30)
        response.raise_for_status()
        return response
    except requests.RequestException as e:
        logger.error(f"请求失败: {url}, 错误: {e}")
        raise

# 数据解析容错
def safe_extract(self, soup, selector, default=""):
    try:
        element = soup.select_one(selector)
        return element.get_text().strip() if element else default
    except Exception as e:
        logger.warning(f"提取失败: {selector}, 错误: {e}")
        return default
```

### 8.3 日志记录标准
```python
import logging

# 日志配置
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# 日志使用
logger.info(f"开始爬取: {url}")
logger.warning(f"跳过已存在文章: {title}")
logger.error(f"爬取失败: {url}, 错误: {e}")
```

---

## 📈 项目扩展指南

### 9.1 高级功能扩展
```python
# 并发爬取
import asyncio
import aiohttp

# 反爬虫对抗
from fake_useragent import UserAgent
import time
import random

# 数据清洗
import re
from markdownify import markdownify

# 监控告警
import smtplib
from email.mime.text import MIMEText
```

### 9.2 多网站架构
```
爬虫系统/
├── shared/                         # 共享模块
│   ├── config.py                  # 通用配置
│   ├── storage.py                 # 存储抽象
│   └── utils.py                   # 工具函数
├── spiders/                       # 爬虫模块
│   ├── twitter/
│   ├── reddit/
│   └── {网站名}/
└── templates/                     # 模板文件
    └── spider_template.py
```

---

## 🎯 使用此模板的步骤

1. **复制模板目录结构**
2. **替换所有 `{变量}` 为实际值**
3. **使用Playwright分析目标网站**
4. **根据分析结果调整CSS选择器**
5. **测试爬虫基础功能**
6. **完善错误处理和边界情况**
7. **编写完整的README文档**
8. **进行端到端测试验证**

---

## 📋 模板变量对照表

| 变量名 | 说明 | 示例 |
|--------|------|------|
| `{网站名称}` | 目标网站名称 | Claude Newsroom |
| `{项目名}` | 项目目录名 | Claude_newsroom |
| `{网站URL}` | 目标网站地址 | https://www.anthropic.com/news |
| `{数据类型}` | 数据分类名称 | articles, products, posts |
| `{WebsiteName}` | 类名前缀 | ClaudeNewsroom, TwitterTrends |

---

**模板版本**: v1.0  
**基于项目**: Claude Newsroom爬虫  
**更新时间**: 2025-01-17  
**适用范围**: 通用网站爬虫项目

---

> 💡 **提示**: 此模板已在Claude Newsroom项目中验证可行，遵循此模板开发可确保项目的标准化和可维护性。每个新项目都应该严格按照此模板的步骤和规范进行开发。
