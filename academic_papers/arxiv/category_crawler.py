#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXivåˆ†ç±»çˆ¬è™«ä¸“ç”¨è„šæœ¬
ä¸“é—¨ç”¨äºæŒ‰åˆ†ç±»æ‰¹é‡çˆ¬å–è®ºæ–‡
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arxiv_system.crawler.arxiv_crawler import ArxivCrawler
from arxiv_system.utils.file_utils import setup_logging, load_config, ensure_directory, save_json


class CategoryCrawler:
    """æŒ‰åˆ†ç±»çˆ¬å–arXivè®ºæ–‡çš„ä¸“ç”¨çˆ¬è™«"""
    
    def __init__(self, config_path: str = "config.json"):
        """åˆå§‹åŒ–åˆ†ç±»çˆ¬è™«
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = load_config(config_path)
        self.logger = logging.getLogger(__name__)
        
    async def crawl_by_categories(self, categories: List[str], 
                                 max_per_category: int = 200,
                                 concurrent: int = 3,
                                 download_pdf: bool = True,
                                 output_base_dir: str = "crawled_data") -> Dict[str, Any]:
        """æŒ‰åˆ†ç±»çˆ¬å–è®ºæ–‡
        
        Args:
            categories: åˆ†ç±»åˆ—è¡¨ï¼Œå¦‚ ["cs.AI", "cs.LG"]
            max_per_category: æ¯ä¸ªåˆ†ç±»æœ€å¤§çˆ¬å–æ•°é‡
            concurrent: å¹¶å‘æ•°é‡
            download_pdf: æ˜¯å¦ä¸‹è½½PDF
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
            
        Returns:
            çˆ¬å–ç»“æœç»Ÿè®¡
        """
        self.logger.info(f"ğŸš€ å¼€å§‹æŒ‰åˆ†ç±»çˆ¬å–arXivè®ºæ–‡")
        self.logger.info(f"ğŸ“‚ åˆ†ç±»åˆ—è¡¨: {categories}")
        self.logger.info(f"ğŸ“Š æ¯åˆ†ç±»æœ€å¤§æ•°é‡: {max_per_category}")
        
        # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = os.path.join(output_base_dir, "by_category", timestamp)
        ensure_directory(output_dir)
        
        results = {
            "crawl_info": {
                "start_time": datetime.now().isoformat(),
                "categories": categories,
                "max_per_category": max_per_category,
                "concurrent": concurrent,
                "download_pdf": download_pdf
            },
            "category_results": {},
            "summary": {
                "total_papers": 0,
                "successful_categories": 0,
                "failed_categories": 0
            }
        }
        
        # é€ä¸ªåˆ†ç±»çˆ¬å–
        for i, category in enumerate(categories, 1):
            self.logger.info(f"ğŸ“‘ [{i}/{len(categories)}] å¼€å§‹çˆ¬å–åˆ†ç±»: {category}")
            
            try:
                # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                query = f"cat:{category}"
                
                # åˆ›å»ºåˆ†ç±»ä¸“ç”¨ç›®å½•
                category_dir = os.path.join(output_dir, category.replace(".", "_"))
                ensure_directory(category_dir)
                
                # æ‰§è¡Œçˆ¬å–
                async with ArxivCrawler(self.config) as crawler:
                    papers = await crawler.search_papers(
                        query=query, 
                        max_results=max_per_category
                    )
                    
                    if papers:
                        self.logger.info(f"âœ… {category}: æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡ï¼Œå¼€å§‹å¤„ç†...")
                        
                        # å¤„ç†è®ºæ–‡æ•°æ®
                        processed_papers = []
                        articles_dir = os.path.join(category_dir, "articles")
                        ensure_directory(articles_dir)
                        
                        for paper in papers:
                            try:
                                processed_paper = await crawler._process_single_paper(
                                    paper, articles_dir, download_pdf
                                )
                                processed_papers.append(processed_paper)
                            except Exception as e:
                                self.logger.error(f"âŒ å¤„ç†è®ºæ–‡å¤±è´¥: {paper.get('arxiv_id', 'unknown')} - {e}")
                        
                        # ä¿å­˜åˆ†ç±»æ•°æ®
                        await self._save_category_data(processed_papers, category_dir, category)
                        
                        # è®°å½•ç»“æœ
                        results["category_results"][category] = {
                            "total_found": len(papers),
                            "successfully_processed": len(processed_papers),
                            "output_dir": category_dir,
                            "status": "success"
                        }
                        results["summary"]["total_papers"] += len(processed_papers)
                        results["summary"]["successful_categories"] += 1
                        
                        self.logger.info(f"ğŸ‰ {category}: æˆåŠŸå¤„ç† {len(processed_papers)} ç¯‡è®ºæ–‡")
                    else:
                        self.logger.warning(f"âš ï¸ {category}: æœªæ‰¾åˆ°è®ºæ–‡")
                        results["category_results"][category] = {
                            "total_found": 0,
                            "successfully_processed": 0,
                            "output_dir": category_dir,
                            "status": "no_papers"
                        }
                        
            except Exception as e:
                self.logger.error(f"âŒ åˆ†ç±» {category} çˆ¬å–å¤±è´¥: {e}")
                results["category_results"][category] = {
                    "error": str(e),
                    "status": "failed"
                }
                results["summary"]["failed_categories"] += 1
            
            # åˆ†ç±»é—´å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡é¢‘
            if i < len(categories):
                await asyncio.sleep(2)
        
        # ä¿å­˜æ€»ä½“ç»“æœ
        results["crawl_info"]["end_time"] = datetime.now().isoformat()
        results_file = os.path.join(output_dir, "crawl_results.json")
        save_json(results, results_file)
        
        # ç”Ÿæˆç®€è¦æŠ¥å‘Š
        self._generate_summary_report(results, output_dir)
        
        self.logger.info(f"ğŸŠ åˆ†ç±»çˆ¬å–å®Œæˆï¼")
        self.logger.info(f"ğŸ“Š æ€»è®¡çˆ¬å–: {results['summary']['total_papers']} ç¯‡è®ºæ–‡")
        self.logger.info(f"âœ… æˆåŠŸåˆ†ç±»: {results['summary']['successful_categories']}/{len(categories)}")
        self.logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        
        return results
    
    async def _save_category_data(self, papers: List[Dict], category_dir: str, category: str):
        """ä¿å­˜åˆ†ç±»æ•°æ®
        
        Args:
            papers: è®ºæ–‡æ•°æ®åˆ—è¡¨
            category_dir: åˆ†ç±»ç›®å½•
            category: åˆ†ç±»åç§°
        """
        # ä¿å­˜è®ºæ–‡åˆ—è¡¨
        papers_file = os.path.join(category_dir, f"{category.replace('.', '_')}_papers.json")
        save_json(papers, papers_file)
        
        # ç”Ÿæˆåˆ†ç±»ç»Ÿè®¡
        stats = {
            "category": category,
            "total_papers": len(papers),
            "crawl_date": datetime.now().isoformat(),
            "authors": list(set(author for paper in papers for author in paper.get('authors', []))),
            "date_range": {
                "earliest": min(paper.get('published', '') for paper in papers if paper.get('published')),
                "latest": max(paper.get('published', '') for paper in papers if paper.get('published'))
            } if papers else {}
        }
        
        stats_file = os.path.join(category_dir, f"{category.replace('.', '_')}_stats.json")
        save_json(stats, stats_file)
    
    def _generate_summary_report(self, results: Dict, output_dir: str):
        """ç”Ÿæˆç®€è¦æŠ¥å‘Š
        
        Args:
            results: çˆ¬å–ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        report_lines = [
            "# arXivåˆ†ç±»çˆ¬å–æŠ¥å‘Š",
            "",
            f"**çˆ¬å–æ—¶é—´**: {results['crawl_info']['start_time']} - {results['crawl_info']['end_time']}",
            f"**æ€»è®¡è®ºæ–‡**: {results['summary']['total_papers']} ç¯‡",
            f"**æˆåŠŸåˆ†ç±»**: {results['summary']['successful_categories']}/{len(results['crawl_info']['categories'])}",
            "",
            "## åˆ†ç±»è¯¦æƒ…",
            ""
        ]
        
        for category, result in results["category_results"].items():
            if result["status"] == "success":
                report_lines.append(f"- **{category}**: {result['successfully_processed']} ç¯‡è®ºæ–‡ âœ…")
            elif result["status"] == "no_papers":
                report_lines.append(f"- **{category}**: æœªæ‰¾åˆ°è®ºæ–‡ âš ï¸")
            else:
                report_lines.append(f"- **{category}**: çˆ¬å–å¤±è´¥ âŒ")
        
        report_content = "\n".join(report_lines)
        report_file = os.path.join(output_dir, "README.md")
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)


async def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # å®šä¹‰è¦çˆ¬å–çš„åˆ†ç±»
    TARGET_CATEGORIES = [
        "cs.AI",   # äººå·¥æ™ºèƒ½
        "cs.LG",   # æœºå™¨å­¦ä¹ 
        "cs.CL",   # è®¡ç®—ä¸è¯­è¨€
        "cs.CV",   # è®¡ç®—æœºè§†è§‰
        "cs.NE"    # ç¥ç»å’Œè¿›åŒ–è®¡ç®—
    ]
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = CategoryCrawler()
    
    # æ‰§è¡Œåˆ†ç±»çˆ¬å–
    results = await crawler.crawl_by_categories(
        categories=TARGET_CATEGORIES,
        max_per_category=300,  # æ¯ä¸ªåˆ†ç±»æœ€å¤š300ç¯‡
        concurrent=3,          # å¹¶å‘æ•°
        download_pdf=True,     # ä¸‹è½½PDF
        output_base_dir="crawled_data"
    )
    
    print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼å…±è·å– {results['summary']['total_papers']} ç¯‡è®ºæ–‡")
    print(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: crawled_data/by_category/")


if __name__ == "__main__":
    asyncio.run(main())
