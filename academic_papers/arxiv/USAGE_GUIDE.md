# ğŸ“– å¢å¼ºç‰ˆ arXiv çˆ¬è™«ä½¿ç”¨æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ5åˆ†é’Ÿï¼‰

### ç¬¬ä¸€æ­¥ï¼šå®‰è£…ä¾èµ–
```bash
cd academic_papers/arxiv
pip install -r requirements.txt
```

### ç¬¬äºŒæ­¥ï¼šæŸ¥çœ‹ç³»ç»Ÿä¿¡æ¯
```bash
# æŸ¥çœ‹æ”¯æŒçš„æ‰€æœ‰AIåˆ†ç±»
python main_enhanced_simple.py info --categories

# æŸ¥çœ‹é»˜è®¤çƒ­é—¨å…³é”®è¯
python main_enhanced_simple.py info --keywords
```

### ç¬¬ä¸‰æ­¥ï¼šæµ‹è¯•çˆ¬å–åŠŸèƒ½
```bash
# å°è§„æ¨¡æµ‹è¯•ï¼šçˆ¬å–cs.AIåˆ†ç±»æœ€è¿‘3å¤©çš„è®ºæ–‡
python main_enhanced_simple.py daily --days 3 --categories cs.AI
```

### ç¬¬å››æ­¥ï¼šæŸ¥çœ‹ç»“æœ
```bash
# æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶
ls crawled_data/daily/*/

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
cat crawled_data/daily/*/daily_stats.json
```

## ğŸ¯ å¸¸ç”¨ä½¿ç”¨åœºæ™¯

### åœºæ™¯1ï¼šæ¯æ—¥AIæ–°é—»è·å–
```bash
# è·å–æ˜¨å¤©çš„æ‰€æœ‰AIæ–°è®ºæ–‡
python main_enhanced_simple.py daily --days 1

# è·å–æœ€è¿‘ä¸€å‘¨çš„AIè®ºæ–‡
python main_enhanced_simple.py daily --days 7
```

### åœºæ™¯2ï¼šä¸“ä¸šæ–¹å‘ç ”ç©¶
```bash
# ä¸“æ³¨è®¡ç®—æœºè§†è§‰
python main_enhanced_simple.py category --categories cs.CV --max-per-category 500

# ä¸“æ³¨è‡ªç„¶è¯­è¨€å¤„ç†
python main_enhanced_simple.py category --categories cs.CL cs.AI --max-per-category 300

# ä¸“æ³¨æœºå™¨å­¦ä¹ ç†è®º
python main_enhanced_simple.py category --categories cs.LG stat.ML --max-per-category 400
```

### åœºæ™¯3ï¼šçƒ­ç‚¹æŠ€æœ¯è¿½è¸ª
```bash
# è¿½è¸ªTransformeræŠ€æœ¯å‘å±•
python main_enhanced_simple.py keyword --keywords "transformer" "attention" --days 14

# è¿½è¸ªå¤§è¯­è¨€æ¨¡å‹è¿›å±•
python main_enhanced_simple.py keyword --keywords "LLM" "large language model" --days 7

# è¿½è¸ªæ‰©æ•£æ¨¡å‹ç ”ç©¶
python main_enhanced_simple.py keyword --keywords "diffusion" "stable diffusion" --days 10
```

### åœºæ™¯4ï¼šå…¨é¢æ•°æ®æ”¶é›†
```bash
# æ”¶é›†æ‰€æœ‰AIç›¸å…³åˆ†ç±»çš„è®ºæ–‡ï¼ˆå¤§é‡æ•°æ®ï¼‰
python main_enhanced_simple.py category --max-per-category 1000

# æ”¶é›†æ‰€æœ‰çƒ­é—¨å…³é”®è¯çš„è®ºæ–‡
python main_enhanced_simple.py keyword --days 14
```

## ğŸ“Š è¾“å‡ºæ•°æ®è¯´æ˜

### æ¯æ—¥çˆ¬å–è¾“å‡º
```
crawled_data/daily/2025-08-10/
â”œâ”€â”€ daily_papers_all.json          # æ‰€æœ‰è®ºæ–‡çš„å®Œæ•´ä¿¡æ¯
â”œâ”€â”€ daily_papers_by_category.json  # æŒ‰åˆ†ç±»ç»„ç»‡çš„è®ºæ–‡
â””â”€â”€ daily_stats.json               # çˆ¬å–ç»Ÿè®¡ä¿¡æ¯
```

### å…³é”®è¯çˆ¬å–è¾“å‡º
```
crawled_data/weekly/2025-W32/
â”œâ”€â”€ weekly_papers_by_keyword.json      # æŒ‰å…³é”®è¯ç»„ç»‡çš„è®ºæ–‡
â”œâ”€â”€ weekly_papers_deduplicated.json    # å»é‡åçš„è®ºæ–‡é›†åˆ
â””â”€â”€ weekly_stats.json                  # çˆ¬å–ç»Ÿè®¡ä¿¡æ¯
```

### åˆ†ç±»çˆ¬å–è¾“å‡º
```
crawled_data/category/
â”œâ”€â”€ papers_by_category.json    # æŒ‰åˆ†ç±»ç»„ç»‡çš„è®ºæ–‡
â”œâ”€â”€ all_papers.json           # æ‰€æœ‰è®ºæ–‡åˆé›†
â””â”€â”€ category_stats.json       # åˆ†ç±»ç»Ÿè®¡ä¿¡æ¯
```

## ğŸ”§ é…ç½®ä¼˜åŒ–

### è°ƒæ•´çˆ¬å–æ•°é‡
ç¼–è¾‘ `config_enhanced.json`:
```json
{
  "crawl_strategies": {
    "daily_new": {
      "max_results": 1000,  // æ¯æ—¥æœ€å¤§çˆ¬å–æ•°é‡
      "concurrent": 5       // å¹¶å‘æ•°é‡
    }
  }
}
```

### æ·»åŠ è‡ªå®šä¹‰å…³é”®è¯
```json
{
  "crawler": {
    "trending_keywords": [
      "ä½ çš„å…³é”®è¯1",
      "ä½ çš„å…³é”®è¯2",
      "transformer",
      "diffusion"
    ]
  }
}
```

### è‡ªå®šä¹‰åˆ†ç±»ç»„åˆ
```json
{
  "crawler": {
    "ai_categories": {
      "my_focus": ["cs.AI", "cs.LG"],
      "nlp_research": ["cs.CL"],
      "vision_research": ["cs.CV"]
    }
  }
}
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ§åˆ¶çˆ¬å–é¢‘ç‡
```bash
# å°é‡æµ‹è¯•ï¼ˆé€‚åˆæ—¥å¸¸ä½¿ç”¨ï¼‰
python main_enhanced_simple.py daily --days 1 --categories cs.AI

# ä¸­é‡çˆ¬å–ï¼ˆé€‚åˆå‘¨åº¦æ›´æ–°ï¼‰
python main_enhanced_simple.py daily --days 7

# å¤§é‡çˆ¬å–ï¼ˆé€‚åˆåˆå§‹åŒ–æ•°æ®ï¼‰
python main_enhanced_simple.py category --max-per-category 1000
```

### 2. åˆç†è®¾ç½®æ—¶é—´èŒƒå›´
- **æ¯æ—¥çˆ¬å–**: 1-3å¤©å›æº¯ï¼ˆè·å–æœ€æ–°è®ºæ–‡ï¼‰
- **å…³é”®è¯çˆ¬å–**: 7-14å¤©å›æº¯ï¼ˆè·å–è¶³å¤Ÿæ ·æœ¬ï¼‰
- **åˆ†ç±»çˆ¬å–**: ä¸é™æ—¶é—´ï¼ˆè·å–å…¨é‡æ•°æ®ï¼‰

### 3. åˆ†æ‰¹å¤„ç†å¤§é‡æ•°æ®
```bash
# åˆ†åˆ«çˆ¬å–ä¸åŒåˆ†ç±»ï¼Œé¿å…ä¸€æ¬¡è¯·æ±‚è¿‡å¤š
python main_enhanced_simple.py category --categories cs.AI --max-per-category 500
python main_enhanced_simple.py category --categories cs.LG --max-per-category 500
python main_enhanced_simple.py category --categories cs.CL --max-per-category 500
```

### 4. å®šæœŸæ¸…ç†æ—§æ•°æ®
```bash
# æ‰‹åŠ¨æ¸…ç†30å¤©å‰çš„æ•°æ®
find crawled_data/daily -type d -mtime +30 -exec rm -rf {} \;
```

## âš ï¸ å¸¸è§é—®é¢˜

### Q1: çˆ¬å–ç»“æœä¸º0ç¯‡è®ºæ–‡ï¼Ÿ
**A**: å¯èƒ½æ˜¯æ—¶é—´èŒƒå›´å¤ªå°ï¼Œå°è¯•å¢åŠ  `--days` å‚æ•°ï¼š
```bash
# ä»1å¤©å¢åŠ åˆ°7å¤©
python main_enhanced_simple.py daily --days 7 --categories cs.AI
```

### Q2: çˆ¬å–é€Ÿåº¦æ…¢ï¼Ÿ
**A**: å¯ä»¥è°ƒæ•´å¹¶å‘å‚æ•°ï¼ˆåœ¨é…ç½®æ–‡ä»¶ä¸­ï¼‰ï¼š
```json
{
  "crawler": {
    "max_concurrent_papers": 5,  // å¢åŠ å¹¶å‘æ•°
    "request_delay": 1           // å‡å°‘å»¶è¿Ÿï¼ˆå°å¿ƒä½¿ç”¨ï¼‰
  }
}
```

### Q3: å†…å­˜å ç”¨é«˜ï¼Ÿ
**A**: å‡å°‘å•æ¬¡çˆ¬å–æ•°é‡ï¼š
```bash
# ä»1000å‡å°‘åˆ°200
python main_enhanced_simple.py category --max-per-category 200
```

### Q4: ç½‘ç»œè¿æ¥å¤±è´¥ï¼Ÿ
**A**: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–å¢åŠ é‡è¯•æ¬¡æ•°ï¼š
```json
{
  "crawler": {
    "max_retries": 5,  // å¢åŠ é‡è¯•æ¬¡æ•°
    "timeout": 60      // å¢åŠ è¶…æ—¶æ—¶é—´
  }
}
```

## ğŸ“ˆ æ•°æ®åˆ†æç¤ºä¾‹

### ä½¿ç”¨Pythonåˆ†æçˆ¬å–æ•°æ®
```python
import json
from collections import Counter

# è¯»å–æ¯æ—¥çˆ¬å–æ•°æ®
with open('crawled_data/daily/2025-08-10/daily_papers_all.json', 'r') as f:
    papers = json.load(f)

# åˆ†æä½œè€…åˆ†å¸ƒ
authors = []
for paper in papers:
    authors.extend(paper.get('authors', []))

author_counts = Counter(authors)
print("é«˜äº§ä½œè€…Top 10:")
for author, count in author_counts.most_common(10):
    print(f"{author}: {count}ç¯‡")

# åˆ†æåˆ†ç±»åˆ†å¸ƒ
categories = []
for paper in papers:
    categories.extend(paper.get('categories', []))

category_counts = Counter(categories)
print("\nçƒ­é—¨åˆ†ç±»Top 10:")
for category, count in category_counts.most_common(10):
    print(f"{category}: {count}ç¯‡")
```

### ç”Ÿæˆç®€å•æŠ¥å‘Š
```python
import json
from datetime import datetime

def generate_daily_report(date_str):
    """ç”Ÿæˆæ¯æ—¥çˆ¬å–æŠ¥å‘Š"""
    stats_file = f'crawled_data/daily/{date_str}/daily_stats.json'
    
    with open(stats_file, 'r') as f:
        stats = json.load(f)
    
    print(f"ğŸ“Š {date_str} AIè®ºæ–‡çˆ¬å–æŠ¥å‘Š")
    print("=" * 40)
    print(f"æ€»è®ºæ–‡æ•°: {stats['total_papers']}")
    print(f"æ—¶é—´èŒƒå›´: {stats['date_range']['earliest']} - {stats['date_range']['latest']}")
    print(f"çˆ¬å–æ—¶é—´: {stats['crawl_date']}")
    
    if 'categories_stats' in stats:
        print("\nåˆ†ç±»ç»Ÿè®¡:")
        for category, count in stats['categories_stats'].items():
            print(f"  {category}: {count}ç¯‡")

# ä½¿ç”¨ç¤ºä¾‹
generate_daily_report('2025-08-10')
```

## ğŸ¯ è¿›é˜¶ç”¨æ³•

### 1. æ‰¹é‡çˆ¬å–è„šæœ¬
```bash
#!/bin/bash
# batch_crawl.sh - æ‰¹é‡çˆ¬å–è„šæœ¬

echo "å¼€å§‹æ‰¹é‡çˆ¬å–AIè®ºæ–‡..."

# æ¯æ—¥æ›´æ–°
python main_enhanced_simple.py daily --days 1

# å…³é”®è¯è¿½è¸ª
python main_enhanced_simple.py keyword --keywords "transformer" "LLM" --days 3

# åˆ†ç±»æ›´æ–°ï¼ˆä»…æ ¸å¿ƒåˆ†ç±»ï¼‰
python main_enhanced_simple.py category --categories cs.AI cs.LG cs.CL --max-per-category 100

echo "æ‰¹é‡çˆ¬å–å®Œæˆï¼"
```

### 2. æ•°æ®åˆå¹¶è„šæœ¬
```python
# merge_data.py - åˆå¹¶å¤šæ¬¡çˆ¬å–çš„æ•°æ®
import json
import glob
from pathlib import Path

def merge_daily_data():
    """åˆå¹¶æ‰€æœ‰æ¯æ—¥çˆ¬å–æ•°æ®"""
    all_papers = []
    
    # è·å–æ‰€æœ‰æ¯æ—¥æ•°æ®æ–‡ä»¶
    daily_files = glob.glob('crawled_data/daily/*/daily_papers_all.json')
    
    for file_path in daily_files:
        with open(file_path, 'r') as f:
            papers = json.load(f)
            all_papers.extend(papers)
    
    # å»é‡ï¼ˆåŸºäºarXiv IDï¼‰
    seen_ids = set()
    unique_papers = []
    
    for paper in all_papers:
        arxiv_id = paper.get('arxiv_id')
        if arxiv_id and arxiv_id not in seen_ids:
            unique_papers.append(paper)
            seen_ids.add(arxiv_id)
    
    # ä¿å­˜åˆå¹¶ç»“æœ
    output_file = 'crawled_data/merged_papers.json'
    with open(output_file, 'w') as f:
        json.dump(unique_papers, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… åˆå¹¶å®Œæˆ: {len(unique_papers)} ç¯‡è®ºæ–‡ â†’ {output_file}")

if __name__ == "__main__":
    merge_daily_data()
```

---

## ğŸ‰ å¼€å§‹ä½¿ç”¨

é€‰æ‹©é€‚åˆä½ éœ€æ±‚çš„å‘½ä»¤ï¼Œå¼€å§‹ä½“éªŒå¢å¼ºç‰ˆarXivçˆ¬è™«çš„å¼ºå¤§åŠŸèƒ½ï¼š

```bash
# ğŸ”¥ æ¨èï¼šæ¯æ—¥AIè®ºæ–‡è·å–
python main_enhanced_simple.py daily --days 3

# ğŸ¯ ç²¾å‡†ï¼šç‰¹å®šæ–¹å‘æ·±å…¥ç ”ç©¶  
python main_enhanced_simple.py category --categories cs.AI --max-per-category 500

# ğŸš€ çƒ­ç‚¹ï¼šè¿½è¸ªå‰æ²¿æŠ€æœ¯å‘å±•
python main_enhanced_simple.py keyword --keywords "transformer" "LLM" --days 7
```

**è®©AIè®ºæ–‡è·å–å˜å¾—ç®€å•é«˜æ•ˆï¼** ğŸ“šâœ¨
