#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆarXivè®ºæ–‡çˆ¬è™«ä¸»ç¨‹åºï¼ˆç®€åŒ–ç‰ˆï¼Œæ— è°ƒåº¦å™¨ï¼‰
æ”¯æŒæ¯æ—¥çˆ¬å–ã€åˆ†ç±»çˆ¬å–ã€å…³é”®è¯çˆ¬å–åŠŸèƒ½
"""

import argparse
import json
import logging
import os
import sys
from pathlib import Path
import asyncio
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arxiv_system.crawler.enhanced_arxiv_crawler import EnhancedArxivCrawler
from arxiv_system.oss.wrapper import OSSUploader
from arxiv_system.utils.file_utils import setup_logging, load_config, save_json, ensure_directory


def setup_argument_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="å¢å¼ºç‰ˆarXivè®ºæ–‡çˆ¬è™«ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æ¯æ—¥çˆ¬å–
  python main_enhanced_simple.py daily --days 1
  python main_enhanced_simple.py daily --days 3 --categories cs.AI cs.LG
  
  # åˆ†ç±»çˆ¬å–
  python main_enhanced_simple.py category --categories cs.AI cs.LG --max-per-category 100
  
  # å…³é”®è¯çˆ¬å–
  python main_enhanced_simple.py keyword --keywords "transformer" "LLM" --days 7
  
  # ä¿¡æ¯æŸ¥çœ‹
  python main_enhanced_simple.py info --categories
  python main_enhanced_simple.py info --keywords
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # æ¯æ—¥çˆ¬å–å‘½ä»¤
    daily_parser = subparsers.add_parser("daily", help="çˆ¬å–æ¯æ—¥æ–°è®ºæ–‡")
    daily_parser.add_argument("--days", type=int, default=1, help="å›æº¯å¤©æ•°")
    daily_parser.add_argument("--categories", nargs="*", help="æŒ‡å®šåˆ†ç±»ï¼ˆå¦‚ï¼šcs.AI cs.LGï¼‰")
    daily_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # åˆ†ç±»çˆ¬å–å‘½ä»¤
    category_parser = subparsers.add_parser("category", help="æŒ‰åˆ†ç±»çˆ¬å–è®ºæ–‡")
    category_parser.add_argument("--categories", nargs="*", help="åˆ†ç±»åˆ—è¡¨ï¼ˆç©º=æ‰€æœ‰AIåˆ†ç±»ï¼‰")
    category_parser.add_argument("--max-per-category", type=int, default=100, help="æ¯ä¸ªåˆ†ç±»æœ€å¤§è®ºæ–‡æ•°")
    category_parser.add_argument("--folders", action="store_true", help="ç”Ÿæˆå®Œæ•´çš„æ–‡ä»¶å¤¹ç»“æ„")
    category_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # å…³é”®è¯çˆ¬å–å‘½ä»¤
    keyword_parser = subparsers.add_parser("keyword", help="æŒ‰å…³é”®è¯çˆ¬å–è®ºæ–‡")
    keyword_parser.add_argument("--keywords", nargs="*", help="å…³é”®è¯åˆ—è¡¨ï¼ˆç©º=ä½¿ç”¨é»˜è®¤çƒ­é—¨å…³é”®è¯ï¼‰")
    keyword_parser.add_argument("--days", type=int, default=7, help="å›æº¯å¤©æ•°")
    keyword_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # ä¿¡æ¯å‘½ä»¤
    info_parser = subparsers.add_parser("info", help="æ˜¾ç¤ºé…ç½®å’Œç»Ÿè®¡ä¿¡æ¯")
    info_parser.add_argument("--categories", action="store_true", help="æ˜¾ç¤ºæ‰€æœ‰åˆ†ç±»")
    info_parser.add_argument("--keywords", action="store_true", help="æ˜¾ç¤ºé»˜è®¤å…³é”®è¯")
    info_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    return parser


async def handle_daily_command(args):
    """å¤„ç†æ¯æ—¥çˆ¬å–å‘½ä»¤"""
    print(f"ğŸŒ… å¼€å§‹æ¯æ—¥çˆ¬å–ä»»åŠ¡ï¼ˆå›æº¯ {args.days} å¤©ï¼‰")
    
    config = load_config(args.config)
    crawler = EnhancedArxivCrawler(config)
    
    async with crawler:
        result = await crawler.crawl_and_save_daily(
            days_back=args.days,
            categories=args.categories
        )
        
        print(f"âœ… æ¯æ—¥çˆ¬å–å®Œæˆ: {result}")
        return result


async def handle_category_command(args):
    """å¤„ç†åˆ†ç±»çˆ¬å–å‘½ä»¤"""
    print(f"ğŸ“Š å¼€å§‹åˆ†ç±»çˆ¬å–ä»»åŠ¡")
    
    config = load_config(args.config)
    crawler = EnhancedArxivCrawler(config)
    
    async with crawler:
        if args.folders:
            # ä½¿ç”¨å¢å¼ºçš„æ–‡ä»¶å¤¹ç”ŸæˆåŠŸèƒ½
            result = await crawler.crawl_and_save_by_categories(
                categories=args.categories,
                max_per_category=args.max_per_category,
                generate_folders=True
            )
            print(f"âœ… åˆ†ç±»çˆ¬å–å®Œæˆï¼ˆå«æ–‡ä»¶å¤¹ç»“æ„ï¼‰: {result}")
        else:
            # åªç”ŸæˆJSONæ•°æ®
            papers_by_category = await crawler.crawl_by_categories(
                categories=args.categories,
                max_per_category=args.max_per_category
            )
            
            # ä¿å­˜ç»“æœ
            output_dir = os.path.join(config.get("crawler", {}).get("output_directory", "crawled_data"), "category")
            await save_category_results(papers_by_category, output_dir)
            
            result = {
                "total_papers": sum(len(papers) for papers in papers_by_category.values()),
                "categories": list(papers_by_category.keys()),
                "output_dir": output_dir,
                "folder_structure_generated": False
            }
            print(f"âœ… åˆ†ç±»çˆ¬å–å®Œæˆï¼ˆä»…JSONï¼‰: {result}")
        
        return result


async def handle_keyword_command(args):
    """å¤„ç†å…³é”®è¯çˆ¬å–å‘½ä»¤"""
    print(f"ğŸ” å¼€å§‹å…³é”®è¯çˆ¬å–ä»»åŠ¡ï¼ˆå›æº¯ {args.days} å¤©ï¼‰")
    
    config = load_config(args.config)
    crawler = EnhancedArxivCrawler(config)
    
    async with crawler:
        result = await crawler.crawl_and_save_weekly(
            keywords=args.keywords,
            days_back=args.days
        )
        
        print(f"âœ… å…³é”®è¯çˆ¬å–å®Œæˆ: {result}")
        return result


def handle_info_command(args):
    """å¤„ç†ä¿¡æ¯å‘½ä»¤"""
    config = load_config(args.config)
    crawler = EnhancedArxivCrawler(config)
    
    print("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
    
    if args.categories:
        print("\nğŸ“‚ æ‰€æœ‰AIç›¸å…³åˆ†ç±»:")
        category_groups = crawler.get_category_groups()
        for group, categories in category_groups.items():
            print(f"\n{group}:")
            for cat in categories:
                print(f"   - {cat}")
        
        print(f"\næ€»è®¡: {len(crawler.get_all_ai_categories())} ä¸ªåˆ†ç±»")
    
    if args.keywords:
        print("\nğŸ”¥ é»˜è®¤çƒ­é—¨å…³é”®è¯:")
        keywords = crawler.trending_keywords
        for i, keyword in enumerate(keywords, 1):
            print(f"   {i:2d}. {keyword}")
        
        print(f"\næ€»è®¡: {len(keywords)} ä¸ªå…³é”®è¯")
    
    if not args.categories and not args.keywords:
        # æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯
        print("\nğŸ“‹ ç³»ç»Ÿé…ç½®æ‘˜è¦:")
        print(f"   é…ç½®æ–‡ä»¶: {args.config}")
        print(f"   AIåˆ†ç±»ç»„: {len(crawler.get_category_groups())} ç»„")
        print(f"   æ€»åˆ†ç±»æ•°: {len(crawler.get_all_ai_categories())} ä¸ª")
        print(f"   çƒ­é—¨å…³é”®è¯: {len(crawler.trending_keywords)} ä¸ª")
        
        oss_config = config.get('oss', {})
        print(f"\nâ˜ï¸ OSSé…ç½®:")
        print(f"   APIåœ°å€: {oss_config.get('base_url', 'N/A')}")
        print(f"   å­˜å‚¨æ¡¶: {oss_config.get('bucket_name', 'N/A')}")


async def save_category_results(papers_by_category, output_dir):
    """ä¿å­˜åˆ†ç±»çˆ¬å–ç»“æœ"""
    ensure_directory(output_dir)
    
    # ä¿å­˜æŒ‰åˆ†ç±»ç»„ç»‡çš„æ•°æ®
    save_json(papers_by_category, os.path.join(output_dir, "papers_by_category.json"))
    
    # åˆå¹¶æ‰€æœ‰è®ºæ–‡
    all_papers = []
    for papers in papers_by_category.values():
        all_papers.extend(papers)
    
    save_json(all_papers, os.path.join(output_dir, "all_papers.json"))
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "crawl_date": datetime.now().isoformat(),
        "total_papers": len(all_papers),
        "categories_stats": {
            category: len(papers) 
            for category, papers in papers_by_category.items()
        }
    }
    save_json(stats, os.path.join(output_dir, "category_stats.json"))


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = setup_argument_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        if args.command == "daily":
            asyncio.run(handle_daily_command(args))
            
        elif args.command == "category":
            asyncio.run(handle_category_command(args))
            
        elif args.command == "keyword":
            asyncio.run(handle_keyword_command(args))
            
        elif args.command == "info":
            handle_info_command(args)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
        sys.exit(0)
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
