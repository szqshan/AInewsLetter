#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆ arXiv çˆ¬è™«åŠŸèƒ½æµ‹è¯•è„šæœ¬
éªŒè¯æ–°å¢çš„æ¯æ—¥çˆ¬å–ã€åˆ†ç±»çˆ¬å–å’Œå…³é”®è¯çˆ¬å–åŠŸèƒ½
"""

import asyncio
import sys
import json
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arxiv_system.crawler.enhanced_arxiv_crawler import EnhancedArxivCrawler
from arxiv_system.utils.file_utils import load_config


async def test_config_loading():
    """æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½"""
    print("ğŸ”§ æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½...")
    
    try:
        config = load_config("config_enhanced.json")
        print("âœ… å¢å¼ºç‰ˆé…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥å…³é”®é…ç½®é¡¹
        ai_categories = config.get("crawler", {}).get("ai_categories", {})
        print(f"   - AIåˆ†ç±»ç»„: {len(ai_categories)} ç»„")
        
        crawl_strategies = config.get("crawl_strategies", {})
        print(f"   - çˆ¬å–ç­–ç•¥: {len(crawl_strategies)} ç§")
        
        trending_keywords = config.get("crawler", {}).get("trending_keywords", [])
        print(f"   - çƒ­é—¨å…³é”®è¯: {len(trending_keywords)} ä¸ª")
        
        return config
        
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return None


async def test_enhanced_crawler_initialization():
    """æµ‹è¯•å¢å¼ºç‰ˆçˆ¬è™«åˆå§‹åŒ–"""
    print("\nğŸ•·ï¸ æµ‹è¯•å¢å¼ºç‰ˆçˆ¬è™«åˆå§‹åŒ–...")
    
    try:
        config = load_config("config_enhanced.json")
        crawler = EnhancedArxivCrawler(config)
        
        print("âœ… å¢å¼ºç‰ˆçˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
        
        # æ£€æŸ¥åˆ†ç±»é…ç½®
        all_categories = crawler.get_all_ai_categories()
        print(f"   - æ‰€æœ‰AIåˆ†ç±»: {len(all_categories)} ä¸ª")
        print(f"   - åˆ†ç±»åˆ—è¡¨: {', '.join(all_categories[:5])}...")
        
        # æ£€æŸ¥å…³é”®è¯é…ç½®
        keywords = crawler.trending_keywords
        print(f"   - çƒ­é—¨å…³é”®è¯: {len(keywords)} ä¸ª")
        print(f"   - å…³é”®è¯åˆ—è¡¨: {', '.join(keywords[:3])}...")
        
        return crawler
        
    except Exception as e:
        print(f"âŒ å¢å¼ºç‰ˆçˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        return None


async def test_daily_crawl_small():
    """æµ‹è¯•å°è§„æ¨¡æ¯æ—¥çˆ¬å–"""
    print("\nğŸŒ… æµ‹è¯•å°è§„æ¨¡æ¯æ—¥çˆ¬å–...")
    
    try:
        config = load_config("config_enhanced.json")
        crawler = EnhancedArxivCrawler(config)
        
        async with crawler:
            # ä»…æµ‹è¯•cs.AIåˆ†ç±»ï¼Œæœ€è¿‘1å¤©ï¼Œå°é‡æ•°æ®
            papers = await crawler.crawl_daily_new_papers(
                days_back=1,
                categories=["cs.AI"]
            )
            
            total_papers = sum(len(papers_list) for papers_list in papers.values())
            print(f"âœ… æ¯æ—¥çˆ¬å–æµ‹è¯•å®Œæˆï¼Œè·å– {total_papers} ç¯‡è®ºæ–‡")
            
            # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            for group, papers_list in papers.items():
                print(f"   - {group}: {len(papers_list)} ç¯‡")
            
            return papers
            
    except Exception as e:
        print(f"âŒ æ¯æ—¥çˆ¬å–æµ‹è¯•å¤±è´¥: {e}")
        return None


async def test_category_crawl_small():
    """æµ‹è¯•å°è§„æ¨¡åˆ†ç±»çˆ¬å–"""
    print("\nğŸ“Š æµ‹è¯•å°è§„æ¨¡åˆ†ç±»çˆ¬å–...")
    
    try:
        config = load_config("config_enhanced.json")
        crawler = EnhancedArxivCrawler(config)
        
        async with crawler:
            # ä»…æµ‹è¯•2ä¸ªåˆ†ç±»ï¼Œæ¯ä¸ªæœ€å¤š10ç¯‡
            papers = await crawler.crawl_by_categories(
                categories=["cs.AI", "cs.LG"],
                max_per_category=10
            )
            
            total_papers = sum(len(papers_list) for papers_list in papers.values())
            print(f"âœ… åˆ†ç±»çˆ¬å–æµ‹è¯•å®Œæˆï¼Œè·å– {total_papers} ç¯‡è®ºæ–‡")
            
            # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            for category, papers_list in papers.items():
                print(f"   - {category}: {len(papers_list)} ç¯‡")
            
            return papers
            
    except Exception as e:
        print(f"âŒ åˆ†ç±»çˆ¬å–æµ‹è¯•å¤±è´¥: {e}")
        return None


async def test_keyword_crawl_small():
    """æµ‹è¯•å°è§„æ¨¡å…³é”®è¯çˆ¬å–"""
    print("\nğŸ” æµ‹è¯•å°è§„æ¨¡å…³é”®è¯çˆ¬å–...")
    
    try:
        config = load_config("config_enhanced.json")
        crawler = EnhancedArxivCrawler(config)
        
        async with crawler:
            # ä»…æµ‹è¯•2ä¸ªå…³é”®è¯ï¼Œæœ€è¿‘3å¤©
            papers = await crawler.crawl_trending_keywords(
                keywords=["transformer", "LLM"],
                days_back=3
            )
            
            total_papers = sum(len(papers_list) for papers_list in papers.values())
            print(f"âœ… å…³é”®è¯çˆ¬å–æµ‹è¯•å®Œæˆï¼Œè·å– {total_papers} ç¯‡è®ºæ–‡")
            
            # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
            for keyword, papers_list in papers.items():
                print(f"   - {keyword}: {len(papers_list)} ç¯‡")
            
            return papers
            
    except Exception as e:
        print(f"âŒ å…³é”®è¯çˆ¬å–æµ‹è¯•å¤±è´¥: {e}")
        return None


async def test_save_functionality():
    """æµ‹è¯•ä¿å­˜åŠŸèƒ½"""
    print("\nğŸ’¾ æµ‹è¯•ä¿å­˜åŠŸèƒ½...")
    
    try:
        config = load_config("config_enhanced.json")
        crawler = EnhancedArxivCrawler(config)
        
        async with crawler:
            # æµ‹è¯•æ¯æ—¥çˆ¬å–å¹¶ä¿å­˜
            result = await crawler.crawl_and_save_daily(
                days_back=1,
                categories=["cs.AI"],
                output_base_dir="test_output"
            )
            
            print(f"âœ… ä¿å­˜åŠŸèƒ½æµ‹è¯•å®Œæˆ: {result}")
            
            # æ£€æŸ¥è¾“å‡ºç›®å½•
            output_path = Path(result["output_dir"])
            if output_path.exists():
                files = list(output_path.glob("*.json"))
                print(f"   - ç”Ÿæˆæ–‡ä»¶: {len(files)} ä¸ª")
                for file in files:
                    print(f"     â€¢ {file.name}")
            
            return result
            
    except Exception as e:
        print(f"âŒ ä¿å­˜åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return None


def test_cli_help():
    """æµ‹è¯•CLIå¸®åŠ©ä¿¡æ¯"""
    print("\nğŸ“‹ æµ‹è¯•CLIå¸®åŠ©ä¿¡æ¯...")
    
    try:
        # å¯¼å…¥CLIæ¨¡å—
        import subprocess
        import sys
        
        # æµ‹è¯•ä¸»ç¨‹åºå¸®åŠ©
        result = subprocess.run([
            sys.executable, "main_enhanced.py", "--help"
        ], capture_output=True, text=True, cwd=Path(__file__).parent)
        
        if result.returncode == 0:
            print("âœ… CLIå¸®åŠ©ä¿¡æ¯æ˜¾ç¤ºæ­£å¸¸")
            print(f"   - å¸®åŠ©æ–‡æœ¬é•¿åº¦: {len(result.stdout)} å­—ç¬¦")
        else:
            print(f"âŒ CLIå¸®åŠ©ä¿¡æ¯æ˜¾ç¤ºå¤±è´¥: {result.stderr}")
        
        return result.returncode == 0
        
    except Exception as e:
        print(f"âŒ CLIæµ‹è¯•å¤±è´¥: {e}")
        return False


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹å¢å¼ºç‰ˆ arXiv çˆ¬è™«åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    test_results = {}
    
    # 1. æµ‹è¯•é…ç½®åŠ è½½
    config = await test_config_loading()
    test_results["config_loading"] = config is not None
    
    # 2. æµ‹è¯•çˆ¬è™«åˆå§‹åŒ–
    crawler = await test_enhanced_crawler_initialization()
    test_results["crawler_init"] = crawler is not None
    
    # 3. æµ‹è¯•æ¯æ—¥çˆ¬å–
    daily_result = await test_daily_crawl_small()
    test_results["daily_crawl"] = daily_result is not None
    
    # 4. æµ‹è¯•åˆ†ç±»çˆ¬å–
    category_result = await test_category_crawl_small()
    test_results["category_crawl"] = category_result is not None
    
    # 5. æµ‹è¯•å…³é”®è¯çˆ¬å–
    keyword_result = await test_keyword_crawl_small()
    test_results["keyword_crawl"] = keyword_result is not None
    
    # 6. æµ‹è¯•ä¿å­˜åŠŸèƒ½
    save_result = await test_save_functionality()
    test_results["save_functionality"] = save_result is not None
    
    # 7. æµ‹è¯•CLI
    cli_result = test_cli_help()
    test_results["cli_help"] = cli_result
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ¯ æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 50)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{test_name:20s}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢å¼ºç‰ˆåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
    
    return test_results


if __name__ == "__main__":
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path("config_enhanced.json").exists():
        print("âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ config_enhanced.json")
        print("è¯·ç¡®ä¿é…ç½®æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸­")
        sys.exit(1)
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(run_all_tests())
