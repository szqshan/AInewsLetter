#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆarXivè®ºæ–‡çˆ¬è™«ä¸»ç¨‹åº
æ”¯æŒæ¯æ—¥çˆ¬å–ã€åˆ†ç±»çˆ¬å–ã€å…³é”®è¯çˆ¬å–å’Œå®šæ—¶ä»»åŠ¡
"""

import argparse
import json
import logging
import os
import sys
from pathlib import Path
import asyncio

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arxiv_system.crawler.enhanced_arxiv_crawler import EnhancedArxivCrawler
from arxiv_system.scheduler.arxiv_scheduler import ArxivScheduler
from arxiv_system.oss.wrapper import OSSUploader
from arxiv_system.utils.file_utils import setup_logging, load_config


def setup_argument_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="å¢å¼ºç‰ˆarXivè®ºæ–‡çˆ¬è™«ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # æ¯æ—¥çˆ¬å–
  python main_enhanced.py daily --days 1
  python main_enhanced.py daily --days 3 --categories cs.AI cs.LG
  
  # åˆ†ç±»çˆ¬å–
  python main_enhanced.py category --categories cs.AI cs.LG --max-per-category 1000
  
  # å…³é”®è¯çˆ¬å–
  python main_enhanced.py keyword --keywords "transformer" "diffusion" --days 7
  
  # å®šæ—¶ä»»åŠ¡
  python main_enhanced.py schedule --daily-time "09:00" --immediate
  
  # ä¼ ç»Ÿçˆ¬å–
  python main_enhanced.py crawl --query "artificial intelligence" --max-results 50
  
  # ä¸Šä¼ å’ŒçŠ¶æ€
  python main_enhanced.py upload --source crawled_data
  python main_enhanced.py status --detail
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # æ¯æ—¥çˆ¬å–å‘½ä»¤
    daily_parser = subparsers.add_parser("daily", help="çˆ¬å–æ¯æ—¥æ–°è®ºæ–‡")
    daily_parser.add_argument("--days", type=int, default=1, help="å›æº¯å¤©æ•°")
    daily_parser.add_argument("--categories", nargs="*", help="æŒ‡å®šåˆ†ç±»ï¼ˆå¦‚ï¼šcs.AI cs.LGï¼‰")
    daily_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    daily_parser.add_argument("--upload", action="store_true", help="å®Œæˆåè‡ªåŠ¨ä¸Šä¼ åˆ°OSS")
    
    # åˆ†ç±»çˆ¬å–å‘½ä»¤
    category_parser = subparsers.add_parser("category", help="æŒ‰åˆ†ç±»çˆ¬å–è®ºæ–‡")
    category_parser.add_argument("--categories", nargs="*", help="åˆ†ç±»åˆ—è¡¨ï¼ˆç©º=æ‰€æœ‰AIåˆ†ç±»ï¼‰")
    category_parser.add_argument("--max-per-category", type=int, default=1000, help="æ¯ä¸ªåˆ†ç±»æœ€å¤§è®ºæ–‡æ•°")
    category_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    category_parser.add_argument("--upload", action="store_true", help="å®Œæˆåè‡ªåŠ¨ä¸Šä¼ åˆ°OSS")
    
    # å…³é”®è¯çˆ¬å–å‘½ä»¤
    keyword_parser = subparsers.add_parser("keyword", help="æŒ‰å…³é”®è¯çˆ¬å–è®ºæ–‡")
    keyword_parser.add_argument("--keywords", nargs="*", help="å…³é”®è¯åˆ—è¡¨ï¼ˆç©º=ä½¿ç”¨é»˜è®¤çƒ­é—¨å…³é”®è¯ï¼‰")
    keyword_parser.add_argument("--days", type=int, default=7, help="å›æº¯å¤©æ•°")
    keyword_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    keyword_parser.add_argument("--upload", action="store_true", help="å®Œæˆåè‡ªåŠ¨ä¸Šä¼ åˆ°OSS")
    
    # å®šæ—¶ä»»åŠ¡å‘½ä»¤
    schedule_parser = subparsers.add_parser("schedule", help="å¯åŠ¨å®šæ—¶ä»»åŠ¡")
    schedule_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    schedule_parser.add_argument("--daily-time", default="09:00", help="æ¯æ—¥æ‰§è¡Œæ—¶é—´")
    schedule_parser.add_argument("--weekly-time", default="02:00", help="æ¯å‘¨æ‰§è¡Œæ—¶é—´")
    schedule_parser.add_argument("--immediate", action="store_true", help="ç«‹å³æ‰§è¡Œä¸€æ¬¡")
    schedule_parser.add_argument("--once-daily", action="store_true", help="ä»…æ‰§è¡Œä¸€æ¬¡æ¯æ—¥ä»»åŠ¡")
    schedule_parser.add_argument("--once-weekly", action="store_true", help="ä»…æ‰§è¡Œä¸€æ¬¡æ¯å‘¨ä»»åŠ¡")
    
    # ä¼ ç»Ÿçˆ¬å–å‘½ä»¤ï¼ˆå…¼å®¹æ€§ï¼‰
    crawl_parser = subparsers.add_parser("crawl", help="ä¼ ç»Ÿçˆ¬å–æ¨¡å¼")
    crawl_parser.add_argument("--query", default="cat:cs.AI OR cat:cs.LG OR cat:cs.CL", 
                             help="æœç´¢æŸ¥è¯¢æ¡ä»¶")
    crawl_parser.add_argument("--max-results", type=int, default=100, 
                             help="æœ€å¤§ç»“æœæ•°é‡")
    crawl_parser.add_argument("--output", default="crawled_data", 
                             help="è¾“å‡ºç›®å½•")
    crawl_parser.add_argument("--concurrent", type=int, default=3, 
                             help="å¹¶å‘æ•°é‡")
    crawl_parser.add_argument("--download-pdf", action="store_true", 
                             help="ä¸‹è½½PDFæ–‡ä»¶")
    crawl_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # ä¸Šä¼ å‘½ä»¤
    upload_parser = subparsers.add_parser("upload", help="ä¸Šä¼ æ•°æ®åˆ°OSS")
    upload_parser.add_argument("--source", default="crawled_data", 
                              help="æºæ•°æ®ç›®å½•")
    upload_parser.add_argument("--bucket", type=str, help="æŒ‡å®šbucketåç§°")
    upload_parser.add_argument("--resume", action="store_true", help="æ–­ç‚¹ç»­ä¼ ")
    upload_parser.add_argument("--concurrent", type=int, default=5, 
                              help="å¹¶å‘ä¸Šä¼ æ•°é‡")
    upload_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # çŠ¶æ€å‘½ä»¤
    status_parser = subparsers.add_parser("status", help="æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
    status_parser.add_argument("--detail", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯")
    status_parser.add_argument("--schedule", action="store_true", help="æ˜¾ç¤ºå®šæ—¶ä»»åŠ¡çŠ¶æ€")
    status_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # ä¿¡æ¯å‘½ä»¤
    info_parser = subparsers.add_parser("info", help="æ˜¾ç¤ºé…ç½®å’Œç»Ÿè®¡ä¿¡æ¯")
    info_parser.add_argument("--categories", action="store_true", help="æ˜¾ç¤ºæ‰€æœ‰åˆ†ç±»")
    info_parser.add_argument("--keywords", action="store_true", help="æ˜¾ç¤ºé»˜è®¤å…³é”®è¯")
    info_parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    return parser


async def handle_daily_command(args):
    """å¤„ç†æ¯æ—¥çˆ¬å–å‘½ä»¤"""
    print(f"ğŸŒ… å¼€å§‹æ¯æ—¥çˆ¬å–ä»»åŠ¡ï¼ˆå›æº¯ {args.days} å¤©ï¼‰")
    
    config = load_config(args.config)
    crawler = EnhancedArxivCrawler(config)
    
    async with crawler:
        result = await crawler.crawl_and_save_daily(
            days_back=args.days,
            categories=args.categories
        )
        
        print(f"âœ… æ¯æ—¥çˆ¬å–å®Œæˆ: {result}")
        
        if args.upload:
            await upload_to_oss(result["output_dir"], config)
        
        return result


async def handle_category_command(args):
    """å¤„ç†åˆ†ç±»çˆ¬å–å‘½ä»¤"""
    print(f"ğŸ“Š å¼€å§‹åˆ†ç±»çˆ¬å–ä»»åŠ¡")
    
    config = load_config(args.config)
    crawler = EnhancedArxivCrawler(config)
    
    async with crawler:
        papers_by_category = await crawler.crawl_by_categories(
            categories=args.categories,
            max_per_category=args.max_per_category
        )
        
        # ä¿å­˜ç»“æœ
        output_dir = os.path.join(config.get("crawler", {}).get("output_directory", "crawled_data"), "category")
        await save_category_results(papers_by_category, output_dir)
        
        result = {
            "total_papers": sum(len(papers) for papers in papers_by_category.values()),
            "categories": list(papers_by_category.keys()),
            "output_dir": output_dir
        }
        
        print(f"âœ… åˆ†ç±»çˆ¬å–å®Œæˆ: {result}")
        
        if args.upload:
            await upload_to_oss(output_dir, config)
        
        return result


async def handle_keyword_command(args):
    """å¤„ç†å…³é”®è¯çˆ¬å–å‘½ä»¤"""
    print(f"ğŸ” å¼€å§‹å…³é”®è¯çˆ¬å–ä»»åŠ¡ï¼ˆå›æº¯ {args.days} å¤©ï¼‰")
    
    config = load_config(args.config)
    crawler = EnhancedArxivCrawler(config)
    
    async with crawler:
        result = await crawler.crawl_and_save_weekly(
            keywords=args.keywords,
            days_back=args.days
        )
        
        print(f"âœ… å…³é”®è¯çˆ¬å–å®Œæˆ: {result}")
        
        if args.upload:
            await upload_to_oss(result["output_dir"], config)
        
        return result


def handle_schedule_command(args):
    """å¤„ç†å®šæ—¶ä»»åŠ¡å‘½ä»¤"""
    print("â° å¯åŠ¨å®šæ—¶ä»»åŠ¡ç³»ç»Ÿ")
    
    scheduler = ArxivScheduler(args.config)
    
    if args.once_daily:
        return scheduler.run_once_daily()
    elif args.once_weekly:
        return scheduler.run_once_weekly()
    else:
        scheduler.start_custom_schedule(
            daily_time=args.daily_time,
            weekly_time=args.weekly_time,
            immediate_run=args.immediate
        )


async def handle_crawl_command(args):
    """å¤„ç†ä¼ ç»Ÿçˆ¬å–å‘½ä»¤ï¼ˆå…¼å®¹æ€§ï¼‰"""
    print("ğŸ•·ï¸ ä½¿ç”¨ä¼ ç»Ÿçˆ¬å–æ¨¡å¼")
    
    config = load_config(args.config)
    crawler = EnhancedArxivCrawler(config)
    
    await crawler._crawl_async(
        query=args.query,
        max_results=args.max_results,
        output_dir=args.output,
        concurrent=args.concurrent,
        download_pdf=args.download_pdf
    )


async def handle_upload_command(args):
    """å¤„ç†ä¸Šä¼ å‘½ä»¤"""
    print(f"â˜ï¸ å¼€å§‹ä¸Šä¼ åˆ°OSS: {args.source}")
    
    config = load_config(args.config)
    
    # å¦‚æœæŒ‡å®šäº†bucketï¼Œæ›´æ–°é…ç½®
    if args.bucket:
        config['oss']['bucket_name'] = args.bucket
    
    await upload_to_oss(args.source, config)


def handle_status_command(args):
    """å¤„ç†çŠ¶æ€å‘½ä»¤"""
    print("ğŸ“Š ç³»ç»ŸçŠ¶æ€ä¿¡æ¯")
    
    config = load_config(args.config)
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("\nğŸ”§ é…ç½®ä¿¡æ¯:")
    print(f"   é…ç½®æ–‡ä»¶: {args.config}")
    print(f"   è¾“å‡ºç›®å½•: {config.get('crawler', {}).get('output_directory', 'crawled_data')}")
    print(f"   æ¯æ—¥çˆ¬å–: {'å¯ç”¨' if config.get('crawler', {}).get('enable_daily_crawl', False) else 'ç¦ç”¨'}")
    
    # æ˜¾ç¤ºAIåˆ†ç±»
    ai_categories = config.get('crawler', {}).get('ai_categories', {})
    print(f"\nğŸ“‚ AIåˆ†ç±»é…ç½®:")
    for group, categories in ai_categories.items():
        print(f"   {group}: {', '.join(categories)}")
    
    # æ˜¾ç¤ºå®šæ—¶ä»»åŠ¡çŠ¶æ€
    if args.schedule:
        scheduler = ArxivScheduler(args.config)
        status = scheduler.get_schedule_status()
        print(f"\nâ° å®šæ—¶ä»»åŠ¡çŠ¶æ€:")
        print(f"   è¿è¡ŒçŠ¶æ€: {'è¿è¡Œä¸­' if status['running'] else 'æœªè¿è¡Œ'}")
        print(f"   ä»»åŠ¡æ•°é‡: {status['total_jobs']}")
        for job in status['jobs']:
            print(f"   - {job['function']}: {job['interval']} {job['unit']}")
    
    # æ˜¾ç¤ºå­˜å‚¨ä¿¡æ¯
    output_dir = Path(config.get('crawler', {}).get('output_directory', 'crawled_data'))
    if output_dir.exists():
        print(f"\nğŸ’¾ å­˜å‚¨ä¿¡æ¯:")
        
        daily_dir = output_dir / "daily"
        if daily_dir.exists():
            daily_count = len(list(daily_dir.iterdir()))
            print(f"   æ¯æ—¥æ•°æ®: {daily_count} ä¸ªç›®å½•")
        
        weekly_dir = output_dir / "weekly"
        if weekly_dir.exists():
            weekly_count = len(list(weekly_dir.iterdir()))
            print(f"   æ¯å‘¨æ•°æ®: {weekly_count} ä¸ªç›®å½•")


def handle_info_command(args):
    """å¤„ç†ä¿¡æ¯å‘½ä»¤"""
    config = load_config(args.config)
    crawler = EnhancedArxivCrawler(config)
    
    print("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
    
    if args.categories:
        print("\nğŸ“‚ æ‰€æœ‰AIç›¸å…³åˆ†ç±»:")
        category_groups = crawler.get_category_groups()
        for group, categories in category_groups.items():
            print(f"\n{group}:")
            for cat in categories:
                print(f"   - {cat}")
        
        print(f"\næ€»è®¡: {len(crawler.get_all_ai_categories())} ä¸ªåˆ†ç±»")
    
    if args.keywords:
        print("\nğŸ”¥ é»˜è®¤çƒ­é—¨å…³é”®è¯:")
        keywords = crawler.trending_keywords
        for i, keyword in enumerate(keywords, 1):
            print(f"   {i:2d}. {keyword}")
        
        print(f"\næ€»è®¡: {len(keywords)} ä¸ªå…³é”®è¯")
    
    if not args.categories and not args.keywords:
        # æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯
        print("\nğŸ“‹ ç³»ç»Ÿé…ç½®æ‘˜è¦:")
        print(f"   é…ç½®æ–‡ä»¶: {args.config}")
        print(f"   AIåˆ†ç±»ç»„: {len(crawler.get_category_groups())} ç»„")
        print(f"   æ€»åˆ†ç±»æ•°: {len(crawler.get_all_ai_categories())} ä¸ª")
        print(f"   çƒ­é—¨å…³é”®è¯: {len(crawler.trending_keywords)} ä¸ª")
        
        oss_config = config.get('oss', {})
        print(f"\nâ˜ï¸ OSSé…ç½®:")
        print(f"   APIåœ°å€: {oss_config.get('base_url', 'N/A')}")
        print(f"   å­˜å‚¨æ¡¶: {oss_config.get('bucket_name', 'N/A')}")
        print(f"   è‡ªåŠ¨ä¸Šä¼ : {'å¯ç”¨' if oss_config.get('enable_auto_upload', False) else 'ç¦ç”¨'}")


async def save_category_results(papers_by_category, output_dir):
    """ä¿å­˜åˆ†ç±»çˆ¬å–ç»“æœ"""
    from arxiv_system.utils.file_utils import ensure_directory, save_json
    
    ensure_directory(output_dir)
    
    # ä¿å­˜æŒ‰åˆ†ç±»ç»„ç»‡çš„æ•°æ®
    save_json(papers_by_category, os.path.join(output_dir, "papers_by_category.json"))
    
    # åˆå¹¶æ‰€æœ‰è®ºæ–‡
    all_papers = []
    for papers in papers_by_category.values():
        all_papers.extend(papers)
    
    save_json(all_papers, os.path.join(output_dir, "all_papers.json"))
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "crawl_date": datetime.now().isoformat(),
        "total_papers": len(all_papers),
        "categories_stats": {
            category: len(papers) 
            for category, papers in papers_by_category.items()
        }
    }
    save_json(stats, os.path.join(output_dir, "category_stats.json"))


async def upload_to_oss(source_dir, config):
    """ä¸Šä¼ åˆ°OSS"""
    oss_config = config.get('oss', {})
    
    async with OSSUploader(oss_config) as uploader:
        result = await uploader.upload_all(
            base_dir=Path(source_dir),
            resume=True
        )
        
        if result.get('success'):
            print(f"âœ… OSSä¸Šä¼ æˆåŠŸï¼å…±ä¸Šä¼  {result.get('uploaded_files', 0)} ä¸ªæ–‡ä»¶")
            if result.get('sample_urls'):
                print("ğŸ“‹ ç¤ºä¾‹URL:")
                for url in result['sample_urls'][:3]:
                    print(f"  ğŸ”— {url}")
        else:
            print(f"âŒ OSSä¸Šä¼ å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = setup_argument_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        if args.command == "daily":
            asyncio.run(handle_daily_command(args))
            
        elif args.command == "category":
            asyncio.run(handle_category_command(args))
            
        elif args.command == "keyword":
            asyncio.run(handle_keyword_command(args))
            
        elif args.command == "schedule":
            handle_schedule_command(args)
            
        elif args.command == "crawl":
            asyncio.run(handle_crawl_command(args))
            
        elif args.command == "upload":
            asyncio.run(handle_upload_command(args))
            
        elif args.command == "status":
            handle_status_command(args)
            
        elif args.command == "info":
            handle_info_command(args)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
        sys.exit(0)
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
