#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXiv çˆ¬è™«å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
æ”¯æŒæ¯æ—¥ã€æ¯å‘¨å’Œè‡ªå®šä¹‰å®šæ—¶ä»»åŠ¡
"""

import schedule
import time
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional
import threading
import signal
import sys

from ..crawler.enhanced_arxiv_crawler import EnhancedArxivCrawler
from ..oss.wrapper import OSSUploader
from ..utils.file_utils import load_config, save_json


class ArxivScheduler:
    """arXivçˆ¬è™«å®šæ—¶è°ƒåº¦å™¨"""
    
    def __init__(self, config_path: str = "config_enhanced.json"):
        """åˆå§‹åŒ–è°ƒåº¦å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = load_config(config_path)
        self.crawler_config = self.config.get("crawler", {})
        self.oss_config = self.config.get("oss", {})
        
        self.logger = self._setup_logging()
        self.crawler = None
        self.running = False
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨ç”¨äºä¼˜é›…é€€å‡º
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        logging_config = self.config.get("logging", {})
        
        logging.basicConfig(
            level=getattr(logging, logging_config.get("level", "INFO")),
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(logging_config.get("file", "arxiv_scheduler.log")),
                logging.StreamHandler()
            ]
        )
        
        return logging.getLogger(__name__)
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨ï¼Œç”¨äºä¼˜é›…é€€å‡º"""
        self.logger.info(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå‡†å¤‡åœæ­¢è°ƒåº¦å™¨...")
        self.stop()
    
    def start_daily_schedule(self, immediate_run: bool = False):
        """å¯åŠ¨æ¯æ—¥å®šæ—¶ä»»åŠ¡
        
        Args:
            immediate_run: æ˜¯å¦ç«‹å³æ‰§è¡Œä¸€æ¬¡
        """
        daily_config = self.crawler_config.get("daily_schedule", {})
        crawl_time = daily_config.get("time", "09:00")
        
        print(f"ğŸ“… è®¾ç½®æ¯æ—¥å®šæ—¶çˆ¬å–: {crawl_time}")
        self.logger.info(f"è®¾ç½®æ¯æ—¥å®šæ—¶ä»»åŠ¡: {crawl_time}")
        
        # è®¾ç½®æ¯æ—¥ä»»åŠ¡
        schedule.every().day.at(crawl_time).do(self._run_daily_crawl)
        
        # è®¾ç½®æ¯å‘¨å…¨é‡æ›´æ–°ï¼ˆå‘¨æ—¥å‡Œæ™¨ï¼‰
        schedule.every().sunday.at("02:00").do(self._run_weekly_crawl)
        
        # è®¾ç½®æ¯æœˆæ¸…ç†ä»»åŠ¡ï¼ˆæ¯æœˆ1å·ï¼‰
        schedule.every().month.do(self._run_monthly_cleanup)
        
        self.running = True
        
        if immediate_run:
            print("ğŸš€ ç«‹å³æ‰§è¡Œä¸€æ¬¡æ¯æ—¥çˆ¬å–ä»»åŠ¡...")
            self._run_daily_crawl()
        
        print("â° å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œç­‰å¾…æ‰§è¡Œ...")
        print("ğŸ“‹ ä»»åŠ¡åˆ—è¡¨:")
        print(f"   - æ¯æ—¥çˆ¬å–: {crawl_time}")
        print(f"   - æ¯å‘¨å…¨é‡: å‘¨æ—¥ 02:00")
        print(f"   - æ¯æœˆæ¸…ç†: æ¯æœˆ1å·")
        print("æŒ‰ Ctrl+C åœæ­¢è°ƒåº¦å™¨")
        
        self._run_scheduler()
    
    def start_custom_schedule(self, daily_time: str = "09:00", 
                            weekly_time: str = "02:00",
                            immediate_run: bool = False):
        """å¯åŠ¨è‡ªå®šä¹‰å®šæ—¶ä»»åŠ¡
        
        Args:
            daily_time: æ¯æ—¥æ‰§è¡Œæ—¶é—´
            weekly_time: æ¯å‘¨æ‰§è¡Œæ—¶é—´
            immediate_run: æ˜¯å¦ç«‹å³æ‰§è¡Œä¸€æ¬¡
        """
        print(f"ğŸ“… è®¾ç½®è‡ªå®šä¹‰å®šæ—¶ä»»åŠ¡:")
        print(f"   - æ¯æ—¥çˆ¬å–: {daily_time}")
        print(f"   - æ¯å‘¨å…¨é‡: å‘¨æ—¥ {weekly_time}")
        
        # è®¾ç½®è‡ªå®šä¹‰ä»»åŠ¡
        schedule.every().day.at(daily_time).do(self._run_daily_crawl)
        schedule.every().sunday.at(weekly_time).do(self._run_weekly_crawl)
        
        self.running = True
        
        if immediate_run:
            print("ğŸš€ ç«‹å³æ‰§è¡Œä¸€æ¬¡æ¯æ—¥çˆ¬å–ä»»åŠ¡...")
            self._run_daily_crawl()
        
        print("â° è‡ªå®šä¹‰å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨...")
        self._run_scheduler()
    
    def run_once_daily(self, days_back: int = 1):
        """æ‰§è¡Œä¸€æ¬¡æ¯æ—¥çˆ¬å–ä»»åŠ¡
        
        Args:
            days_back: å›æº¯å¤©æ•°
        """
        print(f"ğŸŒ… æ‰§è¡Œå•æ¬¡æ¯æ—¥çˆ¬å–ä»»åŠ¡ï¼ˆå›æº¯ {days_back} å¤©ï¼‰")
        
        try:
            result = asyncio.run(self._daily_crawl_task(days_back))
            print(f"âœ… å•æ¬¡æ¯æ—¥çˆ¬å–å®Œæˆ: {result}")
            return result
        except Exception as e:
            self.logger.error(f"å•æ¬¡æ¯æ—¥çˆ¬å–å¤±è´¥: {e}")
            print(f"âŒ å•æ¬¡æ¯æ—¥çˆ¬å–å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def run_once_weekly(self, days_back: int = 7):
        """æ‰§è¡Œä¸€æ¬¡æ¯å‘¨çˆ¬å–ä»»åŠ¡
        
        Args:
            days_back: å›æº¯å¤©æ•°
        """
        print(f"ğŸ“š æ‰§è¡Œå•æ¬¡æ¯å‘¨çˆ¬å–ä»»åŠ¡ï¼ˆå›æº¯ {days_back} å¤©ï¼‰")
        
        try:
            result = asyncio.run(self._weekly_crawl_task(days_back))
            print(f"âœ… å•æ¬¡æ¯å‘¨çˆ¬å–å®Œæˆ: {result}")
            return result
        except Exception as e:
            self.logger.error(f"å•æ¬¡æ¯å‘¨çˆ¬å–å¤±è´¥: {e}")
            print(f"âŒ å•æ¬¡æ¯å‘¨çˆ¬å–å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _run_scheduler(self):
        """è¿è¡Œè°ƒåº¦å™¨ä¸»å¾ªç¯"""
        while self.running:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                self.logger.info("æ¥æ”¶åˆ°é”®ç›˜ä¸­æ–­ï¼Œåœæ­¢è°ƒåº¦å™¨")
                break
            except Exception as e:
                self.logger.error(f"è°ƒåº¦å™¨è¿è¡Œå¼‚å¸¸: {e}")
                time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†ç»§ç»­
        
        print("ğŸ“´ è°ƒåº¦å™¨å·²åœæ­¢")
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.running = False
        schedule.clear()
        print("ğŸ›‘ æ­£åœ¨åœæ­¢è°ƒåº¦å™¨...")
    
    def _run_daily_crawl(self):
        """æ‰§è¡Œæ¯æ—¥çˆ¬å–ä»»åŠ¡"""
        try:
            print(f"\nğŸŒ… å¼€å§‹æ‰§è¡Œæ¯æ—¥çˆ¬å–ä»»åŠ¡: {datetime.now()}")
            self.logger.info("å¼€å§‹æ¯æ—¥çˆ¬å–ä»»åŠ¡")
            
            result = asyncio.run(self._daily_crawl_task())
            
            if result.get("success"):
                print("âœ… æ¯æ—¥çˆ¬å–ä»»åŠ¡å®Œæˆ")
                self.logger.info(f"æ¯æ—¥çˆ¬å–ä»»åŠ¡å®Œæˆ: {result}")
            else:
                print(f"âŒ æ¯æ—¥çˆ¬å–ä»»åŠ¡å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                self.logger.error(f"æ¯æ—¥çˆ¬å–ä»»åŠ¡å¤±è´¥: {result}")
                
        except Exception as e:
            self.logger.error(f"æ¯æ—¥çˆ¬å–ä»»åŠ¡å¼‚å¸¸: {e}")
            print(f"âŒ æ¯æ—¥çˆ¬å–ä»»åŠ¡å¼‚å¸¸: {e}")
    
    def _run_weekly_crawl(self):
        """æ‰§è¡Œæ¯å‘¨çˆ¬å–ä»»åŠ¡"""
        try:
            print(f"\nğŸ“š å¼€å§‹æ‰§è¡Œæ¯å‘¨çˆ¬å–ä»»åŠ¡: {datetime.now()}")
            self.logger.info("å¼€å§‹æ¯å‘¨çˆ¬å–ä»»åŠ¡")
            
            result = asyncio.run(self._weekly_crawl_task())
            
            if result.get("success"):
                print("âœ… æ¯å‘¨çˆ¬å–ä»»åŠ¡å®Œæˆ")
                self.logger.info(f"æ¯å‘¨çˆ¬å–ä»»åŠ¡å®Œæˆ: {result}")
            else:
                print(f"âŒ æ¯å‘¨çˆ¬å–ä»»åŠ¡å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                self.logger.error(f"æ¯å‘¨çˆ¬å–ä»»åŠ¡å¤±è´¥: {result}")
                
        except Exception as e:
            self.logger.error(f"æ¯å‘¨çˆ¬å–ä»»åŠ¡å¼‚å¸¸: {e}")
            print(f"âŒ æ¯å‘¨çˆ¬å–ä»»åŠ¡å¼‚å¸¸: {e}")
    
    def _run_monthly_cleanup(self):
        """æ‰§è¡Œæ¯æœˆæ¸…ç†ä»»åŠ¡"""
        try:
            print(f"\nğŸ§¹ å¼€å§‹æ‰§è¡Œæ¯æœˆæ¸…ç†ä»»åŠ¡: {datetime.now()}")
            self.logger.info("å¼€å§‹æ¯æœˆæ¸…ç†ä»»åŠ¡")
            
            # æ¸…ç†30å¤©å‰çš„ä¸´æ—¶æ–‡ä»¶
            self._cleanup_old_files(days=30)
            
            print("âœ… æ¯æœˆæ¸…ç†ä»»åŠ¡å®Œæˆ")
            self.logger.info("æ¯æœˆæ¸…ç†ä»»åŠ¡å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"æ¯æœˆæ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")
            print(f"âŒ æ¯æœˆæ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")
    
    async def _daily_crawl_task(self, days_back: int = 1) -> Dict[str, Any]:
        """æ¯æ—¥çˆ¬å–ä»»åŠ¡å®ç°
        
        Args:
            days_back: å›æº¯å¤©æ•°
            
        Returns:
            ä»»åŠ¡ç»“æœ
        """
        try:
            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            crawler = EnhancedArxivCrawler(self.config)
            
            async with crawler:
                # çˆ¬å–æ¯æ—¥æ–°è®ºæ–‡
                result = await crawler.crawl_and_save_daily(days_back=days_back)
                
                # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨ä¸Šä¼ ï¼Œåˆ™ä¸Šä¼ åˆ°OSS
                if self.oss_config.get("enable_auto_upload", False):
                    await self._upload_to_oss(result["output_dir"])
                
                return result
                
        except Exception as e:
            self.logger.error(f"æ¯æ—¥çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _weekly_crawl_task(self, days_back: int = 7) -> Dict[str, Any]:
        """æ¯å‘¨çˆ¬å–ä»»åŠ¡å®ç°
        
        Args:
            days_back: å›æº¯å¤©æ•°
            
        Returns:
            ä»»åŠ¡ç»“æœ
        """
        try:
            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            crawler = EnhancedArxivCrawler(self.config)
            
            async with crawler:
                # çˆ¬å–çƒ­é—¨å…³é”®è¯è®ºæ–‡
                result = await crawler.crawl_and_save_weekly(days_back=days_back)
                
                # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨ä¸Šä¼ ï¼Œåˆ™ä¸Šä¼ åˆ°OSS
                if self.oss_config.get("enable_auto_upload", False):
                    await self._upload_to_oss(result["output_dir"])
                
                return result
                
        except Exception as e:
            self.logger.error(f"æ¯å‘¨çˆ¬å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    async def _upload_to_oss(self, local_dir: str) -> bool:
        """ä¸Šä¼ åˆ°OSS
        
        Args:
            local_dir: æœ¬åœ°ç›®å½•è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"â˜ï¸ å¼€å§‹ä¸Šä¼ åˆ°OSS: {local_dir}")
            
            async with OSSUploader(self.oss_config) as uploader:
                result = await uploader.upload_all(
                    base_dir=Path(local_dir),
                    resume=True
                )
                
                if result.get('success'):
                    print(f"âœ… OSSä¸Šä¼ æˆåŠŸ: {result.get('uploaded_files', 0)} ä¸ªæ–‡ä»¶")
                    self.logger.info(f"OSSä¸Šä¼ æˆåŠŸ: {result}")
                    return True
                else:
                    print(f"âŒ OSSä¸Šä¼ å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    self.logger.error(f"OSSä¸Šä¼ å¤±è´¥: {result}")
                    return False
                    
        except Exception as e:
            self.logger.error(f"OSSä¸Šä¼ å¼‚å¸¸: {e}")
            print(f"âŒ OSSä¸Šä¼ å¼‚å¸¸: {e}")
            return False
    
    def _cleanup_old_files(self, days: int = 30) -> None:
        """æ¸…ç†æ—§æ–‡ä»¶
        
        Args:
            days: ä¿ç•™å¤©æ•°
        """
        cutoff_date = datetime.now() - timedelta(days=days)
        output_dir = Path(self.crawler_config.get("output_directory", "crawled_data"))
        
        if not output_dir.exists():
            return
        
        cleaned_files = 0
        
        # æ¸…ç†dailyç›®å½•ä¸­çš„æ—§æ–‡ä»¶
        daily_dir = output_dir / "daily"
        if daily_dir.exists():
            for date_dir in daily_dir.iterdir():
                if date_dir.is_dir():
                    try:
                        dir_date = datetime.strptime(date_dir.name, "%Y-%m-%d")
                        if dir_date < cutoff_date:
                            import shutil
                            shutil.rmtree(date_dir)
                            cleaned_files += 1
                            print(f"ğŸ—‘ï¸ æ¸…ç†æ—§ç›®å½•: {date_dir}")
                    except ValueError:
                        # ç›®å½•åæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡
                        continue
        
        # æ¸…ç†weeklyç›®å½•ä¸­çš„æ—§æ–‡ä»¶
        weekly_dir = output_dir / "weekly"
        if weekly_dir.exists():
            for week_dir in weekly_dir.iterdir():
                if week_dir.is_dir():
                    # ç®€å•æŒ‰åˆ›å»ºæ—¶é—´æ¸…ç†
                    dir_mtime = datetime.fromtimestamp(week_dir.stat().st_mtime)
                    if dir_mtime < cutoff_date:
                        import shutil
                        shutil.rmtree(week_dir)
                        cleaned_files += 1
                        print(f"ğŸ—‘ï¸ æ¸…ç†æ—§ç›®å½•: {week_dir}")
        
        print(f"ğŸ§¹ æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {cleaned_files} ä¸ªæ—§ç›®å½•")
        self.logger.info(f"æ¸…ç†æ—§æ–‡ä»¶å®Œæˆï¼Œæ¸…ç†äº† {cleaned_files} ä¸ªç›®å½•")
    
    def get_schedule_status(self) -> Dict[str, Any]:
        """è·å–è°ƒåº¦çŠ¶æ€
        
        Returns:
            è°ƒåº¦çŠ¶æ€ä¿¡æ¯
        """
        jobs = schedule.get_jobs()
        
        status = {
            "running": self.running,
            "total_jobs": len(jobs),
            "jobs": []
        }
        
        for job in jobs:
            job_info = {
                "function": job.job_func.__name__,
                "interval": str(job.interval),
                "unit": job.unit,
                "at_time": str(job.at_time) if job.at_time else None,
                "next_run": job.next_run.isoformat() if job.next_run else None
            }
            status["jobs"].append(job_info)
        
        return status
