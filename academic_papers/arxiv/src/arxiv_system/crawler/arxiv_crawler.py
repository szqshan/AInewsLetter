#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXivè®ºæ–‡çˆ¬è™«æ¨¡å—
é‡æ„è‡ªåŸå§‹spider.pyï¼Œé‡‡ç”¨æ–°çš„æ•°æ®æ¨¡å‹å’Œå­˜å‚¨æ¶æ„
"""

import asyncio
import aiohttp
import json
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import List, Dict, Optional, Any
import os
from pathlib import Path
import hashlib

from ..utils.file_utils import (
    ensure_directory, safe_filename, calculate_file_hash, 
    save_json, get_file_size_mb, parse_size_string
)


class ArxivCrawler:
    """arXivè®ºæ–‡çˆ¬è™«ç±» - é‡æ„ç‰ˆæœ¬"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.crawler_config = config.get("crawler", {})
        self.file_config = config.get("file_settings", {})
        
        self.base_url = self.crawler_config.get("base_url", "http://export.arxiv.org/api/query")
        self.output_dir = self.crawler_config.get("output_directory", "crawled_data")
        self.request_delay = self.crawler_config.get("request_delay", 1)
        self.max_retries = self.crawler_config.get("max_retries", 3)
        self.timeout = self.crawler_config.get("timeout", 30)
        self.enable_pdf_download = self.crawler_config.get("enable_pdf_download", True)
        self.max_concurrent = self.crawler_config.get("max_concurrent_papers", 3)
        
        # æ–‡ä»¶å¤§å°é™åˆ¶
        pdf_max_size_str = self.file_config.get("pdf_max_size", "50MB")
        self.pdf_max_size = parse_size_string(pdf_max_size_str)
        
        self.session = None
        self.results = []
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.timeout),
            connector=aiohttp.TCPConnector(limit=10)
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def crawl(self, query: str, max_results: int, output_dir: str = None, 
              concurrent: int = None, download_pdf: bool = None) -> None:
        """çˆ¬å–è®ºæ–‡çš„åŒæ­¥å…¥å£
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            output_dir: è¾“å‡ºç›®å½•
            concurrent: å¹¶å‘æ•°
            download_pdf: æ˜¯å¦ä¸‹è½½PDF
        """
        asyncio.run(self._crawl_async(
            query=query,
            max_results=max_results,
            output_dir=output_dir or self.output_dir,
            concurrent=concurrent or self.max_concurrent,
            download_pdf=download_pdf if download_pdf is not None else self.enable_pdf_download
        ))
    
    async def _crawl_async(self, query: str, max_results: int, output_dir: str,
                          concurrent: int, download_pdf: bool) -> None:
        """å¼‚æ­¥çˆ¬å–è®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            output_dir: è¾“å‡ºç›®å½•
            concurrent: å¹¶å‘æ•°
            download_pdf: æ˜¯å¦ä¸‹è½½PDF
        """
        async with self:
            print(f"ğŸš€ å¼€å§‹çˆ¬å–arXivè®ºæ–‡: {query}")
            print(f"ğŸ“Š æœ€å¤§ç»“æœæ•°: {max_results}, å¹¶å‘æ•°: {concurrent}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            
            # æœç´¢è®ºæ–‡
            papers = await self.search_papers(query, max_results)
            
            if not papers:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡")
                return
            
            print(f"âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            articles_dir = os.path.join(output_dir, "articles")
            data_dir = os.path.join(output_dir, "data")
            ensure_directory(articles_dir)
            ensure_directory(data_dir)
            
            # å¹¶å‘å¤„ç†è®ºæ–‡
            semaphore = asyncio.Semaphore(concurrent)
            tasks = []
            
            for paper in papers:
                task = self._process_paper_with_semaphore(
                    semaphore, paper, articles_dir, download_pdf
                )
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            processed_papers = await asyncio.gather(*tasks, return_exceptions=True)
            
            # è¿‡æ»¤æˆåŠŸå¤„ç†çš„è®ºæ–‡
            successful_papers = []
            for result in processed_papers:
                if isinstance(result, dict):
                    successful_papers.append(result)
                elif isinstance(result, Exception):
                    print(f"âŒ å¤„ç†è®ºæ–‡æ—¶å‡ºé”™: {result}")
            
            # ä¿å­˜èšåˆæ•°æ®
            await self._save_aggregated_data(successful_papers, data_dir)
            
            print(f"ğŸ‰ çˆ¬å–å®Œæˆï¼æˆåŠŸå¤„ç† {len(successful_papers)} ç¯‡è®ºæ–‡")
    
    async def _process_paper_with_semaphore(self, semaphore: asyncio.Semaphore, 
                                           paper: Dict, articles_dir: str, 
                                           download_pdf: bool) -> Dict:
        """ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘å¤„ç†è®ºæ–‡"""
        async with semaphore:
            return await self._process_single_paper(paper, articles_dir, download_pdf)
    
    async def _process_single_paper(self, paper: Dict, articles_dir: str, 
                                   download_pdf: bool) -> Dict:
        """å¤„ç†å•ç¯‡è®ºæ–‡
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            articles_dir: æ–‡ç« ç›®å½•
            download_pdf: æ˜¯å¦ä¸‹è½½PDF
            
        Returns:
            å¤„ç†åçš„è®ºæ–‡æ•°æ®
        """
        arxiv_id = paper.get('arxiv_id', 'unknown')
        title = paper.get('title', 'Unknown Title')
        
        print(f"ğŸ“„ å¤„ç†è®ºæ–‡: {arxiv_id} - {title[:50]}...")
        
        # åˆ›å»ºè®ºæ–‡ç›®å½•
        safe_title = safe_filename(title, max_length=50)
        paper_dir_name = f"{arxiv_id}_{safe_title}"
        paper_dir = os.path.join(articles_dir, paper_dir_name)
        ensure_directory(paper_dir)
        
        # åˆ›å»ºå›¾ç‰‡ç›®å½•
        images_dir = os.path.join(paper_dir, "images")
        ensure_directory(images_dir)
        
        # æ„å»ºæ–°çš„æ•°æ®æ¨¡å‹
        processed_paper = {
            "id": arxiv_id,
            "title": title,
            "abstract": paper.get('abstract', ''),
            "authors": paper.get('authors', []),
            "published": paper.get('published', ''),
            "updated": paper.get('updated', ''),
            "categories": paper.get('categories', []),
            "primary_category": paper.get('primary_category', ''),
            "url": paper.get('url', ''),
            "pdf_url": paper.get('pdf_url', ''),

            "local_images": [],
            "oss_urls": {},
            "elasticsearch_indexed": False
        }
        
        # ä¸‹è½½PDFæ–‡ä»¶
        if download_pdf and paper.get('pdf_url'):
            pdf_path = await self._download_pdf_to_paper_dir(
                paper.get('pdf_url'), arxiv_id, paper_dir
            )
            if pdf_path:
                processed_paper["local_pdf_path"] = "paper.pdf"
                processed_paper["content_hash"] = calculate_file_hash(pdf_path)
        
        # åˆ›å»ºcontent.mdæ–‡ä»¶
        content_md_path = os.path.join(paper_dir, "content.md")
        await self._create_content_md(processed_paper, content_md_path)
        
        # ä¿å­˜metadata.json
        metadata_path = os.path.join(paper_dir, "metadata.json")
        save_json(processed_paper, metadata_path)
        
        print(f"âœ… å®Œæˆå¤„ç†: {arxiv_id}")
        return processed_paper
    
    async def _download_pdf_to_paper_dir(self, pdf_url: str, arxiv_id: str, 
                                        paper_dir: str) -> Optional[str]:
        """ä¸‹è½½PDFåˆ°è®ºæ–‡ç›®å½•
        
        Args:
            pdf_url: PDFä¸‹è½½é“¾æ¥
            arxiv_id: arXiv ID
            paper_dir: è®ºæ–‡ç›®å½•
            
        Returns:
            æœ¬åœ°PDFæ–‡ä»¶è·¯å¾„
        """
        try:
            pdf_path = os.path.join(paper_dir, "paper.pdf")
            
            async with self.session.get(pdf_url) as response:
                if response.status == 200:
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    content_length = response.headers.get('content-length')
                    if content_length and int(content_length) > self.pdf_max_size:
                        print(f"âš ï¸ PDFæ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡ä¸‹è½½: {arxiv_id}")
                        return None
                    
                    pdf_content = await response.read()
                    
                    # å†æ¬¡æ£€æŸ¥å®é™…å¤§å°
                    if len(pdf_content) > self.pdf_max_size:
                        print(f"âš ï¸ PDFæ–‡ä»¶è¿‡å¤§ï¼Œè·³è¿‡ä¿å­˜: {arxiv_id}")
                        return None
                    
                    with open(pdf_path, 'wb') as f:
                        f.write(pdf_content)
                    
                    print(f"ğŸ“¥ PDFä¸‹è½½æˆåŠŸ: {arxiv_id} ({get_file_size_mb(pdf_path):.1f}MB)")
                    return pdf_path
                else:
                    print(f"âŒ PDFä¸‹è½½å¤±è´¥: {arxiv_id}, çŠ¶æ€ç : {response.status}")
                    return None
                    
        except Exception as e:
            print(f"âŒ PDFä¸‹è½½å¼‚å¸¸: {arxiv_id}, é”™è¯¯: {e}")
            return None
    
    async def _create_content_md(self, paper: Dict, content_path: str) -> None:
        """åˆ›å»ºcontent.mdæ–‡ä»¶
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            content_path: content.mdæ–‡ä»¶è·¯å¾„
        """
        content = f"""# {paper['title']}

## åŸºæœ¬ä¿¡æ¯

- **arXiv ID**: {paper['id']}
- **å‘å¸ƒæ—¥æœŸ**: {paper['published']}
- **æ›´æ–°æ—¥æœŸ**: {paper['updated']}
- **ä¸»è¦åˆ†ç±»**: {paper['primary_category']}
- **æ‰€æœ‰åˆ†ç±»**: {', '.join(paper['categories'])}
- **ä½œè€…**: {', '.join(paper['authors'])}

## é“¾æ¥

- **è®ºæ–‡é“¾æ¥**: [{paper['url']}]({paper['url']})
- **PDFé“¾æ¥**: [{paper['pdf_url']}]({paper['pdf_url']})

## æ‘˜è¦

{paper['abstract']}
"""
        
        with open(content_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    async def _save_aggregated_data(self, papers: List[Dict], data_dir: str) -> None:
        """ä¿å­˜èšåˆæ•°æ®
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            data_dir: æ•°æ®ç›®å½•
        """
        # ä¿å­˜æ‰€æœ‰è®ºæ–‡å…ƒæ•°æ®
        metadata_path = os.path.join(data_dir, "papers_metadata.json")
        save_json(papers, metadata_path)
        
        # ä¿å­˜å¤„ç†åçš„å®Œæ•´æ•°æ®
        processed_path = os.path.join(data_dir, "processed_papers.json")
        save_json(papers, processed_path)
        
        # ä¿å­˜çˆ¬å–ç»Ÿè®¡
        stats = {
            "total_papers": len(papers),
            "crawl_date": datetime.now().isoformat(),
            "categories": list(set(cat for paper in papers for cat in paper.get('categories', []))),
            "date_range": {
                "earliest": min(paper.get('published', '') for paper in papers if paper.get('published')),
                "latest": max(paper.get('published', '') for paper in papers if paper.get('published'))
            }
        }
        stats_path = os.path.join(data_dir, "crawl_stats.json")
        save_json(stats, stats_path)
        
        print(f"ğŸ“Š èšåˆæ•°æ®å·²ä¿å­˜åˆ°: {data_dir}")
    
    async def search_papers(self, query: str = "cat:cs.AI", 
                           max_results: int = 10, start: int = 0) -> List[Dict]:
        """æœç´¢arXivè®ºæ–‡
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            start: èµ·å§‹ä½ç½®
            
        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        params = {
            'search_query': query,
            'start': start,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        print(f"ğŸ” æœç´¢è®ºæ–‡: {query}, æœ€å¤§ç»“æœ: {max_results}")
        
        for attempt in range(self.max_retries):
            try:
                async with self.session.get(self.base_url, params=params) as response:
                    if response.status == 200:
                        content = await response.text()
                        papers = self._parse_xml_response(content)
                        print(f"âœ… æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                        return papers
                    else:
                        print(f"âŒ æœç´¢å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        
            except Exception as e:
                print(f"âŒ æœç´¢å¼‚å¸¸ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.request_delay)
        
        return []
    
    def _parse_xml_response(self, xml_content: str) -> List[Dict]:
        """è§£æarXiv APIè¿”å›çš„XMLå“åº”
        
        Args:
            xml_content: XMLå“åº”å†…å®¹
            
        Returns:
            è§£æåçš„è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        papers = []
        
        try:
            root = ET.fromstring(xml_content)
            
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            entries = root.findall('atom:entry', namespaces)
            
            for entry in entries:
                paper = {}
                
                # æ ‡é¢˜
                title_elem = entry.find('atom:title', namespaces)
                paper['title'] = title_elem.text.strip().replace('\n', ' ') if title_elem is not None else "æœªçŸ¥æ ‡é¢˜"
                
                # arXiv ID
                id_elem = entry.find('atom:id', namespaces)
                if id_elem is not None:
                    arxiv_id = id_elem.text.split('/')[-1]
                    paper['arxiv_id'] = arxiv_id
                    paper['url'] = f"https://arxiv.org/abs/{arxiv_id}"
                    paper['pdf_url'] = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                
                # æ‘˜è¦
                summary_elem = entry.find('atom:summary', namespaces)
                paper['abstract'] = summary_elem.text.strip().replace('\n', ' ') if summary_elem is not None else "æ— æ‘˜è¦"
                
                # ä½œè€…
                authors = []
                author_elems = entry.findall('atom:author', namespaces)
                for author_elem in author_elems:
                    name_elem = author_elem.find('atom:name', namespaces)
                    if name_elem is not None:
                        authors.append(name_elem.text)
                paper['authors'] = authors
                
                # å‘å¸ƒæ—¶é—´
                published_elem = entry.find('atom:published', namespaces)
                if published_elem is not None:
                    paper['published'] = published_elem.text[:10]
                
                # æ›´æ–°æ—¶é—´
                updated_elem = entry.find('atom:updated', namespaces)
                if updated_elem is not None:
                    paper['updated'] = updated_elem.text[:10]
                
                # åˆ†ç±»
                categories = []
                category_elems = entry.findall('atom:category', namespaces)
                for cat_elem in category_elems:
                    term = cat_elem.get('term')
                    if term:
                        categories.append(term)
                paper['categories'] = categories
                
                # ä¸»è¦åˆ†ç±»
                primary_cat_elem = entry.find('arxiv:primary_category', namespaces)
                if primary_cat_elem is not None:
                    paper['primary_category'] = primary_cat_elem.get('term')
                
                papers.append(paper)
                
        except ET.ParseError as e:
            print(f"âŒ XMLè§£æé”™è¯¯: {e}")
        except Exception as e:
            print(f"âŒ æ•°æ®è§£æå¼‚å¸¸: {e}")
            
        return papers