#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆ arXiv çˆ¬è™« - æ”¯æŒæ¯æ—¥æ›´æ–°å’Œåˆ†ç±»çˆ¬å–
åŸºäºåŸå§‹ ArxivCrawler æ‰©å±•ï¼Œå¢åŠ å¤šç§çˆ¬å–ç­–ç•¥
"""

import asyncio
import aiohttp
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Union
import os
from pathlib import Path
import logging

from .arxiv_crawler import ArxivCrawler
from ..utils.file_utils import ensure_directory, save_json


class EnhancedArxivCrawler(ArxivCrawler):
    """å¢å¼ºç‰ˆ arXiv çˆ¬è™«"""
    
    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–å¢å¼ºç‰ˆçˆ¬è™«
        
        Args:
            config: é…ç½®å­—å…¸
        """
        super().__init__(config)
        
        self.crawl_strategies = config.get("crawl_strategies", {})
        self.logger = logging.getLogger(__name__)
        
        # AIç›¸å…³åˆ†ç±»é…ç½®
        self.ai_categories = self.crawler_config.get("ai_categories", {
            "core_ai": ["cs.AI", "cs.LG", "cs.CL", "cs.CV", "cs.NE"],
            "related_ai": ["cs.IR", "cs.RO", "cs.MA", "cs.CR", "cs.HC"],
            "interdisciplinary": ["stat.ML", "q-bio.QM", "physics.comp-ph", "eess.AS", "eess.IV"]
        })
        
        # çƒ­é—¨å…³é”®è¯
        self.trending_keywords = self.crawler_config.get("trending_keywords", [
            "transformer", "diffusion", "LLM", "large language model",
            "multimodal", "RLHF", "in-context learning", "few-shot"
        ])
    
    async def crawl_daily_new_papers(self, days_back: int = 1, 
                                   categories: List[str] = None) -> Dict[str, List]:
        """çˆ¬å–æœ€è¿‘å‡ å¤©çš„æ–°è®ºæ–‡
        
        Args:
            days_back: å›æº¯å¤©æ•°
            categories: æŒ‡å®šåˆ†ç±»åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™ä½¿ç”¨æ‰€æœ‰AIåˆ†ç±»
            
        Returns:
            æŒ‰åˆ†ç±»ç»„ç»‡çš„è®ºæ–‡å­—å…¸
        """
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        self.logger.info(f"å¼€å§‹çˆ¬å– {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')} çš„æ–°è®ºæ–‡")
        print(f"ğŸš€ å¼€å§‹çˆ¬å– {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')} çš„æ–°è®ºæ–‡")
        
        # ç¡®å®šè¦çˆ¬å–çš„åˆ†ç±»
        if categories:
            target_categories = {"specified": categories}
        else:
            target_categories = self.ai_categories
        
        all_papers = {}
        total_papers = 0
        
        # éå†æ‰€æœ‰ç›®æ ‡åˆ†ç±»
        for category_group, categories_list in target_categories.items():
            print(f"\nğŸ“‚ å¤„ç†åˆ†ç±»ç»„: {category_group}")
            all_papers[category_group] = []
            
            for category in categories_list:
                papers = await self._crawl_category_by_date(
                    category, start_date, end_date
                )
                all_papers[category_group].extend(papers)
                total_papers += len(papers)
                print(f"   âœ… {category}: {len(papers)} ç¯‡è®ºæ–‡")
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                await asyncio.sleep(self.request_delay)
        
        print(f"ğŸ‰ æ¯æ—¥çˆ¬å–å®Œæˆï¼æ€»è®¡è·å– {total_papers} ç¯‡è®ºæ–‡")
        self.logger.info(f"æ¯æ—¥çˆ¬å–å®Œæˆï¼Œæ€»è®¡: {total_papers} ç¯‡è®ºæ–‡")
        
        return all_papers
    
    async def _crawl_category_by_date(self, category: str, start_date: datetime, 
                                     end_date: datetime) -> List[Dict]:
        """æŒ‰åˆ†ç±»å’Œæ—¥æœŸèŒƒå›´çˆ¬å–è®ºæ–‡
        
        Args:
            category: arXivåˆ†ç±»ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            
        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        # æ„å»ºæ—¥æœŸèŒƒå›´æŸ¥è¯¢
        start_str = start_date.strftime("%Y%m%d")
        end_str = end_date.strftime("%Y%m%d")
        
        query = f"cat:{category} AND submittedDate:[{start_str}* TO {end_str}*]"
        
        # åˆ†é¡µè·å–æ‰€æœ‰ç»“æœ
        all_papers = []
        start = 0
        max_results = 200  # æ¯æ¬¡æœ€å¤š200ç¯‡
        
        strategy_config = self.crawl_strategies.get("daily_new", {})
        max_total = strategy_config.get("max_results", 500)
        
        while len(all_papers) < max_total:
            current_batch_size = min(max_results, max_total - len(all_papers))
            
            papers = await self.search_papers(
                query=query,
                max_results=current_batch_size,
                start=start
            )
            
            if not papers:
                break
                
            all_papers.extend(papers)
            
            # å¦‚æœè¿”å›çš„ç»“æœå°‘äºè¯·æ±‚æ•°é‡ï¼Œè¯´æ˜å·²ç»åˆ°åº•äº†
            if len(papers) < current_batch_size:
                break
                
            start += current_batch_size
            await asyncio.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        return all_papers[:max_total]  # ç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
    
    async def crawl_by_categories(self, categories: List[str] = None, 
                                 max_per_category: int = 1000) -> Dict[str, List]:
        """æŒ‰åˆ†ç±»çˆ¬å–è®ºæ–‡
        
        Args:
            categories: è¦çˆ¬å–çš„åˆ†ç±»åˆ—è¡¨
            max_per_category: æ¯ä¸ªåˆ†ç±»æœ€å¤§è®ºæ–‡æ•°
            
        Returns:
            æŒ‰åˆ†ç±»ç»„ç»‡çš„è®ºæ–‡å­—å…¸
        """
        if not categories:
            # é»˜è®¤çˆ¬å–æ‰€æœ‰AIç›¸å…³åˆ†ç±»
            categories = []
            for cat_list in self.ai_categories.values():
                categories.extend(cat_list)
        
        print(f"ğŸ¯ å¼€å§‹æŒ‰åˆ†ç±»çˆ¬å–è®ºæ–‡ï¼Œå…± {len(categories)} ä¸ªåˆ†ç±»")
        self.logger.info(f"å¼€å§‹æŒ‰åˆ†ç±»çˆ¬å–ï¼Œç›®æ ‡åˆ†ç±»: {categories}")
        
        results = {}
        total_papers = 0
        
        for category in categories:
            print(f"\nğŸ“Š çˆ¬å–åˆ†ç±»: {category}")
            
            papers = await self._crawl_category_full(category, max_per_category)
            results[category] = papers
            total_papers += len(papers)
            
            print(f"   âœ… å®Œæˆ: {len(papers)} ç¯‡è®ºæ–‡")
            await asyncio.sleep(self.request_delay)  # åˆ†ç±»é—´å»¶è¿Ÿ
        
        print(f"ğŸ‰ åˆ†ç±»çˆ¬å–å®Œæˆï¼æ€»è®¡è·å– {total_papers} ç¯‡è®ºæ–‡")
        self.logger.info(f"åˆ†ç±»çˆ¬å–å®Œæˆï¼Œæ€»è®¡: {total_papers} ç¯‡è®ºæ–‡")
        
        return results
    
    async def crawl_and_save_by_categories(self, categories: List[str] = None, 
                                          max_per_category: int = 1000,
                                          output_base_dir: str = None,
                                          generate_folders: bool = True) -> Dict[str, Any]:
        """æŒ‰åˆ†ç±»çˆ¬å–è®ºæ–‡å¹¶ä¿å­˜ï¼ˆæ”¯æŒç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„ï¼‰
        
        Args:
            categories: è¦çˆ¬å–çš„åˆ†ç±»åˆ—è¡¨
            max_per_category: æ¯ä¸ªåˆ†ç±»æœ€å¤§è®ºæ–‡æ•°
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
            generate_folders: æ˜¯å¦ç”Ÿæˆå®Œæ•´çš„æ–‡ä»¶å¤¹ç»“æ„
            
        Returns:
            çˆ¬å–ç»“æœç»Ÿè®¡
        """
        if not output_base_dir:
            output_base_dir = self.output_dir
        
        # åˆ›å»ºåˆ†ç±»è¾“å‡ºç›®å½•
        category_dir = os.path.join(output_base_dir, "category")
        ensure_directory(category_dir)
        
        if generate_folders:
            articles_dir = os.path.join(output_base_dir, "articles")
            ensure_directory(articles_dir)
        
        # çˆ¬å–è®ºæ–‡
        papers_by_category = await self.crawl_by_categories(
            categories=categories, 
            max_per_category=max_per_category
        )
        
        # åˆå¹¶æ‰€æœ‰è®ºæ–‡
        all_papers = []
        for papers in papers_by_category.values():
            all_papers.extend(papers)
        
        # ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if generate_folders and all_papers:
            print(f"\nğŸ“ å¼€å§‹ç”Ÿæˆ {len(all_papers)} ç¯‡è®ºæ–‡çš„æ–‡ä»¶å¤¹ç»“æ„...")
            await self._generate_paper_folders(all_papers, articles_dir)
        
        # ä¿å­˜JSONæ•°æ®
        await self._save_category_results(papers_by_category, all_papers, category_dir)
        
        # ç»Ÿè®¡ç»“æœ
        total_papers = len(all_papers)
        
        result = {
            "success": True,
            "total_papers": total_papers,
            "categories": list(papers_by_category.keys()),
            "output_dir": category_dir,
            "articles_dir": articles_dir if generate_folders else None,
            "folder_structure_generated": generate_folders
        }
        
        self.logger.info(f"åˆ†ç±»çˆ¬å–ä¿å­˜å®Œæˆ: {result}")
        return result
    
    async def _generate_paper_folders(self, papers: List[Dict], articles_dir: str) -> None:
        """ä¸ºè®ºæ–‡åˆ—è¡¨ç”Ÿæˆå®Œæ•´çš„æ–‡ä»¶å¤¹ç»“æ„
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            articles_dir: æ–‡ç« ç›®å½•
        """
        semaphore = asyncio.Semaphore(5)  # é™åˆ¶å¹¶å‘æ•°
        tasks = []
        
        for paper in papers:
            task = self._generate_single_paper_folder(semaphore, paper, articles_dir)
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        successful = sum(1 for r in results if not isinstance(r, Exception))
        failed = len(results) - successful
        
        print(f"ğŸ“ æ–‡ä»¶å¤¹ç”Ÿæˆå®Œæˆ: æˆåŠŸ {successful}, å¤±è´¥ {failed}")
    
    async def _generate_single_paper_folder(self, semaphore: asyncio.Semaphore,
                                           paper: Dict, articles_dir: str) -> Dict:
        """ä¸ºå•ç¯‡è®ºæ–‡ç”Ÿæˆæ–‡ä»¶å¤¹ç»“æ„"""
        async with semaphore:
            return await self._create_enhanced_paper_folder(paper, articles_dir)
    
    async def _create_enhanced_paper_folder(self, paper: Dict, articles_dir: str) -> Dict:
        """åˆ›å»ºå¢å¼ºçš„è®ºæ–‡æ–‡ä»¶å¤¹ç»“æ„
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            articles_dir: æ–‡ç« ç›®å½•
            
        Returns:
            å¤„ç†åçš„è®ºæ–‡æ•°æ®
        """
        from ..utils.file_utils import safe_filename, save_json
        
        arxiv_id = paper.get('arxiv_id', 'unknown')
        title = paper.get('title', 'Unknown Title')
        
        # åˆ›å»ºè®ºæ–‡ç›®å½•
        safe_title = safe_filename(title, max_length=50)
        paper_dir_name = f"{arxiv_id}_{safe_title}"
        paper_dir = os.path.join(articles_dir, paper_dir_name)
        ensure_directory(paper_dir)
        
        # åˆ›å»ºå­ç›®å½•
        images_dir = os.path.join(paper_dir, "images")
        ensure_directory(images_dir)
        
        # æ„å»ºå¢å¼ºçš„æ•°æ®æ¨¡å‹
        processed_paper = {
            "id": arxiv_id,
            "title": title,
            "abstract": paper.get('abstract', ''),
            "authors": paper.get('authors', []),
            "published": paper.get('published', ''),
            "updated": paper.get('updated', ''),
            "categories": paper.get('categories', []),
            "primary_category": paper.get('primary_category', ''),
            "url": paper.get('url', ''),
            "pdf_url": paper.get('pdf_url', ''),
            "local_pdf_path": "",
            "content_hash": "",
            "processed_date": datetime.now().isoformat(),
            "wordcount": len(paper.get('abstract', '').split()),
            "local_images": [],
            "oss_urls": {},
            "elasticsearch_indexed": False
        }
        
        # åˆ›å»ºå¢å¼ºçš„æ–‡ä»¶ç»“æ„
        await self._create_enhanced_files(processed_paper, paper_dir)
        
        print(f"âœ… æ–‡ä»¶å¤¹åˆ›å»ºå®Œæˆ: {arxiv_id}")
        return processed_paper
    
    async def _create_enhanced_files(self, paper: Dict, paper_dir: str) -> None:
        """åˆ›å»ºå¢å¼ºçš„æ–‡ä»¶ç»“æ„
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            paper_dir: è®ºæ–‡ç›®å½•
        """
        from ..utils.file_utils import save_json
        
        # 1. åˆ›å»ºä¸»content.mdæ–‡ä»¶
        content_md_path = os.path.join(paper_dir, "content.md")
        await self._create_enhanced_content_md(paper, content_md_path)
        
        # 2. åˆ›å»ºè¯¦ç»†metadata.json
        metadata_path = os.path.join(paper_dir, "metadata.json")
        save_json(paper, metadata_path)
        
        # 3. åˆ›å»ºå•ç‹¬çš„æ‘˜è¦æ–‡ä»¶
        abstract_path = os.path.join(paper_dir, "abstract.txt")
        with open(abstract_path, 'w', encoding='utf-8') as f:
            f.write(paper.get('abstract', ''))
        
        # 4. åˆ›å»ºä½œè€…ä¿¡æ¯æ–‡ä»¶
        authors_data = {
            "authors": paper.get('authors', []),
            "author_count": len(paper.get('authors', [])),
            "first_author": paper.get('authors', [''])[0] if paper.get('authors') else '',
            "last_author": paper.get('authors', [''])[-1] if paper.get('authors') else ''
        }
        authors_path = os.path.join(paper_dir, "authors.json")
        save_json(authors_data, authors_path)
        
        # 5. åˆ›å»ºåˆ†ç±»æ ‡ç­¾æ–‡ä»¶
        categories_data = {
            "categories": paper.get('categories', []),
            "primary_category": paper.get('primary_category', ''),
            "category_count": len(paper.get('categories', [])),
            "is_ai_related": any(cat.startswith(('cs.AI', 'cs.LG', 'cs.CL', 'cs.CV', 'cs.NE')) 
                               for cat in paper.get('categories', []))
        }
        categories_path = os.path.join(paper_dir, "categories.json")
        save_json(categories_data, categories_path)
        
        # 6. åˆ›å»ºé“¾æ¥æ–‡ä»¶
        links_data = {
            "arxiv_url": paper.get('url', ''),
            "pdf_url": paper.get('pdf_url', ''),
            "arxiv_id": paper.get('id', ''),
            "doi": "",  # å¯ä»¥åç»­æ‰©å±•
            "external_links": []  # å¯ä»¥åç»­æ‰©å±•
        }
        links_path = os.path.join(paper_dir, "links.json")
        save_json(links_data, links_path)
    
    async def _create_enhanced_content_md(self, paper: Dict, content_path: str) -> None:
        """åˆ›å»ºå¢å¼ºçš„content.mdæ–‡ä»¶
        
        Args:
            paper: è®ºæ–‡æ•°æ®
            content_path: content.mdæ–‡ä»¶è·¯å¾„
        """
        content = f"""# {paper['title']}

## åŸºæœ¬ä¿¡æ¯

- **arXiv ID**: {paper['id']}
- **å‘å¸ƒæ—¥æœŸ**: {paper['published']}
- **æ›´æ–°æ—¥æœŸ**: {paper['updated']}
- **ä¸»è¦åˆ†ç±»**: {paper['primary_category']}
- **æ‰€æœ‰åˆ†ç±»**: {', '.join(paper['categories'])}
- **ä½œè€…**: {', '.join(paper['authors'])}

## é“¾æ¥

- **arXivé¡µé¢**: [{paper['url']}]({paper['url']})
- **PDFä¸‹è½½**: [{paper['pdf_url']}]({paper['pdf_url']})

## æ‘˜è¦

{paper['abstract']}

## å¤„ç†ä¿¡æ¯

- **å¤„ç†æ—¶é—´**: {paper['processed_date']}
- **å­—æ•°ç»Ÿè®¡**: {paper['wordcount']}
- **æœ¬åœ°PDF**: {paper['local_pdf_path'] or 'æœªä¸‹è½½'}
- **å†…å®¹å“ˆå¸Œ**: {paper['content_hash'] or 'æ— '}
"""
        
        with open(content_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    async def _save_category_results(self, papers_by_category: Dict[str, List], 
                                    all_papers: List[Dict], output_dir: str) -> None:
        """ä¿å­˜åˆ†ç±»çˆ¬å–ç»“æœ
        
        Args:
            papers_by_category: æŒ‰åˆ†ç±»ç»„ç»‡çš„è®ºæ–‡æ•°æ®
            all_papers: æ‰€æœ‰è®ºæ–‡åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
        """
        # ä¿å­˜æŒ‰åˆ†ç±»ç»„ç»‡çš„æ•°æ®
        save_json(papers_by_category, os.path.join(output_dir, "papers_by_category.json"))
        
        # ä¿å­˜æ‰€æœ‰è®ºæ–‡çš„åˆå¹¶æ•°æ®
        save_json(all_papers, os.path.join(output_dir, "all_papers.json"))
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "crawl_date": datetime.now().isoformat(),
            "total_papers": len(all_papers),
            "categories_stats": {
                category: len(papers) 
                for category, papers in papers_by_category.items()
            }
        }
        save_json(stats, os.path.join(output_dir, "category_stats.json"))
        
        print(f"ğŸ“Š åˆ†ç±»ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    async def _crawl_category_full(self, category: str, max_results: int) -> List[Dict]:
        """å…¨é‡çˆ¬å–æŸä¸ªåˆ†ç±»çš„è®ºæ–‡
        
        Args:
            category: arXivåˆ†ç±»ä»£ç 
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
        """
        query = f"cat:{category}"
        
        all_papers = []
        start = 0
        batch_size = 200
        
        while len(all_papers) < max_results:
            current_batch_size = min(batch_size, max_results - len(all_papers))
            
            papers = await self.search_papers(
                query=query,
                max_results=current_batch_size,
                start=start
            )
            
            if not papers:
                break
                
            all_papers.extend(papers)
            
            if len(papers) < current_batch_size:
                break
                
            start += current_batch_size
            await asyncio.sleep(1)
        
        return all_papers[:max_results]  # ç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
    
    async def crawl_trending_keywords(self, keywords: List[str] = None, 
                                    days_back: int = 7) -> Dict[str, List]:
        """çˆ¬å–çƒ­é—¨å…³é”®è¯ç›¸å…³è®ºæ–‡
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤çƒ­é—¨å…³é”®è¯
            days_back: å›æº¯å¤©æ•°
            
        Returns:
            æŒ‰å…³é”®è¯ç»„ç»‡çš„è®ºæ–‡å­—å…¸
        """
        if not keywords:
            keywords = self.trending_keywords
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        start_str = start_date.strftime("%Y%m%d")
        end_str = end_date.strftime("%Y%m%d")
        
        print(f"ğŸ”¥ çˆ¬å–çƒ­é—¨å…³é”®è¯è®ºæ–‡ï¼Œæ—¶é—´èŒƒå›´: {start_str} - {end_str}")
        print(f"ğŸ¯ ç›®æ ‡å…³é”®è¯: {', '.join(keywords)}")
        self.logger.info(f"å¼€å§‹å…³é”®è¯çˆ¬å–ï¼Œå…³é”®è¯: {keywords}, æ—¶é—´èŒƒå›´: {start_str}-{end_str}")
        
        results = {}
        total_papers = 0
        
        # æ„å»ºæ ¸å¿ƒAIåˆ†ç±»æŸ¥è¯¢æ¡ä»¶
        core_categories = " OR ".join([f"cat:{cat}" for cat in self.ai_categories["core_ai"]])
        
        for keyword in keywords:
            print(f"\nğŸ” æœç´¢å…³é”®è¯: {keyword}")
            
            # åœ¨AIç›¸å…³åˆ†ç±»ä¸­æœç´¢å…³é”®è¯
            query = f"({core_categories}) AND all:{keyword} AND submittedDate:[{start_str}* TO {end_str}*]"
            
            strategy_config = self.crawl_strategies.get("trending_weekly", {})
            max_results = strategy_config.get("max_results", 500)
            
            papers = await self.search_papers(
                query=query,
                max_results=max_results
            )
            
            results[keyword] = papers
            total_papers += len(papers)
            print(f"   âœ… æ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡")
            await asyncio.sleep(self.request_delay)
        
        print(f"ğŸ‰ å…³é”®è¯çˆ¬å–å®Œæˆï¼æ€»è®¡è·å– {total_papers} ç¯‡è®ºæ–‡")
        self.logger.info(f"å…³é”®è¯çˆ¬å–å®Œæˆï¼Œæ€»è®¡: {total_papers} ç¯‡è®ºæ–‡")
        
        return results
    
    async def crawl_and_save_daily(self, days_back: int = 1, 
                                  categories: List[str] = None,
                                  output_base_dir: str = None) -> Dict[str, Any]:
        """çˆ¬å–æ¯æ—¥æ–°è®ºæ–‡å¹¶ä¿å­˜åˆ°æŒ‡å®šç›®å½•
        
        Args:
            days_back: å›æº¯å¤©æ•°
            categories: æŒ‡å®šåˆ†ç±»åˆ—è¡¨
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
            
        Returns:
            çˆ¬å–ç»“æœç»Ÿè®¡
        """
        if not output_base_dir:
            output_base_dir = self.output_dir
        
        # åˆ›å»ºæ¯æ—¥è¾“å‡ºç›®å½•
        date_str = datetime.now().strftime("%Y-%m-%d")
        daily_dir = os.path.join(output_base_dir, "daily", date_str)
        ensure_directory(daily_dir)
        
        # çˆ¬å–è®ºæ–‡
        papers_by_category = await self.crawl_daily_new_papers(
            days_back=days_back, 
            categories=categories
        )
        
        # ä¿å­˜ç»“æœ
        await self._save_daily_results(papers_by_category, daily_dir)
        
        # ç»Ÿè®¡ç»“æœ
        total_papers = sum(len(papers) for papers in papers_by_category.values())
        
        result = {
            "success": True,
            "date": date_str,
            "total_papers": total_papers,
            "categories": list(papers_by_category.keys()),
            "output_dir": daily_dir
        }
        
        self.logger.info(f"æ¯æ—¥çˆ¬å–ä¿å­˜å®Œæˆ: {result}")
        return result
    
    async def crawl_and_save_weekly(self, keywords: List[str] = None,
                                   days_back: int = 7,
                                   output_base_dir: str = None) -> Dict[str, Any]:
        """çˆ¬å–æ¯å‘¨çƒ­é—¨å…³é”®è¯è®ºæ–‡å¹¶ä¿å­˜
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            days_back: å›æº¯å¤©æ•°
            output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•
            
        Returns:
            çˆ¬å–ç»“æœç»Ÿè®¡
        """
        if not output_base_dir:
            output_base_dir = self.output_dir
        
        # åˆ›å»ºæ¯å‘¨è¾“å‡ºç›®å½•
        week_str = datetime.now().strftime("%Y-W%U")
        weekly_dir = os.path.join(output_base_dir, "weekly", week_str)
        ensure_directory(weekly_dir)
        
        # çˆ¬å–è®ºæ–‡
        papers_by_keyword = await self.crawl_trending_keywords(
            keywords=keywords,
            days_back=days_back
        )
        
        # ä¿å­˜ç»“æœ
        await self._save_weekly_results(papers_by_keyword, weekly_dir)
        
        # ç»Ÿè®¡ç»“æœ
        total_papers = sum(len(papers) for papers in papers_by_keyword.values())
        
        result = {
            "success": True,
            "week": week_str,
            "total_papers": total_papers,
            "keywords": list(papers_by_keyword.keys()),
            "output_dir": weekly_dir
        }
        
        self.logger.info(f"æ¯å‘¨çˆ¬å–ä¿å­˜å®Œæˆ: {result}")
        return result
    
    async def _save_daily_results(self, papers_by_category: Dict[str, List], 
                                 output_dir: str) -> None:
        """ä¿å­˜æ¯æ—¥çˆ¬å–ç»“æœ
        
        Args:
            papers_by_category: æŒ‰åˆ†ç±»ç»„ç»‡çš„è®ºæ–‡æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        # ä¿å­˜åŸå§‹æ•°æ®
        save_json(papers_by_category, os.path.join(output_dir, "daily_papers_by_category.json"))
        
        # åˆå¹¶æ‰€æœ‰è®ºæ–‡
        all_papers = []
        for papers in papers_by_category.values():
            all_papers.extend(papers)
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        save_json(all_papers, os.path.join(output_dir, "daily_papers_all.json"))
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "crawl_date": datetime.now().isoformat(),
            "total_papers": len(all_papers),
            "categories_stats": {
                category: len(papers) 
                for category, papers in papers_by_category.items()
            },
            "date_range": self._get_date_range(all_papers) if all_papers else {}
        }
        save_json(stats, os.path.join(output_dir, "daily_stats.json"))
        
        print(f"ğŸ“Š æ¯æ—¥ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    async def _save_weekly_results(self, papers_by_keyword: Dict[str, List], 
                                  output_dir: str) -> None:
        """ä¿å­˜æ¯å‘¨çˆ¬å–ç»“æœ
        
        Args:
            papers_by_keyword: æŒ‰å…³é”®è¯ç»„ç»‡çš„è®ºæ–‡æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
        """
        # ä¿å­˜åŸå§‹æ•°æ®
        save_json(papers_by_keyword, os.path.join(output_dir, "weekly_papers_by_keyword.json"))
        
        # åˆå¹¶æ‰€æœ‰è®ºæ–‡å¹¶å»é‡
        all_papers = []
        seen_ids = set()
        
        for papers in papers_by_keyword.values():
            for paper in papers:
                arxiv_id = paper.get('arxiv_id')
                if arxiv_id and arxiv_id not in seen_ids:
                    all_papers.append(paper)
                    seen_ids.add(arxiv_id)
        
        # ä¿å­˜å»é‡åçš„æ•°æ®
        save_json(all_papers, os.path.join(output_dir, "weekly_papers_deduplicated.json"))
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "crawl_date": datetime.now().isoformat(),
            "total_papers": len(all_papers),
            "total_papers_with_duplicates": sum(len(papers) for papers in papers_by_keyword.values()),
            "keywords_stats": {
                keyword: len(papers) 
                for keyword, papers in papers_by_keyword.items()
            },
            "date_range": self._get_date_range(all_papers) if all_papers else {}
        }
        save_json(stats, os.path.join(output_dir, "weekly_stats.json"))
        
        print(f"ğŸ“Š æ¯å‘¨ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def _get_date_range(self, papers: List[Dict]) -> Dict[str, str]:
        """è·å–è®ºæ–‡çš„æ—¥æœŸèŒƒå›´
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            
        Returns:
            åŒ…å«æœ€æ—©å’Œæœ€æ™šæ—¥æœŸçš„å­—å…¸
        """
        if not papers:
            return {}
        
        dates = [
            paper.get('published', '') 
            for paper in papers 
            if paper.get('published')
        ]
        
        if not dates:
            return {}
        
        return {
            "earliest": min(dates),
            "latest": max(dates)
        }
    
    def get_all_ai_categories(self) -> List[str]:
        """è·å–æ‰€æœ‰AIç›¸å…³åˆ†ç±»
        
        Returns:
            æ‰€æœ‰AIåˆ†ç±»çš„åˆ—è¡¨
        """
        all_categories = []
        for cat_list in self.ai_categories.values():
            all_categories.extend(cat_list)
        return list(set(all_categories))  # å»é‡
    
    def get_category_groups(self) -> Dict[str, List[str]]:
        """è·å–åˆ†ç±»ç»„é…ç½®
        
        Returns:
            åˆ†ç±»ç»„å­—å…¸
        """
        return self.ai_categories.copy()
