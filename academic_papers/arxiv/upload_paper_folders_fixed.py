#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆï¼šæŒ‰æ–‡ä»¶å¤¹ç»“æ„ä¸Šä¼ arXivè®ºæ–‡åˆ°MinIOå’ŒElasticsearch
æ¯ç¯‡è®ºæ–‡ä¸Šä¼ ä¸»è¦æ–‡ä»¶ï¼Œä¿æŒæ¸…æ´çš„MinIOç»“æ„
"""

import json
import requests
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
import logging
from datetime import datetime
import argparse

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arxiv_system.utils.file_utils import load_config, setup_logging


class PaperFolderUploaderFixed:
    """ä¿®å¤ç‰ˆè®ºæ–‡æ–‡ä»¶å¤¹ä¸Šä¼ å™¨"""
    
    def __init__(self, config_path: str = "config_enhanced.json"):
        """åˆå§‹åŒ–ä¸Šä¼ å™¨"""
        self.config = load_config(config_path)
        self.oss_config = self.config.get("oss", {})
        
        self.api_endpoint = self.oss_config.get("api_endpoint", "http://localhost:9011")
        self.bucket_name = self.oss_config.get("bucket_name", "arxiv-papers")
        
        self.logger = logging.getLogger(__name__)
        
        print(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
        print(f"   APIåœ°å€: {self.api_endpoint}")
        print(f"   å­˜å‚¨æ¡¶: {self.bucket_name}")
    
    def find_paper_folders(self, articles_dir: str) -> List[str]:
        """æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ–‡ä»¶å¤¹"""
        paper_folders = []
        
        if not os.path.exists(articles_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {articles_dir}")
            return paper_folders
        
        for item in os.listdir(articles_dir):
            item_path = os.path.join(articles_dir, item)
            if os.path.isdir(item_path):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦æ–‡ä»¶
                content_md = os.path.join(item_path, "content.md")
                metadata_json = os.path.join(item_path, "metadata.json")
                
                if os.path.exists(content_md) and os.path.exists(metadata_json):
                    paper_folders.append(item_path)
        
        print(f"ğŸ“ æ‰¾åˆ° {len(paper_folders)} ä¸ªè®ºæ–‡æ–‡ä»¶å¤¹")
        return paper_folders
    
    def create_bucket_if_not_exists(self) -> bool:
        """åˆ›å»ºå­˜å‚¨æ¡¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        try:
            # å…ˆæ£€æŸ¥å­˜å‚¨æ¡¶æ˜¯å¦å­˜åœ¨
            response = requests.get(f"{self.api_endpoint}/api/v1/buckets", timeout=10)
            if response.status_code == 200:
                buckets_data = response.json()
                if isinstance(buckets_data, dict):
                    bucket_list = buckets_data.get("buckets", [])
                else:
                    bucket_list = buckets_data if isinstance(buckets_data, list) else []
                bucket_names = [bucket.get("name", "") if isinstance(bucket, dict) else str(bucket) for bucket in bucket_list]
                
                if self.bucket_name in bucket_names:
                    print(f"âœ… å­˜å‚¨æ¡¶ {self.bucket_name} å·²å­˜åœ¨")
                    return True
                else:
                    print(f"ğŸ”§ åˆ›å»ºå­˜å‚¨æ¡¶ {self.bucket_name}")
                    create_response = requests.post(
                        f"{self.api_endpoint}/api/v1/buckets",
                        json={"name": self.bucket_name},
                        timeout=10
                    )
                    if create_response.status_code == 201:
                        print(f"âœ… å­˜å‚¨æ¡¶ {self.bucket_name} åˆ›å»ºæˆåŠŸ")
                        return True
                    else:
                        print(f"âŒ åˆ›å»ºå­˜å‚¨æ¡¶å¤±è´¥: {create_response.status_code}")
                        return False
            else:
                print(f"âŒ è·å–å­˜å‚¨æ¡¶åˆ—è¡¨å¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥/åˆ›å»ºå­˜å‚¨æ¡¶å¼‚å¸¸: {e}")
            return False
    
    def upload_paper_folder_as_single_file(self, paper_folder: str) -> Dict[str, Any]:
        """å°†è®ºæ–‡æ–‡ä»¶å¤¹ä½œä¸ºå•ä¸ªå¢å¼ºçš„Markdownæ–‡ä»¶ä¸Šä¼ 
        
        Args:
            paper_folder: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            ä¸Šä¼ ç»“æœ
        """
        try:
            folder_name = os.path.basename(paper_folder)
            
            # è¯»å–æ‰€æœ‰ç›¸å…³æ–‡ä»¶
            files_data = self._collect_all_file_data(paper_folder)
            
            arxiv_id = files_data["metadata"].get('id', 'unknown')
            title = files_data["metadata"].get('title', 'Unknown')
            
            print(f"ğŸ“„ ä¸Šä¼ è®ºæ–‡: {arxiv_id} - {title[:50]}...")
            
            # åˆ›å»ºå¢å¼ºçš„Markdownå†…å®¹
            enhanced_markdown = self._create_enhanced_markdown(files_data)
            
            # åˆ›å»ºå®Œæ•´çš„å…ƒæ•°æ®
            complete_metadata = self._create_complete_metadata(files_data, folder_name)
            
            # ä¿æŒæ–‡ä»¶å¤¹ç»“æ„ - ä¸Šä¼ ä¸»è¦content.mdæ–‡ä»¶
            object_name = f"{folder_name}/content.md"
            
            # å‡†å¤‡ä¸Šä¼ æ•°æ®
            markdown_bytes = enhanced_markdown.encode('utf-8')
            
            files = {'file': (f"{folder_name}.md", markdown_bytes, 'text/markdown')}
            data = {
                'object_name': object_name,
                'metadata': json.dumps(complete_metadata),
                'use_pipeline': 'true'  # å¯ç”¨ESç´¢å¼•
            }
            
            response = requests.post(
                f"{self.api_endpoint}/api/v1/objects/{self.bucket_name}/upload",
                files=files,
                data=data,
                timeout=120
            )
            
            if response.status_code in [200, 201]:
                result = response.json()
                return {
                    "success": True,
                    "arxiv_id": arxiv_id,
                    "title": title,
                    "folder_name": folder_name,
                    "object_name": object_name,
                    "public_url": result.get("public_url", ""),
                    "es_indexed": result.get("es_indexed", False),
                    "es_document_id": result.get("es_document_id", "")
                }
            else:
                return {
                    "success": False,
                    "arxiv_id": arxiv_id,
                    "error": f"HTTP {response.status_code}: {response.text}"
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "arxiv_id": "unknown",
                "folder_name": os.path.basename(paper_folder) if paper_folder else "unknown"
            }
    
    def _collect_all_file_data(self, paper_folder: str) -> Dict[str, Any]:
        """æ”¶é›†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ•°æ®"""
        files_data = {
            "metadata": {},
            "content": "",
            "abstract": "",
            "authors": {},
            "categories": {},
            "links": {}
        }
        
        # è¯»å–metadata.json
        metadata_path = os.path.join(paper_folder, "metadata.json")
        if os.path.exists(metadata_path):
            with open(metadata_path, 'r', encoding='utf-8') as f:
                files_data["metadata"] = json.load(f)
        
        # è¯»å–content.md
        content_path = os.path.join(paper_folder, "content.md")
        if os.path.exists(content_path):
            with open(content_path, 'r', encoding='utf-8') as f:
                files_data["content"] = f.read()
        
        # è¯»å–abstract.txt
        abstract_path = os.path.join(paper_folder, "abstract.txt")
        if os.path.exists(abstract_path):
            with open(abstract_path, 'r', encoding='utf-8') as f:
                files_data["abstract"] = f.read()
        
        # è¯»å–authors.json
        authors_path = os.path.join(paper_folder, "authors.json")
        if os.path.exists(authors_path):
            with open(authors_path, 'r', encoding='utf-8') as f:
                files_data["authors"] = json.load(f)
        
        # è¯»å–categories.json
        categories_path = os.path.join(paper_folder, "categories.json")
        if os.path.exists(categories_path):
            with open(categories_path, 'r', encoding='utf-8') as f:
                files_data["categories"] = json.load(f)
        
        # è¯»å–links.json
        links_path = os.path.join(paper_folder, "links.json")
        if os.path.exists(links_path):
            with open(links_path, 'r', encoding='utf-8') as f:
                files_data["links"] = json.load(f)
        
        return files_data
    
    def _create_enhanced_markdown(self, files_data: Dict[str, Any]) -> str:
        """åˆ›å»ºå¢å¼ºçš„Markdownå†…å®¹ï¼ŒåŒ…å«æ‰€æœ‰ä¿¡æ¯"""
        metadata = files_data["metadata"]
        authors_data = files_data["authors"]
        categories_data = files_data["categories"]
        links_data = files_data["links"]
        
        # åŸºæœ¬ä¿¡æ¯
        title = metadata.get('title', 'Unknown Title')
        arxiv_id = metadata.get('id', 'unknown')
        published = metadata.get('published', '')
        updated = metadata.get('updated', '')
        abstract = files_data.get("abstract", metadata.get('abstract', ''))
        
        # ä½œè€…ä¿¡æ¯
        authors = metadata.get('authors', [])
        author_count = authors_data.get('author_count', len(authors))
        first_author = authors_data.get('first_author', authors[0] if authors else '')
        last_author = authors_data.get('last_author', authors[-1] if authors else '')
        
        # åˆ†ç±»ä¿¡æ¯
        categories = metadata.get('categories', [])
        primary_category = metadata.get('primary_category', '')
        is_ai_related = categories_data.get('is_ai_related', False)
        
        # é“¾æ¥ä¿¡æ¯
        arxiv_url = links_data.get('arxiv_url', metadata.get('url', ''))
        pdf_url = links_data.get('pdf_url', metadata.get('pdf_url', ''))
        
        # å¤„ç†ä¿¡æ¯
        processed_date = metadata.get('processed_date', '')
        wordcount = metadata.get('wordcount', 0)
        
        # æ„å»ºå¢å¼ºçš„Markdown
        enhanced_content = f"""# {title}

## ğŸ“‹ åŸºæœ¬ä¿¡æ¯

- **arXiv ID**: {arxiv_id}
- **å‘å¸ƒæ—¥æœŸ**: {published}
- **æ›´æ–°æ—¥æœŸ**: {updated}
- **ä¸»è¦åˆ†ç±»**: {primary_category}
- **AIç›¸å…³**: {'âœ… æ˜¯' if is_ai_related else 'âŒ å¦'}

## ğŸ‘¥ ä½œè€…ä¿¡æ¯

- **ä½œè€…æ€»æ•°**: {author_count}
- **ç¬¬ä¸€ä½œè€…**: {first_author}
- **æœ€åä½œè€…**: {last_author}
- **å®Œæ•´ä½œè€…åˆ—è¡¨**: {', '.join(authors)}

## ğŸ·ï¸ åˆ†ç±»æ ‡ç­¾

- **ä¸»è¦åˆ†ç±»**: {primary_category}
- **æ‰€æœ‰åˆ†ç±»**: {', '.join(categories)}
- **åˆ†ç±»æ€»æ•°**: {len(categories)}

## ğŸ”— ç›¸å…³é“¾æ¥

- **arXivé¡µé¢**: [{arxiv_url}]({arxiv_url})
- **PDFä¸‹è½½**: [{pdf_url}]({pdf_url})

## ğŸ“„ è®ºæ–‡æ‘˜è¦

{abstract}

## ğŸ”§ å¤„ç†ä¿¡æ¯

- **å¤„ç†æ—¶é—´**: {processed_date}
- **å­—æ•°ç»Ÿè®¡**: {wordcount}
- **æ•°æ®æ¥æº**: arXivçˆ¬è™«ç³»ç»Ÿå¢å¼ºç‰ˆ

---

*æ­¤æ–‡æ¡£åŒ…å«å®Œæ•´çš„è®ºæ–‡å…ƒæ•°æ®å’Œæ ‡ç­¾ä¿¡æ¯*
"""
        
        return enhanced_content
    
    def _create_complete_metadata(self, files_data: Dict[str, Any], folder_name: str) -> Dict[str, Any]:
        """åˆ›å»ºå®Œæ•´çš„å…ƒæ•°æ®ç”¨äºMinIOå’ŒES"""
        metadata = files_data["metadata"]
        authors_data = files_data["authors"]
        categories_data = files_data["categories"]
        
        return {
            # åŸºæœ¬ä¿¡æ¯
            "arxiv_id": metadata.get('id', ''),
            "title": metadata.get('title', ''),
            "published": metadata.get('published', ''),
            "updated": metadata.get('updated', ''),
            
            # ä½œè€…ä¿¡æ¯
            "authors": ', '.join(metadata.get('authors', [])),
            "author_count": authors_data.get('author_count', 0),
            "first_author": authors_data.get('first_author', ''),
            "last_author": authors_data.get('last_author', ''),
            
            # åˆ†ç±»ä¿¡æ¯
            "categories": ', '.join(metadata.get('categories', [])),
            "primary_category": metadata.get('primary_category', ''),
            "category_count": categories_data.get('category_count', 0),
            "is_ai_related": categories_data.get('is_ai_related', False),
            
            # æŠ€æœ¯ä¿¡æ¯
            "wordcount": metadata.get('wordcount', 0),
            "processed_date": metadata.get('processed_date', ''),
            "folder_name": folder_name,
            "source": "arxiv_crawler_enhanced",
            
            # é“¾æ¥ä¿¡æ¯
            "arxiv_url": metadata.get('url', ''),
            "pdf_url": metadata.get('pdf_url', ''),
            
            # æ–‡ä»¶ä¿¡æ¯
            "file_type": "markdown",
            "content_type": "text/markdown"
        }
    
    def upload_paper_folders_batch(self, paper_folders: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡ä¸Šä¼ è®ºæ–‡æ–‡ä»¶å¤¹"""
        print(f"ğŸ“š å¼€å§‹æ‰¹é‡ä¸Šä¼  {len(paper_folders)} ä¸ªè®ºæ–‡æ–‡ä»¶å¤¹...")
        
        total_uploaded = 0
        total_failed = 0
        failed_folders = []
        successful_folders = []
        
        for i, paper_folder in enumerate(paper_folders, 1):
            folder_name = os.path.basename(paper_folder)
            print(f"\nğŸ“ ä¸Šä¼ è¿›åº¦ {i}/{len(paper_folders)}: {folder_name}")
            
            result = self.upload_paper_folder_as_single_file(paper_folder)
            
            if result["success"]:
                total_uploaded += 1
                successful_folders.append(result)
                print(f"   âœ… ä¸Šä¼ æˆåŠŸ")
                if result.get("es_indexed"):
                    print(f"   ğŸ“Š å·²ç´¢å¼•åˆ°Elasticsearch")
                if result.get("public_url"):
                    print(f"   ğŸ”— å…¬å¼€URL: {result['public_url']}")
            else:
                total_failed += 1
                failed_folders.append(result)
                print(f"   âŒ ä¸Šä¼ å¤±è´¥: {result.get('error', 'Unknown error')}")
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i % 10 == 0:
                print(f"   ğŸ’¤ ä¼‘æ¯2ç§’...")
                import time
                time.sleep(2)
        
        # æ±‡æ€»ç»“æœ
        result = {
            "total_folders": len(paper_folders),
            "uploaded_count": total_uploaded,
            "failed_count": total_failed,
            "failed_folders": failed_folders,
            "successful_folders": successful_folders,
            "success_rate": (total_uploaded / len(paper_folders)) * 100 if len(paper_folders) > 0 else 0
        }
        
        print(f"\nğŸ¯ ä¸Šä¼ å®Œæˆç»Ÿè®¡:")
        print(f"   æ€»æ–‡ä»¶å¤¹æ•°: {result['total_folders']}")
        print(f"   æˆåŠŸä¸Šä¼ : {result['uploaded_count']}")
        print(f"   å¤±è´¥æ•°é‡: {result['failed_count']}")
        print(f"   æˆåŠŸç‡: {result['success_rate']:.1f}%")
        
        return result


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¿®å¤ç‰ˆï¼šæŒ‰æ–‡ä»¶å¤¹ç»“æ„ä¸Šä¼ arXivè®ºæ–‡")
    parser.add_argument("--articles-dir", default="crawled_data/articles", help="è®ºæ–‡æ–‡ä»¶å¤¹ç›®å½•")
    parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    print("ğŸš€ ä¿®å¤ç‰ˆè®ºæ–‡æ–‡ä»¶å¤¹ä¸Šä¼ å·¥å…·")
    print("=" * 50)
    
    # åˆå§‹åŒ–ä¸Šä¼ å™¨
    uploader = PaperFolderUploaderFixed(args.config)
    
    # åˆ›å»ºå­˜å‚¨æ¡¶
    if not uploader.create_bucket_if_not_exists():
        print("âŒ å­˜å‚¨æ¡¶åˆ›å»º/æ£€æŸ¥å¤±è´¥")
        sys.exit(1)
    
    # æŸ¥æ‰¾è®ºæ–‡æ–‡ä»¶å¤¹
    paper_folders = uploader.find_paper_folders(args.articles_dir)
    if not paper_folders:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®ºæ–‡æ–‡ä»¶å¤¹")
        sys.exit(1)
    
    # ä¸Šä¼ è®ºæ–‡æ–‡ä»¶å¤¹
    result = uploader.upload_paper_folders_batch(paper_folders)
    
    # ä¿å­˜ä¸Šä¼ ç»“æœ
    result_file = f"fixed_folder_upload_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ ä¸Šä¼ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    if result["success_rate"] >= 90:
        print("ğŸ‰ ä¸Šä¼ æˆåŠŸï¼")
        sys.exit(0)
    else:
        print("âš ï¸ éƒ¨åˆ†ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)


if __name__ == "__main__":
    main()
