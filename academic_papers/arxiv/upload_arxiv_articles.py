#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXivè®ºæ–‡æ–‡ç« ä¸Šä¼ åˆ°MinIOå’ŒElasticsearch
å°†è®ºæ–‡è½¬æ¢ä¸ºMarkdownæ–‡ä»¶åä¸Šä¼ ï¼Œåˆ©ç”¨æ–‡æ¡£å¤„ç†ç®¡é“è‡ªåŠ¨ç´¢å¼•
"""

import json
import requests
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
import logging
from datetime import datetime
import argparse

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arxiv_system.utils.file_utils import load_config, setup_logging, ensure_directory


class ArxivArticleUploader:
    """arXivè®ºæ–‡æ–‡ç« ä¸Šä¼ å™¨"""
    
    def __init__(self, config_path: str = "config_enhanced.json"):
        """åˆå§‹åŒ–ä¸Šä¼ å™¨"""
        self.config = load_config(config_path)
        self.oss_config = self.config.get("oss", {})
        
        self.api_endpoint = self.oss_config.get("api_endpoint", "http://localhost:9011")
        self.bucket_name = self.oss_config.get("bucket_name", "arxiv-papers")
        
        self.logger = logging.getLogger(__name__)
        
        print(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
        print(f"   APIåœ°å€: {self.api_endpoint}")
        print(f"   å­˜å‚¨æ¡¶: {self.bucket_name}")
    
    def check_service_health(self) -> bool:
        """æ£€æŸ¥MinIOè¿æ¥å™¨æœåŠ¡å¥åº·çŠ¶æ€"""
        try:
            response = requests.get(f"{self.api_endpoint}/health", timeout=10)
            if response.status_code == 200:
                print("âœ… MinIOè¿æ¥å™¨æœåŠ¡æ­£å¸¸è¿è¡Œ")
                return True
            else:
                print(f"âŒ MinIOè¿æ¥å™¨æœåŠ¡å¼‚å¸¸ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ°MinIOè¿æ¥å™¨æœåŠ¡: {e}")
            return False
    
    def load_papers_from_file(self, file_path: str) -> List[Dict[str, Any]]:
        """ä»JSONæ–‡ä»¶åŠ è½½è®ºæ–‡æ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                papers = json.load(f)
            
            if isinstance(papers, list):
                print(f"ğŸ“„ ä» {file_path} åŠ è½½äº† {len(papers)} ç¯‡è®ºæ–‡")
                return papers
            else:
                print(f"âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åˆ—è¡¨æ ¼å¼")
                return []
                
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []
    
    def create_bucket_if_not_exists(self) -> bool:
        """åˆ›å»ºå­˜å‚¨æ¡¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        try:
            # å…ˆæ£€æŸ¥å­˜å‚¨æ¡¶æ˜¯å¦å­˜åœ¨
            response = requests.get(f"{self.api_endpoint}/api/v1/buckets", timeout=10)
            if response.status_code == 200:
                buckets_data = response.json()
                if isinstance(buckets_data, dict):
                    bucket_list = buckets_data.get("buckets", [])
                else:
                    bucket_list = buckets_data if isinstance(buckets_data, list) else []
                bucket_names = [bucket.get("name", "") if isinstance(bucket, dict) else str(bucket) for bucket in bucket_list]
                
                if self.bucket_name in bucket_names:
                    print(f"âœ… å­˜å‚¨æ¡¶ {self.bucket_name} å·²å­˜åœ¨")
                    return True
                else:
                    print(f"ğŸ”§ åˆ›å»ºå­˜å‚¨æ¡¶ {self.bucket_name}")
                    create_response = requests.post(
                        f"{self.api_endpoint}/api/v1/buckets",
                        json={"name": self.bucket_name},
                        timeout=10
                    )
                    if create_response.status_code == 201:
                        print(f"âœ… å­˜å‚¨æ¡¶ {self.bucket_name} åˆ›å»ºæˆåŠŸ")
                        return True
                    else:
                        print(f"âŒ åˆ›å»ºå­˜å‚¨æ¡¶å¤±è´¥: {create_response.status_code}")
                        return False
            else:
                print(f"âŒ è·å–å­˜å‚¨æ¡¶åˆ—è¡¨å¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥/åˆ›å»ºå­˜å‚¨æ¡¶å¼‚å¸¸: {e}")
            return False
    
    def convert_paper_to_markdown(self, paper: Dict[str, Any]) -> str:
        """å°†è®ºæ–‡æ•°æ®è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        title = paper.get("title", "").strip()
        abstract = paper.get("abstract", "").strip()
        authors = paper.get("authors", [])
        arxiv_id = paper.get("arxiv_id", "")
        url = paper.get("url", "")
        pdf_url = paper.get("pdf_url", "")
        published = paper.get("published", "")
        categories = paper.get("categories", [])
        primary_category = paper.get("primary_category", "")
        
        # ç”ŸæˆMarkdownå†…å®¹
        markdown_content = f"""# {title}

## åŸºæœ¬ä¿¡æ¯

- **arXiv ID**: {arxiv_id}
- **å‘å¸ƒæ—¥æœŸ**: {published}
- **ä¸»è¦åˆ†ç±»**: {primary_category}
- **æ‰€æœ‰åˆ†ç±»**: {', '.join(categories)}
- **ä½œè€…**: {', '.join(authors)}

## é“¾æ¥

- **arXivé¡µé¢**: {url}
- **PDFä¸‹è½½**: {pdf_url}

## æ‘˜è¦

{abstract}
"""
        
        return markdown_content
    
    def upload_paper_as_markdown(self, paper: Dict[str, Any], temp_dir: str) -> Dict[str, Any]:
        """å°†å•ç¯‡è®ºæ–‡è½¬æ¢ä¸ºMarkdownå¹¶ä¸Šä¼ """
        try:
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            title = paper.get("title", "").strip()
            abstract = paper.get("abstract", "").strip()
            arxiv_id = paper.get("arxiv_id", "")
            
            if not title or not abstract or not arxiv_id:
                return {
                    "success": False,
                    "error": "ç¼ºå°‘å¿…è¦å­—æ®µ",
                    "arxiv_id": arxiv_id
                }
            
            # è½¬æ¢ä¸ºMarkdown
            markdown_content = self.convert_paper_to_markdown(paper)
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            safe_title = "".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()[:50]
            filename = f"{arxiv_id}_{safe_title}.md"
            file_path = os.path.join(temp_dir, filename)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            # ä¸Šä¼ åˆ°MinIO
            object_name = f"arxiv/{datetime.now().strftime('%Y/%m')}/{filename}"
            
            with open(file_path, 'rb') as f:
                files = {'file': (filename, f, 'text/markdown')}
                data = {
                    'object_name': object_name,
                    'use_pipeline': 'true'  # å¯ç”¨æ–‡æ¡£å¤„ç†ç®¡é“
                }
                
                response = requests.post(
                    f"{self.api_endpoint}/api/v1/objects/{self.bucket_name}/upload",
                    files=files,
                    data=data,
                    timeout=60
                )
            
            # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(file_path)
            except:
                pass
            
            if response.status_code in [200, 201]:
                result = response.json()
                return {
                    "success": True,
                    "title": title,
                    "arxiv_id": arxiv_id,
                    "object_name": object_name,
                    "public_url": result.get("public_url", ""),
                    "es_indexed": result.get("es_indexed", False),
                    "es_document_id": result.get("es_document_id", "")
                }
            else:
                return {
                    "success": False,
                    "error": f"ä¸Šä¼ å¤±è´¥ HTTP {response.status_code}: {response.text}",
                    "arxiv_id": arxiv_id
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "arxiv_id": paper.get("arxiv_id", "unknown")
            }
    
    def upload_papers_batch(self, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰¹é‡ä¸Šä¼ è®ºæ–‡"""
        print(f"ğŸ“š å¼€å§‹æ‰¹é‡ä¸Šä¼  {len(papers)} ç¯‡è®ºæ–‡...")
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = "temp_markdown_files"
        ensure_directory(temp_dir)
        
        total_uploaded = 0
        total_failed = 0
        failed_papers = []
        successful_papers = []
        
        try:
            for i, paper in enumerate(papers, 1):
                print(f"\nğŸ“„ ä¸Šä¼ è¿›åº¦ {i}/{len(papers)}: {paper.get('title', 'Unknown')[:60]}...")
                
                result = self.upload_paper_as_markdown(paper, temp_dir)
                
                if result["success"]:
                    total_uploaded += 1
                    successful_papers.append(result)
                    print(f"   âœ… ä¸Šä¼ æˆåŠŸ")
                    if result.get("es_indexed"):
                        print(f"   ğŸ“Š å·²ç´¢å¼•åˆ°Elasticsearch")
                    if result.get("public_url"):
                        print(f"   ğŸ”— å…¬å¼€URL: {result['public_url']}")
                else:
                    total_failed += 1
                    failed_papers.append(result)
                    print(f"   âŒ ä¸Šä¼ å¤±è´¥: {result.get('error', 'Unknown error')}")
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                if i % 10 == 0:
                    print(f"   ğŸ’¤ ä¼‘æ¯2ç§’...")
                    import time
                    time.sleep(2)
        
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            try:
                import shutil
                shutil.rmtree(temp_dir)
            except:
                pass
        
        # æ±‡æ€»ç»“æœ
        result = {
            "total_papers": len(papers),
            "uploaded_count": total_uploaded,
            "failed_count": total_failed,
            "failed_papers": failed_papers,
            "successful_papers": successful_papers,
            "success_rate": (total_uploaded / len(papers)) * 100 if len(papers) > 0 else 0
        }
        
        print(f"\nğŸ¯ ä¸Šä¼ å®Œæˆç»Ÿè®¡:")
        print(f"   æ€»è®ºæ–‡æ•°: {result['total_papers']}")
        print(f"   æˆåŠŸä¸Šä¼ : {result['uploaded_count']}")
        print(f"   å¤±è´¥æ•°é‡: {result['failed_count']}")
        print(f"   æˆåŠŸç‡: {result['success_rate']:.1f}%")
        
        return result
    
    def search_uploaded_papers(self, query: str = "arxiv", size: int = 10) -> Dict[str, Any]:
        """æœç´¢å·²ä¸Šä¼ çš„è®ºæ–‡"""
        try:
            response = requests.get(
                f"{self.api_endpoint}/api/v1/documents/search",
                params={
                    "query": query,
                    "size": size,
                    "bucket_name": self.bucket_name
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                print(f"ğŸ” æœç´¢ç»“æœ:")
                print(f"   æ€»æ–‡æ¡£æ•°: {result.get('total', 0)}")
                
                documents = result.get('documents', [])
                if documents:
                    print(f"   è¿”å›ç»“æœ: {len(documents)}")
                    print(f"   æœ€æ–°è®ºæ–‡ç¤ºä¾‹:")
                    for i, doc in enumerate(documents[:3], 1):
                        title = doc.get("title", "Unknown")[:60]
                        score = doc.get("_score", 0)
                        print(f"      {i}. {title}... (è¯„åˆ†: {score:.2f})")
                
                return result
            else:
                print(f"âŒ æœç´¢å¤±è´¥: HTTP {response.status_code}")
                return {}
                
        except Exception as e:
            print(f"âŒ æœç´¢å¼‚å¸¸: {e}")
            return {}
    
    def get_document_stats(self) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        try:
            response = requests.get(
                f"{self.api_endpoint}/api/v1/documents/stats",
                timeout=30
            )
            
            if response.status_code == 200:
                stats = response.json()
                print(f"ğŸ“Š æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯:")
                print(f"   æ€»æ–‡æ¡£æ•°: {stats.get('total_documents', 0)}")
                print(f"   å¹³å‡å­—æ•°: {stats.get('average_word_count', 0)}")
                print(f"   æ€»å­˜å‚¨å¤§å°: {stats.get('total_size_bytes', 0)} å­—èŠ‚")
                
                by_bucket = stats.get('by_bucket', [])
                if by_bucket:
                    print(f"   æŒ‰å­˜å‚¨æ¡¶åˆ†å¸ƒ:")
                    for bucket_stat in by_bucket:
                        print(f"      {bucket_stat['bucket']}: {bucket_stat['count']} ä¸ªæ–‡æ¡£")
                
                return stats
            else:
                print(f"âŒ è·å–ç»Ÿè®¡å¤±è´¥: HTTP {response.status_code}")
                return {}
                
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡å¼‚å¸¸: {e}")
            return {}


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¸Šä¼ arXivè®ºæ–‡åˆ°MinIOå’ŒElasticsearch")
    parser.add_argument("--file", required=True, help="è®ºæ–‡JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--search", action="store_true", help="ä¸Šä¼ åæœç´¢éªŒè¯")
    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯")
    parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    print("ğŸš€ arXivè®ºæ–‡æ–‡ç« ä¸Šä¼ å·¥å…·")
    print("=" * 50)
    
    # åˆå§‹åŒ–ä¸Šä¼ å™¨
    uploader = ArxivArticleUploader(args.config)
    
    # æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
    if not uploader.check_service_health():
        print("âŒ MinIOè¿æ¥å™¨æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡")
        sys.exit(1)
    
    # åˆ›å»ºå­˜å‚¨æ¡¶
    if not uploader.create_bucket_if_not_exists():
        print("âŒ å­˜å‚¨æ¡¶åˆ›å»º/æ£€æŸ¥å¤±è´¥")
        sys.exit(1)
    
    # åŠ è½½è®ºæ–‡æ•°æ®
    papers = uploader.load_papers_from_file(args.file)
    if not papers:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®ºæ–‡æ•°æ®")
        sys.exit(1)
    
    # ä¸Šä¼ è®ºæ–‡
    result = uploader.upload_papers_batch(papers)
    
    # æœç´¢éªŒè¯
    if args.search:
        print("\n" + "=" * 50)
        uploader.search_uploaded_papers("arxiv", 5)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if args.stats:
        print("\n" + "=" * 50)
        uploader.get_document_stats()
    
    # ä¿å­˜ä¸Šä¼ ç»“æœ
    result_file = f"upload_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ ä¸Šä¼ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    if result["success_rate"] >= 90:
        print("ğŸ‰ ä¸Šä¼ æˆåŠŸï¼")
        sys.exit(0)
    else:
        print("âš ï¸ éƒ¨åˆ†ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)


if __name__ == "__main__":
    main()
