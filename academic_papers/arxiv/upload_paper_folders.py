#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒ‰æ–‡ä»¶å¤¹ç»“æ„ä¸Šä¼ arXivè®ºæ–‡åˆ°MinIOå’ŒElasticsearch
æ¯ç¯‡è®ºæ–‡ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ŒåŒ…å«å®Œæ•´çš„å…ƒæ•°æ®å’Œæ ‡ç­¾
"""

import json
import requests
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
import logging
from datetime import datetime
import argparse
import glob

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arxiv_system.utils.file_utils import load_config, setup_logging


class PaperFolderUploader:
    """è®ºæ–‡æ–‡ä»¶å¤¹ä¸Šä¼ å™¨"""
    
    def __init__(self, config_path: str = "config_enhanced.json"):
        """åˆå§‹åŒ–ä¸Šä¼ å™¨"""
        self.config = load_config(config_path)
        self.oss_config = self.config.get("oss", {})
        
        self.api_endpoint = self.oss_config.get("api_endpoint", "http://localhost:9011")
        self.bucket_name = self.oss_config.get("bucket_name", "arxiv-papers")
        
        self.logger = logging.getLogger(__name__)
        
        print(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
        print(f"   APIåœ°å€: {self.api_endpoint}")
        print(f"   å­˜å‚¨æ¡¶: {self.bucket_name}")
    
    def find_paper_folders(self, articles_dir: str) -> List[str]:
        """æŸ¥æ‰¾æ‰€æœ‰è®ºæ–‡æ–‡ä»¶å¤¹"""
        paper_folders = []
        
        if not os.path.exists(articles_dir):
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {articles_dir}")
            return paper_folders
        
        for item in os.listdir(articles_dir):
            item_path = os.path.join(articles_dir, item)
            if os.path.isdir(item_path):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦æ–‡ä»¶
                content_md = os.path.join(item_path, "content.md")
                metadata_json = os.path.join(item_path, "metadata.json")
                
                if os.path.exists(content_md) and os.path.exists(metadata_json):
                    paper_folders.append(item_path)
        
        print(f"ğŸ“ æ‰¾åˆ° {len(paper_folders)} ä¸ªè®ºæ–‡æ–‡ä»¶å¤¹")
        return paper_folders
    
    def create_bucket_if_not_exists(self) -> bool:
        """åˆ›å»ºå­˜å‚¨æ¡¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        try:
            # å…ˆæ£€æŸ¥å­˜å‚¨æ¡¶æ˜¯å¦å­˜åœ¨
            response = requests.get(f"{self.api_endpoint}/api/v1/buckets", timeout=10)
            if response.status_code == 200:
                buckets_data = response.json()
                if isinstance(buckets_data, dict):
                    bucket_list = buckets_data.get("buckets", [])
                else:
                    bucket_list = buckets_data if isinstance(buckets_data, list) else []
                bucket_names = [bucket.get("name", "") if isinstance(bucket, dict) else str(bucket) for bucket in bucket_list]
                
                if self.bucket_name in bucket_names:
                    print(f"âœ… å­˜å‚¨æ¡¶ {self.bucket_name} å·²å­˜åœ¨")
                    return True
                else:
                    print(f"ğŸ”§ åˆ›å»ºå­˜å‚¨æ¡¶ {self.bucket_name}")
                    create_response = requests.post(
                        f"{self.api_endpoint}/api/v1/buckets",
                        json={"name": self.bucket_name},
                        timeout=10
                    )
                    if create_response.status_code == 201:
                        print(f"âœ… å­˜å‚¨æ¡¶ {self.bucket_name} åˆ›å»ºæˆåŠŸ")
                        return True
                    else:
                        print(f"âŒ åˆ›å»ºå­˜å‚¨æ¡¶å¤±è´¥: {create_response.status_code}")
                        return False
            else:
                print(f"âŒ è·å–å­˜å‚¨æ¡¶åˆ—è¡¨å¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"âŒ æ£€æŸ¥/åˆ›å»ºå­˜å‚¨æ¡¶å¼‚å¸¸: {e}")
            return False
    
    def upload_paper_folder(self, paper_folder: str) -> Dict[str, Any]:
        """ä¸Šä¼ å•ä¸ªè®ºæ–‡æ–‡ä»¶å¤¹
        
        Args:
            paper_folder: è®ºæ–‡æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            ä¸Šä¼ ç»“æœ
        """
        try:
            folder_name = os.path.basename(paper_folder)
            
            # è¯»å–metadata.json
            metadata_path = os.path.join(paper_folder, "metadata.json")
            with open(metadata_path, 'r', encoding='utf-8') as f:
                paper_metadata = json.load(f)
            
            arxiv_id = paper_metadata.get('id', 'unknown')
            title = paper_metadata.get('title', 'Unknown')
            
            print(f"ğŸ“„ ä¸Šä¼ è®ºæ–‡æ–‡ä»¶å¤¹: {arxiv_id} - {title[:50]}...")
            
            # æ”¶é›†æ‰€æœ‰éœ€è¦ä¸Šä¼ çš„æ–‡ä»¶
            files_to_upload = self._collect_folder_files(paper_folder)
            
            upload_results = []
            
            # é€ä¸ªä¸Šä¼ æ–‡ä»¶
            for file_info in files_to_upload:
                result = self._upload_single_file(file_info, folder_name, paper_metadata)
                upload_results.append(result)
                
                if not result["success"]:
                    print(f"   âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {file_info['relative_path']}")
                else:
                    print(f"   âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file_info['relative_path']}")
            
            # ç»Ÿè®¡ç»“æœ
            successful_uploads = sum(1 for r in upload_results if r["success"])
            total_files = len(upload_results)
            
            return {
                "success": successful_uploads == total_files,
                "arxiv_id": arxiv_id,
                "title": title,
                "folder_name": folder_name,
                "total_files": total_files,
                "successful_uploads": successful_uploads,
                "upload_results": upload_results
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "arxiv_id": "unknown",
                "folder_name": os.path.basename(paper_folder) if paper_folder else "unknown"
            }
    
    def _collect_folder_files(self, paper_folder: str) -> List[Dict[str, str]]:
        """æ”¶é›†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        files_info = []
        
        for root, dirs, files in os.walk(paper_folder):
            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, paper_folder)
                
                # ç¡®å®šæ–‡ä»¶ç±»å‹
                if file.endswith('.md'):
                    content_type = 'text/markdown'
                elif file.endswith('.json'):
                    content_type = 'application/json'
                elif file.endswith('.txt'):
                    content_type = 'text/plain'
                elif file.endswith(('.png', '.jpg', '.jpeg', '.gif')):
                    content_type = 'image/*'
                elif file.endswith('.pdf'):
                    content_type = 'application/pdf'
                else:
                    content_type = 'application/octet-stream'
                
                files_info.append({
                    "file_path": file_path,
                    "relative_path": relative_path,
                    "filename": file,
                    "content_type": content_type
                })
        
        return files_info
    
    def _upload_single_file(self, file_info: Dict[str, str], 
                           folder_name: str, paper_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
        try:
            file_path = file_info["file_path"]
            relative_path = file_info["relative_path"]
            content_type = file_info["content_type"]
            
            # æ„å»ºMinIOå¯¹è±¡åï¼ˆä¿æŒæ–‡ä»¶å¤¹ç»“æ„ï¼‰
            object_name = f"{folder_name}/{relative_path}"
            
            # å‡†å¤‡å…ƒæ•°æ®
            metadata = {
                "arxiv_id": paper_metadata.get('id', ''),
                "title": paper_metadata.get('title', ''),
                "authors": ', '.join(paper_metadata.get('authors', [])),
                "categories": ', '.join(paper_metadata.get('categories', [])),
                "primary_category": paper_metadata.get('primary_category', ''),
                "published": paper_metadata.get('published', ''),
                "file_type": relative_path.split('.')[-1] if '.' in relative_path else 'unknown',
                "relative_path": relative_path,
                "source": "arxiv_crawler_enhanced"
            }
            
            # ä¸Šä¼ æ–‡ä»¶
            with open(file_path, 'rb') as f:
                files = {'file': (relative_path, f, content_type)}
                data = {
                    'object_name': object_name,
                    'metadata': json.dumps(metadata),
                    'use_pipeline': 'true' if relative_path == 'content.md' else 'false'
                }
                
                response = requests.post(
                    f"{self.api_endpoint}/api/v1/objects/{self.bucket_name}/upload",
                    files=files,
                    data=data,
                    timeout=60
                )
            
            if response.status_code in [200, 201]:
                result = response.json()
                return {
                    "success": True,
                    "relative_path": relative_path,
                    "object_name": object_name,
                    "public_url": result.get("public_url", ""),
                    "es_indexed": result.get("es_indexed", False)
                }
            else:
                return {
                    "success": False,
                    "relative_path": relative_path,
                    "error": f"HTTP {response.status_code}: {response.text}"
                }
                
        except Exception as e:
            return {
                "success": False,
                "relative_path": file_info.get("relative_path", "unknown"),
                "error": str(e)
            }
    
    def upload_paper_folders_batch(self, paper_folders: List[str]) -> Dict[str, Any]:
        """æ‰¹é‡ä¸Šä¼ è®ºæ–‡æ–‡ä»¶å¤¹"""
        print(f"ğŸ“š å¼€å§‹æ‰¹é‡ä¸Šä¼  {len(paper_folders)} ä¸ªè®ºæ–‡æ–‡ä»¶å¤¹...")
        
        total_uploaded = 0
        total_failed = 0
        failed_folders = []
        successful_folders = []
        
        for i, paper_folder in enumerate(paper_folders, 1):
            folder_name = os.path.basename(paper_folder)
            print(f"\nğŸ“ ä¸Šä¼ è¿›åº¦ {i}/{len(paper_folders)}: {folder_name}")
            
            result = self.upload_paper_folder(paper_folder)
            
            if result["success"]:
                total_uploaded += 1
                successful_folders.append(result)
                print(f"   âœ… æ–‡ä»¶å¤¹ä¸Šä¼ æˆåŠŸ: {result['successful_uploads']}/{result['total_files']} ä¸ªæ–‡ä»¶")
            else:
                total_failed += 1
                failed_folders.append(result)
                print(f"   âŒ æ–‡ä»¶å¤¹ä¸Šä¼ å¤±è´¥: {result.get('error', 'Unknown error')}")
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i % 5 == 0:
                print(f"   ğŸ’¤ ä¼‘æ¯2ç§’...")
                import time
                time.sleep(2)
        
        # æ±‡æ€»ç»“æœ
        result = {
            "total_folders": len(paper_folders),
            "uploaded_count": total_uploaded,
            "failed_count": total_failed,
            "failed_folders": failed_folders,
            "successful_folders": successful_folders,
            "success_rate": (total_uploaded / len(paper_folders)) * 100 if len(paper_folders) > 0 else 0
        }
        
        print(f"\nğŸ¯ ä¸Šä¼ å®Œæˆç»Ÿè®¡:")
        print(f"   æ€»æ–‡ä»¶å¤¹æ•°: {result['total_folders']}")
        print(f"   æˆåŠŸä¸Šä¼ : {result['uploaded_count']}")
        print(f"   å¤±è´¥æ•°é‡: {result['failed_count']}")
        print(f"   æˆåŠŸç‡: {result['success_rate']:.1f}%")
        
        return result


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æŒ‰æ–‡ä»¶å¤¹ç»“æ„ä¸Šä¼ arXivè®ºæ–‡")
    parser.add_argument("--articles-dir", default="crawled_data/articles", help="è®ºæ–‡æ–‡ä»¶å¤¹ç›®å½•")
    parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    print("ğŸš€ è®ºæ–‡æ–‡ä»¶å¤¹ä¸Šä¼ å·¥å…·")
    print("=" * 50)
    
    # åˆå§‹åŒ–ä¸Šä¼ å™¨
    uploader = PaperFolderUploader(args.config)
    
    # åˆ›å»ºå­˜å‚¨æ¡¶
    if not uploader.create_bucket_if_not_exists():
        print("âŒ å­˜å‚¨æ¡¶åˆ›å»º/æ£€æŸ¥å¤±è´¥")
        sys.exit(1)
    
    # æŸ¥æ‰¾è®ºæ–‡æ–‡ä»¶å¤¹
    paper_folders = uploader.find_paper_folders(args.articles_dir)
    if not paper_folders:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®ºæ–‡æ–‡ä»¶å¤¹")
        sys.exit(1)
    
    # ä¸Šä¼ è®ºæ–‡æ–‡ä»¶å¤¹
    result = uploader.upload_paper_folders_batch(paper_folders)
    
    # ä¿å­˜ä¸Šä¼ ç»“æœ
    result_file = f"folder_upload_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ ä¸Šä¼ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    if result["success_rate"] >= 90:
        print("ğŸ‰ ä¸Šä¼ æˆåŠŸï¼")
        sys.exit(0)
    else:
        print("âš ï¸ éƒ¨åˆ†ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)


if __name__ == "__main__":
    main()
