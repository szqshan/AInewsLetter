#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆåˆ†ç±»æ‰¹é‡çˆ¬å–è„šæœ¬
ç”¨äºå¿«é€ŸæŒ‰åˆ†ç±»çˆ¬å–arXivè®ºæ–‡
"""

import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path


def run_command(command):
    """æ‰§è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {command}")
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        if result.returncode == 0:
            print("âœ… å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
            return True
        else:
            print(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {result.stderr}")
            return False
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¼‚å¸¸: {e}")
        return False


def batch_crawl_categories():
    """æ‰¹é‡æŒ‰åˆ†ç±»çˆ¬å–"""
    
    # è¦çˆ¬å–çš„åˆ†ç±»
    categories = [
        "cs.AI",   # äººå·¥æ™ºèƒ½
        "cs.LG",   # æœºå™¨å­¦ä¹ 
        "cs.CL",   # è®¡ç®—ä¸è¯­è¨€
        "cs.CV",   # è®¡ç®—æœºè§†è§‰
        "cs.NE"    # ç¥ç»å’Œè¿›åŒ–è®¡ç®—
    ]
    
    # çˆ¬å–å‚æ•°
    max_results_per_category = 200  # æ¯ä¸ªåˆ†ç±»æœ€å¤§æ•°é‡
    concurrent = 3                  # å¹¶å‘æ•°
    
    print("=" * 60)
    print("ğŸ¯ arXivåˆ†ç±»æ‰¹é‡çˆ¬å–å·¥å…·")
    print("=" * 60)
    print(f"ğŸ“‚ ç›®æ ‡åˆ†ç±»: {', '.join(categories)}")
    print(f"ğŸ“Š æ¯åˆ†ç±»æœ€å¤§æ•°é‡: {max_results_per_category}")
    print(f"âš¡ å¹¶å‘æ•°: {concurrent}")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    successful_categories = 0
    failed_categories = 0
    
    # é€ä¸ªåˆ†ç±»çˆ¬å–
    for i, category in enumerate(categories, 1):
        print(f"\nğŸ“‘ [{i}/{len(categories)}] å¼€å§‹çˆ¬å–åˆ†ç±»: {category}")
        print("-" * 40)
        
        # æ„å»ºçˆ¬å–å‘½ä»¤
        query = f"cat:{category}"
        output_dir = f"crawled_data/categories/{category.replace('.', '_')}"
        
        command = f'python main.py crawl --query "{query}" --max-results {max_results_per_category} --concurrent {concurrent} --output {output_dir} --download-pdf'
        
        # æ‰§è¡Œçˆ¬å–
        success = run_command(command)
        
        if success:
            successful_categories += 1
            print(f"ğŸ‰ {category} çˆ¬å–å®Œæˆ")
        else:
            failed_categories += 1
            print(f"ğŸ’¥ {category} çˆ¬å–å¤±è´¥")
        
        # åˆ†ç±»é—´å»¶è¿Ÿ
        if i < len(categories):
            print("â³ ç­‰å¾…2ç§’åç»§ç»­ä¸‹ä¸€ä¸ªåˆ†ç±»...")
            time.sleep(2)
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸŠ æ‰¹é‡çˆ¬å–å®Œæˆï¼")
    print("=" * 60)
    print(f"âœ… æˆåŠŸåˆ†ç±»: {successful_categories}")
    print(f"âŒ å¤±è´¥åˆ†ç±»: {failed_categories}")
    print(f"ğŸ“ æ•°æ®ç›®å½•: crawled_data/categories/")
    print(f"â° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ˜¾ç¤ºåç»­æ“ä½œå»ºè®®
    if successful_categories > 0:
        print("\nğŸ’¡ åç»­æ“ä½œå»ºè®®:")
        print("1. ä¸Šä¼ åˆ°å­˜å‚¨ç³»ç»Ÿ:")
        print("   python main.py upload --source crawled_data/categories")
        print("2. æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€:")
        print("   python main.py status --detail")


if __name__ == "__main__":
    # ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•
    script_dir = Path(__file__).parent
    print(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {script_dir}")
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not (script_dir / "main.py").exists():
        print("âŒ æœªæ‰¾åˆ° main.py æ–‡ä»¶ï¼Œè¯·ç¡®ä¿åœ¨ academic_papers/arxiv ç›®å½•ä¸‹è¿è¡Œ")
        sys.exit(1)
    
    if not (script_dir / "config.json").exists():
        print("âŒ æœªæ‰¾åˆ° config.json æ–‡ä»¶ï¼Œè¯·ç¡®ä¿é…ç½®æ–‡ä»¶å­˜åœ¨")
        sys.exit(1)
    
    # å¼€å§‹æ‰¹é‡çˆ¬å–
    batch_crawl_categories()
