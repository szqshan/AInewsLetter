#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXivè®ºæ–‡çˆ¬è™«ä¸»ç¨‹åº
å‚ç…§nlpSp1deræ¶æ„è®¾è®¡çš„ç»Ÿä¸€å…¥å£ç¨‹åº
"""

import argparse
import json
import logging
import os
import sys
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arxiv_system.crawler.arxiv_crawler import ArxivCrawler
from arxiv_system.oss.wrapper import OSSUploader
from arxiv_system.utils.file_utils import setup_logging, load_config


def setup_argument_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="arXivè®ºæ–‡çˆ¬è™«ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python main.py crawl --query "artificial intelligence" --max-results 50
  python main.py upload --source crawled_data
  python main.py status
        """
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # çˆ¬å–å‘½ä»¤
    crawl_parser = subparsers.add_parser("crawl", help="çˆ¬å–arXivè®ºæ–‡")
    crawl_parser.add_argument("--query", default="cat:cs.AI OR cat:cs.LG OR cat:cs.CL", 
                             help="æœç´¢æŸ¥è¯¢æ¡ä»¶")
    crawl_parser.add_argument("--max-results", type=int, default=100, 
                             help="æœ€å¤§ç»“æœæ•°é‡")
    crawl_parser.add_argument("--output", default="crawled_data", 
                             help="è¾“å‡ºç›®å½•")
    crawl_parser.add_argument("--concurrent", type=int, default=3, 
                             help="å¹¶å‘æ•°é‡")
    crawl_parser.add_argument("--download-pdf", action="store_true", 
                             help="ä¸‹è½½PDFæ–‡ä»¶")
    
    # ä¸Šä¼ å‘½ä»¤
    upload_parser = subparsers.add_parser("upload", help="ä¸Šä¼ æ•°æ®åˆ°OSS")
    upload_parser.add_argument("--source", default="crawled_data", 
                              help="æºæ•°æ®ç›®å½•")
    upload_parser.add_argument("--bucket", type=str, help="æŒ‡å®šbucketåç§°")
    upload_parser.add_argument("--resume", action="store_true", help="æ–­ç‚¹ç»­ä¼ ")
    upload_parser.add_argument("--concurrent", type=int, default=5, 
                              help="å¹¶å‘ä¸Šä¼ æ•°é‡")
    
    # çŠ¶æ€å‘½ä»¤
    status_parser = subparsers.add_parser("status", help="æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
    
    return parser


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    logger = logging.getLogger(__name__)
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = setup_argument_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        # åŠ è½½é…ç½®
        config = load_config("config.json")
        
        if args.command == "crawl":
            logger.info(f"å¼€å§‹çˆ¬å–arXivè®ºæ–‡: {args.query}")
            crawler = ArxivCrawler(config)
            crawler.crawl(
                query=args.query,
                max_results=args.max_results,
                output_dir=args.output,
                concurrent=args.concurrent,
                download_pdf=args.download_pdf
            )
            
        elif args.command == "upload":
            logger.info(f"å¼€å§‹ä¸Šä¼ æ•°æ®åˆ°OSS: {args.source}")
            import asyncio
            
            async def upload_task():
                # å¦‚æœæŒ‡å®šäº†bucketï¼Œæ›´æ–°é…ç½®
                if args.bucket:
                    config['oss']['bucket_name'] = args.bucket
                    
                async with OSSUploader(config['oss']) as uploader:
                    result = await uploader.upload_all(
                        base_dir=Path(args.source),
                        resume=args.resume
                    )
                    
                    if result['success']:
                        logger.info(f"âœ… ä¸Šä¼ æˆåŠŸï¼å…±ä¸Šä¼  {result['uploaded_files']} ä¸ªæ–‡ä»¶")
                        if result.get('sample_urls'):
                            logger.info("ğŸ“‹ ç¤ºä¾‹URL:")
                            for url in result['sample_urls'][:3]:
                                logger.info(f"  ğŸ”— {url}")
                    else:
                        logger.error(f"âŒ ä¸Šä¼ å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                        sys.exit(1)
            
            asyncio.run(upload_task())
            
        elif args.command == "status":
            logger.info("æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
            # TODO: å®ç°çŠ¶æ€æŸ¥çœ‹åŠŸèƒ½
            print("ç³»ç»ŸçŠ¶æ€åŠŸèƒ½å¾…å®ç°")
            
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()