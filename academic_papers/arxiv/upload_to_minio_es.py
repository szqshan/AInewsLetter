#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXivè®ºæ–‡æ•°æ®ä¸Šä¼ åˆ°MinIOå’ŒElasticsearch
é€šè¿‡MinIOè¿æ¥å™¨APIä¸Šä¼ è®ºæ–‡æ•°æ®
"""

import json
import requests
import asyncio
import aiohttp
import sys
from pathlib import Path
from typing import List, Dict, Any
import logging
from datetime import datetime
import argparse

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from arxiv_system.utils.file_utils import load_config, setup_logging


class ArxivUploader:
    """arXivè®ºæ–‡ä¸Šä¼ å™¨"""
    
    def __init__(self, config_path: str = "config_enhanced.json"):
        """åˆå§‹åŒ–ä¸Šä¼ å™¨"""
        self.config = load_config(config_path)
        self.oss_config = self.config.get("oss", {})
        
        self.api_endpoint = self.oss_config.get("api_endpoint", "http://localhost:9011")
        self.upload_endpoint = self.oss_config.get("upload_endpoint", "/api/documents/upload-articles")
        self.es_endpoint = self.oss_config.get("elasticsearch_endpoint", "/api/elasticsearch/articles")
        self.bucket_name = self.oss_config.get("bucket_name", "arxiv-papers")
        
        self.logger = logging.getLogger(__name__)
        
        print(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
        print(f"   APIåœ°å€: {self.api_endpoint}")
        print(f"   ä¸Šä¼ ç«¯ç‚¹: {self.upload_endpoint}")
        print(f"   ESç«¯ç‚¹: {self.es_endpoint}")
        print(f"   å­˜å‚¨æ¡¶: {self.bucket_name}")
    
    def check_service_health(self) -> bool:
        """æ£€æŸ¥MinIOè¿æ¥å™¨æœåŠ¡å¥åº·çŠ¶æ€"""
        try:
            response = requests.get(f"{self.api_endpoint}/health", timeout=10)
            if response.status_code == 200:
                print("âœ… MinIOè¿æ¥å™¨æœåŠ¡æ­£å¸¸è¿è¡Œ")
                return True
            else:
                print(f"âŒ MinIOè¿æ¥å™¨æœåŠ¡å¼‚å¸¸ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ æ— æ³•è¿æ¥åˆ°MinIOè¿æ¥å™¨æœåŠ¡: {e}")
            return False
    
    def load_papers_from_file(self, file_path: str) -> List[Dict[str, Any]]:
        """ä»JSONæ–‡ä»¶åŠ è½½è®ºæ–‡æ•°æ®"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                papers = json.load(f)
            
            if isinstance(papers, list):
                print(f"ğŸ“„ ä» {file_path} åŠ è½½äº† {len(papers)} ç¯‡è®ºæ–‡")
                return papers
            else:
                print(f"âŒ æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼ŒæœŸæœ›åˆ—è¡¨æ ¼å¼")
                return []
                
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []
    
    def convert_to_upload_format(self, papers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†arXivè®ºæ–‡æ•°æ®è½¬æ¢ä¸ºä¸Šä¼ æ ¼å¼"""
        converted_papers = []
        
        for paper in papers:
            # è½¬æ¢ä¸ºMinIOè¿æ¥å™¨æœŸæœ›çš„æ ¼å¼
            converted_paper = {
                "title": paper.get("title", "").strip(),
                "content": paper.get("abstract", "").strip(),
                "authors": paper.get("authors", []),
                "published_date": paper.get("published", ""),
                "url": paper.get("url", ""),
                "arxiv_id": paper.get("arxiv_id", ""),
                "categories": paper.get("categories", []),
                "pdf_url": paper.get("pdf_url", ""),
                "source": "arxiv",
                "source_id": paper.get("arxiv_id", ""),
                "tags": paper.get("categories", []),
                "metadata": {
                    "updated": paper.get("updated", ""),
                    "primary_category": paper.get("primary_category", ""),
                    "updated_date": paper.get("updated", "")
                }
            }
            
            # ç¡®ä¿å¿…è¦å­—æ®µä¸ä¸ºç©º
            if converted_paper["title"] and converted_paper["content"]:
                converted_papers.append(converted_paper)
            else:
                self.logger.warning(f"è·³è¿‡æ— æ•ˆè®ºæ–‡: {paper.get('arxiv_id', 'unknown')}")
        
        print(f"ğŸ“‹ è½¬æ¢å®Œæˆ: {len(converted_papers)} ç¯‡æœ‰æ•ˆè®ºæ–‡")
        return converted_papers
    
    def upload_papers_to_minio_es(self, papers: List[Dict[str, Any]], 
                                 batch_size: int = 50) -> Dict[str, Any]:
        """æ‰¹é‡ä¸Šä¼ è®ºæ–‡åˆ°MinIOå’ŒElasticsearch"""
        print(f"â˜ï¸ å¼€å§‹æ‰¹é‡ä¸Šä¼  {len(papers)} ç¯‡è®ºæ–‡...")
        
        total_uploaded = 0
        total_failed = 0
        failed_papers = []
        
        # åˆ†æ‰¹ä¸Šä¼ 
        for i in range(0, len(papers), batch_size):
            batch = papers[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(papers) + batch_size - 1) // batch_size
            
            print(f"\nğŸ“¦ ä¸Šä¼ æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ç¯‡è®ºæ–‡)")
            
            try:
                # å‡†å¤‡ä¸Šä¼ æ•°æ®
                upload_data = {
                    "articles": batch,
                    "bucket_name": self.bucket_name,
                    "batch_size": len(batch)
                }
                
                # å‘é€POSTè¯·æ±‚
                response = requests.post(
                    f"{self.api_endpoint}{self.upload_endpoint}",
                    json=upload_data,
                    timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    uploaded_count = result.get("uploaded_count", 0)
                    total_uploaded += uploaded_count
                    print(f"   âœ… æ‰¹æ¬¡ä¸Šä¼ æˆåŠŸ: {uploaded_count} ç¯‡")
                    
                    # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                    if "results" in result:
                        for res in result["results"]:
                            if res.get("success"):
                                print(f"      ğŸ“„ {res.get('title', 'Unknown')[:50]}...")
                            else:
                                print(f"      âŒ å¤±è´¥: {res.get('error', 'Unknown error')}")
                else:
                    print(f"   âŒ æ‰¹æ¬¡ä¸Šä¼ å¤±è´¥: HTTP {response.status_code}")
                    print(f"      é”™è¯¯ä¿¡æ¯: {response.text}")
                    total_failed += len(batch)
                    failed_papers.extend([p.get("arxiv_id", "unknown") for p in batch])
                    
            except Exception as e:
                print(f"   âŒ æ‰¹æ¬¡ä¸Šä¼ å¼‚å¸¸: {e}")
                total_failed += len(batch)
                failed_papers.extend([p.get("arxiv_id", "unknown") for p in batch])
        
        # æ±‡æ€»ç»“æœ
        result = {
            "total_papers": len(papers),
            "uploaded_count": total_uploaded,
            "failed_count": total_failed,
            "failed_papers": failed_papers,
            "success_rate": (total_uploaded / len(papers)) * 100 if len(papers) > 0 else 0
        }
        
        print(f"\nğŸ¯ ä¸Šä¼ å®Œæˆç»Ÿè®¡:")
        print(f"   æ€»è®ºæ–‡æ•°: {result['total_papers']}")
        print(f"   æˆåŠŸä¸Šä¼ : {result['uploaded_count']}")
        print(f"   å¤±è´¥æ•°é‡: {result['failed_count']}")
        print(f"   æˆåŠŸç‡: {result['success_rate']:.1f}%")
        
        return result
    
    def query_elasticsearch(self, query: str = "*", size: int = 10) -> Dict[str, Any]:
        """æŸ¥è¯¢Elasticsearchä¸­çš„è®ºæ–‡æ•°æ®"""
        try:
            search_data = {
                "query": query,
                "size": size,
                "from": 0
            }
            
            response = requests.post(
                f"{self.api_endpoint}{self.es_endpoint}/search",
                json=search_data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                total_hits = result.get("total", 0)
                hits = result.get("hits", [])
                
                print(f"ğŸ” ElasticsearchæŸ¥è¯¢ç»“æœ:")
                print(f"   æ€»æ–‡æ¡£æ•°: {total_hits}")
                print(f"   è¿”å›ç»“æœ: {len(hits)}")
                
                if hits:
                    print(f"   æœ€æ–°è®ºæ–‡ç¤ºä¾‹:")
                    for i, hit in enumerate(hits[:3], 1):
                        source = hit.get("_source", {})
                        title = source.get("title", "Unknown")[:60]
                        arxiv_id = source.get("arxiv_id", "Unknown")
                        print(f"      {i}. {title}... (ID: {arxiv_id})")
                
                return result
            else:
                print(f"âŒ ElasticsearchæŸ¥è¯¢å¤±è´¥: HTTP {response.status_code}")
                return {}
                
        except Exception as e:
            print(f"âŒ ElasticsearchæŸ¥è¯¢å¼‚å¸¸: {e}")
            return {}
    
    def get_elasticsearch_stats(self) -> Dict[str, Any]:
        """è·å–Elasticsearchç»Ÿè®¡ä¿¡æ¯"""
        try:
            response = requests.get(
                f"{self.api_endpoint}{self.es_endpoint}/stats",
                timeout=30
            )
            
            if response.status_code == 200:
                stats = response.json()
                print(f"ğŸ“Š Elasticsearchç»Ÿè®¡ä¿¡æ¯:")
                print(f"   ç´¢å¼•åç§°: {stats.get('index_name', 'Unknown')}")
                print(f"   æ–‡æ¡£æ€»æ•°: {stats.get('doc_count', 0)}")
                print(f"   å­˜å‚¨å¤§å°: {stats.get('store_size', 'Unknown')}")
                
                return stats
            else:
                print(f"âŒ è·å–ESç»Ÿè®¡å¤±è´¥: HTTP {response.status_code}")
                return {}
                
        except Exception as e:
            print(f"âŒ è·å–ESç»Ÿè®¡å¼‚å¸¸: {e}")
            return {}


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä¸Šä¼ arXivè®ºæ–‡åˆ°MinIOå’ŒElasticsearch")
    parser.add_argument("--file", required=True, help="è®ºæ–‡JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--batch-size", type=int, default=50, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--query", action="store_true", help="ä¸Šä¼ åæŸ¥è¯¢éªŒè¯")
    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºESç»Ÿè®¡ä¿¡æ¯")
    parser.add_argument("--config", default="config_enhanced.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    print("ğŸš€ arXivè®ºæ–‡ä¸Šä¼ å·¥å…·")
    print("=" * 50)
    
    # åˆå§‹åŒ–ä¸Šä¼ å™¨
    uploader = ArxivUploader(args.config)
    
    # æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
    if not uploader.check_service_health():
        print("âŒ MinIOè¿æ¥å™¨æœåŠ¡ä¸å¯ç”¨ï¼Œè¯·å…ˆå¯åŠ¨æœåŠ¡")
        sys.exit(1)
    
    # åŠ è½½è®ºæ–‡æ•°æ®
    papers = uploader.load_papers_from_file(args.file)
    if not papers:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®ºæ–‡æ•°æ®")
        sys.exit(1)
    
    # è½¬æ¢æ•°æ®æ ¼å¼
    converted_papers = uploader.convert_to_upload_format(papers)
    if not converted_papers:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®ºæ–‡å¯ä»¥ä¸Šä¼ ")
        sys.exit(1)
    
    # ä¸Šä¼ è®ºæ–‡
    result = uploader.upload_papers_to_minio_es(converted_papers, args.batch_size)
    
    # æŸ¥è¯¢éªŒè¯
    if args.query:
        print("\n" + "=" * 50)
        uploader.query_elasticsearch("*", 5)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if args.stats:
        print("\n" + "=" * 50)
        uploader.get_elasticsearch_stats()
    
    # ä¿å­˜ä¸Šä¼ ç»“æœ
    result_file = f"upload_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ ä¸Šä¼ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    if result["success_rate"] >= 90:
        print("ğŸ‰ ä¸Šä¼ æˆåŠŸï¼")
        sys.exit(0)
    else:
        print("âš ï¸ éƒ¨åˆ†ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)


if __name__ == "__main__":
    main()
