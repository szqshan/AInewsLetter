# Towards Generalizable Safety in Crowd Navigation via Conformal   Uncertainty Handling

## 基本信息

- **arXiv ID**: 2508.05634v1
- **发布日期**: 2025-08-07
- **主要分类**: cs.RO
- **所有分类**: cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY
- **作者**: Jianpeng Yao, Xiaopan Zhang, Yu Xia, Zejin Wang, Amit K. Roy-Chowdhury, Jiachen Li

## 链接

- **arXiv页面**: https://arxiv.org/abs/2508.05634v1
- **PDF下载**: https://arxiv.org/pdf/2508.05634v1.pdf

## 摘要

Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agent's behavior through constrained reinforcement learning. The system helps regulate the agent's actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/.
