#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…ä¿å®ˆçš„Semantic Scholar APIæµ‹è¯•
ä½¿ç”¨æä½çš„è¯·æ±‚é¢‘ç‡é¿å…429é”™è¯¯
"""

import asyncio
import aiohttp
import json
import time
from datetime import datetime


class ConservativeCrawler:
    """è¶…ä¿å®ˆçš„çˆ¬è™«ï¼Œé¿å…é¢‘ç‡é™åˆ¶"""
    
    def __init__(self):
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.request_interval = 15.0  # æ¯15ç§’1æ¬¡è¯·æ±‚
        self.last_request_time = 0
        self.request_count = 0
        
        self.headers = {
            'User-Agent': 'Academic-Research-Tool/1.0',
            'Accept': 'application/json'
        }
        
    async def wait_for_rate_limit(self):
        """ç­‰å¾…é¢‘ç‡é™åˆ¶"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.request_interval:
            sleep_time = self.request_interval - time_since_last
            print(f"â° é¢‘ç‡æ§åˆ¶ç­‰å¾… {sleep_time:.1f} ç§’...")
            await asyncio.sleep(sleep_time)
        
        self.last_request_time = time.time()
        self.request_count += 1
        
    async def safe_request(self, session, url, params):
        """å®‰å…¨çš„APIè¯·æ±‚"""
        await self.wait_for_rate_limit()
        
        print(f"ğŸŒ è¯·æ±‚ #{self.request_count}: {url}")
        print(f"ğŸ“ å‚æ•°: {params}")
        
        try:
            async with session.get(url, params=params) as response:
                print(f"ğŸ“¡ å“åº”çŠ¶æ€: {response.status}")
                
                if response.status == 200:
                    data = await response.json()
                    print(f"âœ… è¯·æ±‚æˆåŠŸ")
                    return data
                    
                elif response.status == 429:
                    retry_after = response.headers.get('Retry-After', '300')
                    print(f"ğŸš« é¢‘ç‡é™åˆ¶! å»ºè®®ç­‰å¾… {retry_after} ç§’")
                    
                    # ç­‰å¾…æ›´é•¿æ—¶é—´
                    wait_time = max(int(retry_after), 300)  # è‡³å°‘ç­‰å¾…5åˆ†é’Ÿ
                    print(f"ğŸ˜´ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                    
                    # é‡è¯•ä¸€æ¬¡
                    await self.wait_for_rate_limit()
                    async with session.get(url, params=params) as retry_response:
                        if retry_response.status == 200:
                            return await retry_response.json()
                        else:
                            print(f"âŒ é‡è¯•å¤±è´¥: {retry_response.status}")
                            return None
                else:
                    text = await response.text()
                    print(f"âŒ è¯·æ±‚å¤±è´¥ {response.status}: {text[:200]}")
                    return None
                    
        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            return None
    
    async def test_single_search(self):
        """æµ‹è¯•å•ä¸ªæœç´¢è¯·æ±‚"""
        print("\nğŸ” æµ‹è¯•å•ä¸ªæœç´¢è¯·æ±‚")
        print("-" * 30)
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            search_url = f"{self.base_url}/paper/search"
            
            params = {
                'query': 'machine learning',
                'limit': 5,
                'fields': 'paperId,title,year,citationCount'
            }
            
            result = await self.safe_request(session, search_url, params)
            
            if result and isinstance(result, dict) and 'data' in result:
                papers = result['data']
                print(f"ğŸ“š æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡:")
                
                for i, paper in enumerate(papers, 1):
                    title = paper.get('title', 'æ— æ ‡é¢˜')[:60]
                    year = paper.get('year', 'æœªçŸ¥')
                    citations = paper.get('citationCount', 0)
                    print(f"  {i}. {title}... ({year}, {citations}å¼•ç”¨)")
                
                return papers
            else:
                print("âŒ æœç´¢å¤±è´¥æˆ–æ— æ•°æ®")
                return None
    
    async def test_multiple_searches(self):
        """æµ‹è¯•å¤šä¸ªæœç´¢ï¼ˆéå¸¸ä¿å®ˆï¼‰"""
        print("\nğŸ”„ æµ‹è¯•å¤šä¸ªæœç´¢è¯·æ±‚")
        print("-" * 30)
        
        queries = ["artificial intelligence", "deep learning"]  # åªæµ‹è¯•2ä¸ª
        all_papers = []
        
        async with aiohttp.ClientSession(headers=self.headers) as session:
            for i, query in enumerate(queries, 1):
                print(f"\nğŸ“ æœç´¢ {i}/{len(queries)}: {query}")
                
                search_url = f"{self.base_url}/paper/search"
                params = {
                    'query': query,
                    'limit': 3,  # å‡å°‘æ•°é‡
                    'fields': 'paperId,title,year,citationCount'
                }
                
                result = await self.safe_request(session, search_url, params)
                
                if result and isinstance(result, dict) and 'data' in result:
                    papers = result['data']
                    print(f"  âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
                    all_papers.extend(papers)
                else:
                    print(f"  âŒ æœç´¢å¤±è´¥: {query}")
        
        print(f"\nğŸ“Š æ€»è®¡: {len(all_papers)} ç¯‡è®ºæ–‡")
        return all_papers


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸŒ Semantic Scholar è¶…ä¿å®ˆæµ‹è¯•")
    print("=" * 50)
    print("âš ï¸  ä½¿ç”¨æä½é¢‘ç‡è¯·æ±‚ï¼Œé¿å…429é”™è¯¯")
    print("â° æ¯ä¸ªè¯·æ±‚é—´éš”15ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print("=" * 50)
    
    crawler = ConservativeCrawler()
    
    try:
        start_time = time.time()
        
        # æµ‹è¯•1: å•ä¸ªæœç´¢
        papers1 = await crawler.test_single_search()
        
        if papers1:
            print(f"\nâœ… å•ä¸ªæœç´¢æˆåŠŸ: {len(papers1)} ç¯‡è®ºæ–‡")
            
            # ç»§ç»­æµ‹è¯•å¤šä¸ªæœç´¢
            papers2 = await crawler.test_multiple_searches()
            
            if papers2:
                print(f"âœ… å¤šä¸ªæœç´¢æˆåŠŸ: {len(papers2)} ç¯‡è®ºæ–‡")
            
            # ä¿å­˜ç»“æœ
            all_papers = (papers1 or []) + (papers2 or [])
            if all_papers:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"conservative_test_{timestamp}.json"
                
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(all_papers, f, ensure_ascii=False, indent=2)
                
                print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {filename}")
        
        elapsed = time.time() - start_time
        print(f"\nâ±ï¸  æ€»è€—æ—¶: {elapsed:.1f} ç§’")
        print(f"ğŸ“Š æ€»è¯·æ±‚: {crawler.request_count} æ¬¡")
        print(f"ğŸ“ˆ å¹³å‡é—´éš”: {elapsed/max(crawler.request_count, 1):.1f} ç§’/è¯·æ±‚")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        print(traceback.format_exc())


if __name__ == "__main__":
    asyncio.run(main())
