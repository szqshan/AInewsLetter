#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Semantic Scholarçˆ¬è™«å¯åŠ¨è„šæœ¬
ç®€åŒ–ç‰ˆå¯åŠ¨æ¥å£
"""

import argparse
import asyncio
import json
import sys
from pathlib import Path

# å¯¼å…¥çˆ¬è™«
from spider import SemanticScholarPublicCrawler


def load_config(config_path: str = "config.json") -> dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
        return {}
    except json.JSONDecodeError:
        print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {config_path}")
        return {}


async def run_crawler(max_papers: int = 200, output_dir: str = None):
    """è¿è¡Œçˆ¬è™«"""
    print("ğŸ”¬ Semantic Scholar æ— APIå¯†é’¥çˆ¬è™«")
    print("=" * 50)
    print(f"ğŸ“Š è®¡åˆ’çˆ¬å–: {max_papers} ç¯‡è®ºæ–‡")
    print(f"â±ï¸  é¢„è®¡è€—æ—¶: {max_papers * 2 / 60:.1f} åˆ†é’Ÿ")
    print("ğŸŒ æ…¢é€Ÿçˆ¬å–ä¸­ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    print("-" * 50)
    
    if output_dir is None:
        output_dir = "crawled_data"
    
    async with SemanticScholarPublicCrawler(output_dir) as crawler:
        try:
            # å¼€å§‹çˆ¬å–
            papers = await crawler.crawl_ai_papers(max_papers=max_papers)
            
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆ!")
            print(f"   ğŸ“š æˆåŠŸçˆ¬å–: {len(papers)} ç¯‡è®ºæ–‡")
            print(f"   ğŸŒ APIè¯·æ±‚: {crawler.request_count} æ¬¡")
            
            # ä¿å­˜ç»“æœ
            await crawler.save_results()
            print("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°æœ¬åœ°")
            
            return papers
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
            if crawler.papers_data:
                await crawler.save_results()
                print("ğŸ’¾ å·²ä¿å­˜éƒ¨åˆ†ç»“æœ")
            return crawler.papers_data
        except Exception as e:
            print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            if crawler.papers_data:
                await crawler.save_results()
                print("ğŸ’¾ å·²ä¿å­˜éƒ¨åˆ†ç»“æœ")
            return crawler.papers_data


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Semantic Scholaræ— APIå¯†é’¥çˆ¬è™«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  python run_crawler.py                    # é»˜è®¤çˆ¬å–200ç¯‡è®ºæ–‡
  python run_crawler.py --papers 100      # çˆ¬å–100ç¯‡è®ºæ–‡
  python run_crawler.py --papers 500 --output results  # è‡ªå®šä¹‰è¾“å‡ºç›®å½•

æ³¨æ„äº‹é¡¹:
  - æ— éœ€APIå¯†é’¥ï¼Œä½†è¯·æ±‚é¢‘ç‡å—é™
  - æ¯2ç§’å‘é€1æ¬¡è¯·æ±‚ï¼Œè¯·è€å¿ƒç­‰å¾…
  - å¯éšæ—¶Ctrl+Cä¸­æ–­ï¼Œä¼šä¿å­˜å·²çˆ¬å–çš„æ•°æ®
        """
    )
    
    parser.add_argument(
        '--papers', '-p',
        type=int,
        default=200,
        help='è¦çˆ¬å–çš„è®ºæ–‡æ•°é‡ (é»˜è®¤: 200)'
    )
    
    parser.add_argument(
        '--output', '-o',
        type=str,
        default='crawled_data',
        help='è¾“å‡ºç›®å½• (é»˜è®¤: crawled_data)'
    )
    
    parser.add_argument(
        '--config', '-c',
        type=str,
        default='config.json',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.json)'
    )
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.papers <= 0:
        print("âŒ è®ºæ–‡æ•°é‡å¿…é¡»å¤§äº0")
        sys.exit(1)
    
    if args.papers > 1000:
        print("âš ï¸ è®ºæ–‡æ•°é‡è¿‡å¤§ï¼Œå»ºè®®ä¸è¶…è¿‡1000ç¯‡")
        choice = input("æ˜¯å¦ç»§ç»­? (y/N): ")
        if choice.lower() != 'y':
            sys.exit(0)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è¿è¡Œçˆ¬è™«
    try:
        papers = asyncio.run(run_crawler(args.papers, args.output))
        
        print(f"\nâœ… ä»»åŠ¡å®Œæˆ!")
        print(f"   ğŸ“ è¾“å‡ºç›®å½•: {output_path.absolute()}")
        print(f"   ğŸ“Š è®ºæ–‡æ•°é‡: {len(papers) if papers else 0}")
        
        if papers:
            # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            years = [p.get('year') for p in papers if p.get('year')]
            if years:
                print(f"   ğŸ“… å¹´ä»½èŒƒå›´: {min(years)} - {max(years)}")
            
            citations = [p.get('citationCount', 0) for p in papers]
            if citations:
                avg_citations = sum(citations) / len(citations)
                print(f"   ğŸ“ˆ å¹³å‡å¼•ç”¨: {avg_citations:.1f}")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‡ºç°é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
