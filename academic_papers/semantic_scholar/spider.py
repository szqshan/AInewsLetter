#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Semantic Scholar æ— APIå¯†é’¥çˆ¬è™«
ä½¿ç”¨å…¬å…±APIï¼Œä¸¥æ ¼æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…è¢«é™åˆ¶
"""

import asyncio
import json
import logging
import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
import aiohttp
import aiofiles

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SemanticScholarPublicCrawler:
    """Semantic Scholarå…¬å…±APIçˆ¬è™«
    
    ä¸¥æ ¼éµå®ˆé¢‘ç‡é™åˆ¶ï¼š
    - æ— APIå¯†é’¥ï¼š100 requests/5min (çº¦1.33 RPS)
    - å®é™…ä½¿ç”¨ï¼š0.5 RPS (æ¯2ç§’1æ¬¡è¯·æ±‚)
    """
    
    def __init__(self, output_dir: str = "crawled_data"):
        self.base_url = "https://api.semanticscholar.org/graph/v1"
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¶…ä¸¥æ ¼çš„é¢‘ç‡æ§åˆ¶
        self.request_interval = 10.0  # æ¯10ç§’1æ¬¡è¯·æ±‚
        self.last_request_time = 0
        
        # è¯·æ±‚ç»Ÿè®¡
        self.request_count = 0
        self.session_start_time = time.time()
        
        # ä¼šè¯é…ç½®
        self.session = None
        self.headers = {
            'User-Agent': 'AI-Newsletter-Scholar-Crawler/1.0',
            'Accept': 'application/json'
        }
        
        # æ•°æ®å­˜å‚¨
        self.papers_data = []
        self.failed_requests = []
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        connector = aiohttp.TCPConnector(limit=1)  # é™åˆ¶å¹¶å‘è¿æ¥æ•°
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=self.headers
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def _rate_limit(self):
        """ä¸¥æ ¼çš„é¢‘ç‡é™åˆ¶"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time
        
        if time_since_last < self.request_interval:
            sleep_time = self.request_interval - time_since_last
            logger.info(f"ğŸ•’ é¢‘ç‡é™åˆ¶ç­‰å¾… {sleep_time:.1f} ç§’")
            await asyncio.sleep(sleep_time)
        
        self.last_request_time = time.time()
        self.request_count += 1
        
        # æ¯50æ¬¡è¯·æ±‚è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        if self.request_count % 50 == 0:
            elapsed = time.time() - self.session_start_time
            avg_rps = self.request_count / elapsed
            logger.info(f"ğŸ“Š è¯·æ±‚ç»Ÿè®¡: {self.request_count} æ¬¡è¯·æ±‚, å¹³å‡ {avg_rps:.2f} RPS")
    
    async def _make_request(self, endpoint: str, params: Dict = None) -> Optional[Dict]:
        """å‘èµ·APIè¯·æ±‚ï¼ŒåŒ…å«é‡è¯•æœºåˆ¶"""
        await self._rate_limit()
        
        url = f"{self.base_url}/{endpoint}"
        logger.info(f"ğŸŒ è¯·æ±‚URL: {url}")
        logger.info(f"ğŸ“ è¯·æ±‚å‚æ•°: {params}")
        
        for attempt in range(3):  # æœ€å¤šé‡è¯•3æ¬¡
            try:
                async with self.session.get(url, params=params) as response:
                    logger.info(f"ğŸ“¡ å“åº”çŠ¶æ€: {response.status}")
                    
                    if response.status == 200:
                        try:
                            data = await response.json()
                            logger.info(f"âœ… è¯·æ±‚æˆåŠŸ: {endpoint}, æ•°æ®ç±»å‹: {type(data)}")
                            if isinstance(data, dict) and 'data' in data:
                                logger.info(f"ğŸ“Š è¿”å›æ•°æ®æ•°é‡: {len(data.get('data', []))}")
                            return data
                        except Exception as json_error:
                            logger.error(f"âŒ JSONè§£æé”™è¯¯: {json_error}")
                            response_text = await response.text()
                            logger.error(f"âŒ å“åº”å†…å®¹: {response_text[:500]}")
                            return None
                        
                    elif response.status == 429:
                        # è¢«é™åˆ¶é¢‘ç‡
                        retry_after = int(response.headers.get('Retry-After', 60))
                        logger.warning(f"âš ï¸ é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {retry_after} ç§’åé‡è¯•")
                        await asyncio.sleep(retry_after)
                        continue
                        
                    elif response.status == 404:
                        logger.warning(f"âš ï¸ èµ„æºæœªæ‰¾åˆ°: {endpoint}")
                        return None
                        
                    else:
                        response_text = await response.text()
                        logger.error(f"âŒ HTTPé”™è¯¯ {response.status}: {endpoint}")
                        logger.error(f"âŒ å“åº”å†…å®¹: {response_text[:500]}")
                        if attempt < 2:  # ä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                            await asyncio.sleep(5)  # ç­‰å¾…5ç§’åé‡è¯•
                        
            except aiohttp.ClientError as e:
                logger.error(f"âŒ ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/3): {e}")
                if attempt < 2:
                    await asyncio.sleep(5)
            except Exception as e:
                logger.error(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
                import traceback
                logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                break
        
        # è®°å½•å¤±è´¥çš„è¯·æ±‚
        self.failed_requests.append({
            'endpoint': endpoint,
            'params': params,
            'timestamp': datetime.now().isoformat()
        })
        return None
    
    async def search_papers(self, query: str, limit: int = 100, offset: int = 0) -> Optional[Dict]:
        """æœç´¢è®ºæ–‡"""
        # åŸºç¡€å­—æ®µï¼Œé¿å…è¯·æ±‚è¿‡å¤šæ•°æ®
        fields = [
            'paperId', 'title', 'abstract', 'authors', 'year',
            'citationCount', 'referenceCount', 'influentialCitationCount',
            'fieldsOfStudy', 'venue', 'url'
        ]
        
        params = {
            'query': query,
            'limit': min(limit, 100),  # APIé™åˆ¶
            'offset': offset,
            'fields': ','.join(fields)
        }
        
        logger.info(f"ğŸ” æœç´¢è®ºæ–‡: {query} (limit={limit}, offset={offset})")
        return await self._make_request('paper/search', params)
    
    async def get_paper_details(self, paper_id: str) -> Optional[Dict]:
        """è·å–è®ºæ–‡è¯¦æƒ…"""
        fields = [
            'paperId', 'title', 'abstract', 'authors', 'year',
            'citationCount', 'referenceCount', 'influentialCitationCount',
            'fieldsOfStudy', 'venue', 'journal', 'url', 'externalIds',
            'publicationDate', 'publicationTypes'
        ]
        
        params = {'fields': ','.join(fields)}
        
        logger.info(f"ğŸ“„ è·å–è®ºæ–‡è¯¦æƒ…: {paper_id}")
        return await self._make_request(f'paper/{paper_id}', params)
    
    def filter_relevant_papers(self, papers: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤ç›¸å…³è®ºæ–‡"""
        filtered = []
        
        for paper in papers:
            # åŸºæœ¬è¿‡æ»¤æ¡ä»¶
            year = paper.get('year')
            citation_count = paper.get('citationCount', 0)
            title = paper.get('title', '')
            abstract = paper.get('abstract', '')
            
            # å¹´ä»½è¿‡æ»¤ï¼šåªè¦2020å¹´ä»¥åçš„è®ºæ–‡ï¼ˆæ›´å®¹æ˜“æ‰¾åˆ°æ•°æ®ï¼‰
            if not year or year < 2020:
                continue
            
            # å¼•ç”¨æ•°è¿‡æ»¤ï¼šè‡³å°‘æœ‰1æ¬¡å¼•ç”¨ï¼ˆé™ä½é—¨æ§›ï¼‰
            if citation_count < 1:
                continue
            
            # æ ‡é¢˜å’Œæ‘˜è¦ä¸èƒ½ä¸ºç©º
            if not title or not abstract:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸AIç›¸å…³
            if self._is_ai_related(paper):
                filtered.append(paper)
        
        return filtered
    
    def _is_ai_related(self, paper: Dict) -> bool:
        """åˆ¤æ–­è®ºæ–‡æ˜¯å¦ä¸AIç›¸å…³"""
        # æ£€æŸ¥ç ”ç©¶é¢†åŸŸ
        fields_of_study = paper.get('fieldsOfStudy', [])
        ai_fields = {'Computer Science', 'Mathematics', 'Engineering'}
        
        if not any(field in ai_fields for field in fields_of_study):
            return False
        
        # æ£€æŸ¥æ ‡é¢˜å’Œæ‘˜è¦ä¸­çš„AIå…³é”®è¯
        title = paper.get('title', '').lower()
        abstract = paper.get('abstract', '').lower()
        text = f"{title} {abstract}"
        
        ai_keywords = [
            'artificial intelligence', 'machine learning', 'deep learning',
            'neural network', 'transformer', 'attention', 'bert', 'gpt',
            'computer vision', 'natural language', 'nlp', 'reinforcement learning',
            'convolutional', 'recurrent', 'lstm', 'gan', 'generative',
            'classification', 'detection', 'recognition', 'prediction'
        ]
        
        return any(keyword in text for keyword in ai_keywords)
    
    async def crawl_ai_papers(self, max_papers: int = 500):
        """çˆ¬å–AIç›¸å…³è®ºæ–‡"""
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–AIè®ºæ–‡ï¼Œç›®æ ‡æ•°é‡: {max_papers}")
        
        # AIç›¸å…³æœç´¢æŸ¥è¯¢
        search_queries = [
            "artificial intelligence",
            "machine learning", 
            "deep learning",
            "neural networks",
            "computer vision",
            "natural language processing",
            "transformer models",
            "reinforcement learning"
        ]
        
        all_papers = []
        papers_per_query = max_papers // len(search_queries)
        
        for query in search_queries:
            logger.info(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
            
            offset = 0
            papers_collected = 0
            
            while papers_collected < papers_per_query:
                # è®¡ç®—æœ¬æ¬¡è¯·æ±‚æ•°é‡
                limit = min(100, papers_per_query - papers_collected)
                
                # æœç´¢è®ºæ–‡
                result = await self.search_papers(query, limit, offset)
                
                if not result or not isinstance(result, dict) or 'data' not in result:
                    logger.warning(f"âŒ æœç´¢å¤±è´¥æˆ–æ— æ•°æ®: {query}, result: {result}")
                    break
                
                papers = result.get('data', [])
                if not papers or not isinstance(papers, list):
                    logger.info(f"ğŸ“­ æ— æ›´å¤šç»“æœ: {query}")
                    break
                
                # è¿‡æ»¤ç›¸å…³è®ºæ–‡
                relevant_papers = self.filter_relevant_papers(papers)
                
                for paper in relevant_papers:
                    paper['search_query'] = query
                    paper['crawl_timestamp'] = datetime.now().isoformat()
                    all_papers.append(paper)
                
                papers_collected += len(papers)
                offset += len(papers)
                
                logger.info(f"ğŸ“Š å·²æ”¶é›† {len(relevant_papers)}/{len(papers)} ç›¸å…³è®ºæ–‡")
                
                # å¦‚æœè¿”å›çš„è®ºæ–‡æ•°å°‘äºè¯·æ±‚æ•°ï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šç»“æœ
                if len(papers) < limit:
                    break
        
        # å»é‡
        unique_papers = self._deduplicate_papers(all_papers)
        logger.info(f"ğŸ“‹ å»é‡åè®ºæ–‡æ•°é‡: {len(unique_papers)}")
        
        self.papers_data = unique_papers
        return unique_papers
    
    def _deduplicate_papers(self, papers: List[Dict]) -> List[Dict]:
        """å»é‡è®ºæ–‡"""
        seen_ids = set()
        seen_titles = set()
        unique_papers = []
        
        for paper in papers:
            paper_id = paper.get('paperId')
            title = paper.get('title', '').lower().strip()
            
            # æŒ‰è®ºæ–‡IDå»é‡
            if paper_id and paper_id in seen_ids:
                continue
            
            # æŒ‰æ ‡é¢˜å»é‡
            if title and title in seen_titles:
                continue
            
            if paper_id:
                seen_ids.add(paper_id)
            if title:
                seen_titles.add(title)
            
            unique_papers.append(paper)
        
        return unique_papers
    
    async def save_results(self):
        """ä¿å­˜çˆ¬å–ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è®ºæ–‡æ•°æ®
        papers_file = self.output_dir / f"papers_{timestamp}.json"
        async with aiofiles.open(papers_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(self.papers_data, ensure_ascii=False, indent=2))
        
        # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
        summary_file = self.output_dir / f"summary_{timestamp}.md"
        summary = self._generate_summary()
        async with aiofiles.open(summary_file, 'w', encoding='utf-8') as f:
            await f.write(summary)
        
        # ä¿å­˜å¤±è´¥çš„è¯·æ±‚
        if self.failed_requests:
            failed_file = self.output_dir / f"failed_requests_{timestamp}.json"
            async with aiofiles.open(failed_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(self.failed_requests, ensure_ascii=False, indent=2))
        
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜:")
        logger.info(f"   è®ºæ–‡æ•°æ®: {papers_file}")
        logger.info(f"   æ‘˜è¦æŠ¥å‘Š: {summary_file}")
        if self.failed_requests:
            logger.info(f"   å¤±è´¥è¯·æ±‚: {failed_file}")
    
    def _generate_summary(self) -> str:
        """ç”Ÿæˆçˆ¬å–æ‘˜è¦æŠ¥å‘Š"""
        total_papers = len(self.papers_data)
        
        # æŒ‰å¹´ä»½ç»Ÿè®¡
        papers_by_year = {}
        for paper in self.papers_data:
            year = paper.get('year', 'Unknown')
            papers_by_year[year] = papers_by_year.get(year, 0) + 1
        
        # æŒ‰é¢†åŸŸç»Ÿè®¡
        papers_by_field = {}
        for paper in self.papers_data:
            fields = paper.get('fieldsOfStudy', [])
            for field in fields:
                papers_by_field[field] = papers_by_field.get(field, 0) + 1
        
        # å¼•ç”¨ç»Ÿè®¡
        citations = [paper.get('citationCount', 0) for paper in self.papers_data]
        avg_citations = sum(citations) / len(citations) if citations else 0
        max_citations = max(citations) if citations else 0
        
        summary = f"""# Semantic Scholar çˆ¬å–æ‘˜è¦æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **çˆ¬å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **è®ºæ–‡æ€»æ•°**: {total_papers}
- **è¯·æ±‚æ€»æ•°**: {self.request_count}
- **å¤±è´¥è¯·æ±‚**: {len(self.failed_requests)}

## æ•°æ®ç»Ÿè®¡

### æŒ‰å¹´ä»½åˆ†å¸ƒ
"""
        
        for year in sorted(papers_by_year.keys(), reverse=True):
            count = papers_by_year[year]
            summary += f"- **{year}**: {count} ç¯‡\n"
        
        summary += "\n### æŒ‰ç ”ç©¶é¢†åŸŸåˆ†å¸ƒ\n"
        sorted_fields = sorted(papers_by_field.items(), key=lambda x: x[1], reverse=True)
        for field, count in sorted_fields[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            summary += f"- **{field}**: {count} ç¯‡\n"
        
        summary += f"""
### å¼•ç”¨ç»Ÿè®¡
- **å¹³å‡å¼•ç”¨æ•°**: {avg_citations:.1f}
- **æœ€é«˜å¼•ç”¨æ•°**: {max_citations}
- **å¼•ç”¨æ•° > 100**: {len([c for c in citations if c > 100])} ç¯‡
- **å¼•ç”¨æ•° > 50**: {len([c for c in citations if c > 50])} ç¯‡

## é«˜è´¨é‡è®ºæ–‡ (å¼•ç”¨æ•° > 100)
"""
        
        high_quality_papers = [p for p in self.papers_data if p.get('citationCount', 0) > 100]
        high_quality_papers.sort(key=lambda x: x.get('citationCount', 0), reverse=True)
        
        for paper in high_quality_papers[:10]:  # æ˜¾ç¤ºå‰10ç¯‡
            title = paper.get('title', 'æ— æ ‡é¢˜')
            citations = paper.get('citationCount', 0)
            year = paper.get('year', 'æœªçŸ¥')
            summary += f"- **{title}** ({year}) - {citations} å¼•ç”¨\n"
        
        if self.failed_requests:
            summary += f"\n## å¤±è´¥è¯·æ±‚\n"
            summary += f"- **å¤±è´¥è¯·æ±‚æ•°**: {len(self.failed_requests)}\n"
            summary += f"- **å»ºè®®**: æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé™åˆ¶\n"
        
        return summary


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ Semantic Scholar æ— APIå¯†é’¥çˆ¬è™«")
    print("=" * 50)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "academic_papers/semantic_scholar/crawled_data"
    
    async with SemanticScholarPublicCrawler(output_dir) as crawler:
        try:
            # çˆ¬å–AIè®ºæ–‡
            papers = await crawler.crawl_ai_papers(max_papers=200)  # ä»å°æ•°é‡å¼€å§‹
            
            print(f"\nğŸ“Š çˆ¬å–å®Œæˆ!")
            print(f"   è®ºæ–‡æ•°é‡: {len(papers)}")
            print(f"   è¯·æ±‚æ€»æ•°: {crawler.request_count}")
            
            # ä¿å­˜ç»“æœ
            await crawler.save_results()
            
            print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
            
        except KeyboardInterrupt:
            print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
            if crawler.papers_data:
                await crawler.save_results()
                print("ğŸ’¾ å·²ä¿å­˜éƒ¨åˆ†ç»“æœ")
        except Exception as e:
            print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            if crawler.papers_data:
                await crawler.save_results()
                print("ğŸ’¾ å·²ä¿å­˜éƒ¨åˆ†ç»“æœ")


if __name__ == "__main__":
    asyncio.run(main())
