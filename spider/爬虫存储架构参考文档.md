# ğŸš€ arXivçˆ¬è™«å­˜å‚¨æ¶æ„å‚è€ƒæ–‡æ¡£

> åŸºäºnlpSp1deræ¶æ„çš„ç°ä»£åŒ–çˆ¬è™«å­˜å‚¨è§£å†³æ–¹æ¡ˆ

## ğŸ“‹ ç›®å½•

- [å­˜å‚¨æ¶æ„æ¦‚è¿°](#å­˜å‚¨æ¶æ„æ¦‚è¿°)
- [æ–‡ä»¶å­˜å‚¨ç»“æ„](#æ–‡ä»¶å­˜å‚¨ç»“æ„)
- [OSSäº‘å­˜å‚¨é…ç½®](#ossäº‘å­˜å‚¨é…ç½®)
- [æ•°æ®æ¨¡å‹è®¾è®¡](#æ•°æ®æ¨¡å‹è®¾è®¡)
- [ä¸Šä¼ æµç¨‹å®ç°](#ä¸Šä¼ æµç¨‹å®ç°)
- [ç¤ºä¾‹ä»£ç ](#ç¤ºä¾‹ä»£ç )
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## ğŸ—ï¸ å­˜å‚¨æ¶æ„æ¦‚è¿°

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

1. **åˆ†å±‚å­˜å‚¨**: æœ¬åœ°å­˜å‚¨ + äº‘å­˜å‚¨åŒé‡ä¿éšœ
2. **æ¨¡å—åŒ–è®¾è®¡**: çˆ¬è™«ã€å­˜å‚¨ã€ä¸Šä¼ æ¨¡å—å®Œå…¨è§£è€¦
3. **å¼‚æ­¥å¤„ç†**: å…¨å¼‚æ­¥æ¶æ„ï¼Œæ”¯æŒé«˜å¹¶å‘
4. **æ ‡å‡†åŒ–æ ¼å¼**: ç»Ÿä¸€çš„æ•°æ®æ¨¡å‹ï¼Œä¾¿äºè·¨é¡¹ç›®å¤ç”¨

### æŠ€æœ¯æ ˆ

- **çˆ¬è™«å¼•æ“**: asyncio + aiohttp
- **æœ¬åœ°å­˜å‚¨**: JSON + Markdown + PDF
- **äº‘å­˜å‚¨**: MinIO / é˜¿é‡Œäº‘OSS
- **æ•°æ®å¤„ç†**: å¼‚æ­¥æ–‡ä»¶æ“ä½œ

## ğŸ“ æ–‡ä»¶å­˜å‚¨ç»“æ„

```
crawled_data/
â”œâ”€â”€ articles/                    # æ–‡ç« è¯¦ç»†æ•°æ®
â”‚   â””â”€â”€ {paper_id}/             # æŒ‰è®ºæ–‡IDç»„ç»‡
â”‚       â”œâ”€â”€ metadata.json       # è®ºæ–‡å…ƒæ•°æ®
â”‚       â”œâ”€â”€ content.md          # è®ºæ–‡å†…å®¹(Markdown)
â”‚       â”œâ”€â”€ paper.pdf           # åŸå§‹PDFæ–‡ä»¶
â”‚       â””â”€â”€ images/             # å›¾ç‰‡èµ„æº
â”‚           â”œâ”€â”€ figure_1.png
â”‚           â””â”€â”€ figure_2.png
â””â”€â”€ data/                       # èšåˆæ•°æ®
    â”œâ”€â”€ papers_metadata.json    # æ‰€æœ‰è®ºæ–‡å…ƒæ•°æ®æ±‡æ€»
    â”œâ”€â”€ processed_papers.json   # å¤„ç†çŠ¶æ€è®°å½•
    â””â”€â”€ crawl_stats.json        # çˆ¬å–ç»Ÿè®¡ä¿¡æ¯
```

### ç›®å½•ç»“æ„è¯´æ˜

- **articles/**: æŒ‰è®ºæ–‡IDç»„ç»‡çš„è¯¦ç»†æ•°æ®ï¼Œæ¯ç¯‡è®ºæ–‡ä¸€ä¸ªç‹¬ç«‹ç›®å½•
- **data/**: èšåˆæ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯ï¼Œä¾¿äºæ‰¹é‡å¤„ç†å’Œåˆ†æ
- **images/**: è®ºæ–‡ä¸­çš„å›¾ç‰‡èµ„æºï¼Œæ”¯æŒå¤šç§æ ¼å¼

## â˜ï¸ OSSäº‘å­˜å‚¨é…ç½®

### MinIOé…ç½®ç¤ºä¾‹

```json
{
  "oss": {
    "provider": "minio",
    "endpoint": "http://localhost:9000",
    "access_key": "minioadmin",
    "secret_key": "minioadmin",
    "bucket_name": "arxiv-papers",
    "public_url_base": "http://localhost:9000",
    "upload_config": {
      "max_concurrent_uploads": 5,
      "retry_attempts": 3,
      "chunk_size": 8388608
    }
  }
}
```

### é˜¿é‡Œäº‘OSSé…ç½®ç¤ºä¾‹

```json
{
  "oss": {
    "provider": "aliyun",
    "endpoint": "https://oss-cn-hangzhou.aliyuncs.com",
    "access_key": "your_access_key",
    "secret_key": "your_secret_key",
    "bucket_name": "your-bucket-name",
    "public_url_base": "https://your-bucket-name.oss-cn-hangzhou.aliyuncs.com",
    "upload_config": {
      "max_concurrent_uploads": 10,
      "retry_attempts": 3,
      "chunk_size": 16777216
    }
  }
}
```

## ğŸ“Š æ•°æ®æ¨¡å‹è®¾è®¡

### metadata.json æ ‡å‡†æ ¼å¼

```json
{
  "id": "2508.05635v1",
  "title": "è®ºæ–‡æ ‡é¢˜",
  "abstract": "è®ºæ–‡æ‘˜è¦",
  "authors": ["ä½œè€…1", "ä½œè€…2"],
  "published_date": "2025-08-09",
  "updated_date": "2025-08-09",
  "categories": ["cs.AI", "cs.LG"],
  "arxiv_url": "https://arxiv.org/abs/2508.05635",
  "pdf_url": "https://arxiv.org/pdf/2508.05635.pdf",
  "local_pdf_path": "articles/2508.05635v1/paper.pdf",
  "content_hash": "sha256_hash",
  "processed_date": "2025-08-09T16:20:00Z",
  "wordcount": 8500,
  "local_images": ["figure_1.png", "figure_2.png"],
  "oss_urls": {
    "metadata": "http://localhost:9000/bucket/articles/2508.05635v1/metadata.json",
    "content": "http://localhost:9000/bucket/articles/2508.05635v1/content.md",
    "pdf": "http://localhost:9000/bucket/articles/2508.05635v1/paper.pdf"
  },
  "elasticsearch_indexed": false
}
```

### content.md æ ‡å‡†æ ¼å¼

```markdown
# è®ºæ–‡æ ‡é¢˜

## åŸºæœ¬ä¿¡æ¯
- **arXiv ID**: 2508.05635v1
- **å‘å¸ƒæ—¥æœŸ**: 2025-08-09
- **ä½œè€…**: ä½œè€…1, ä½œè€…2
- **åˆ†ç±»**: cs.AI, cs.LG

## æ‘˜è¦
è®ºæ–‡æ‘˜è¦å†…å®¹...

## ä¸»è¦å†…å®¹
è®ºæ–‡çš„ä¸»è¦å†…å®¹å’Œå…³é”®ç‚¹...

## å›¾ç‰‡
![Figure 1](images/figure_1.png)
![Figure 2](images/figure_2.png)

## é“¾æ¥
- [arXivé“¾æ¥](https://arxiv.org/abs/2508.05635)
- [PDFä¸‹è½½](https://arxiv.org/pdf/2508.05635.pdf)
```

## ğŸ”„ ä¸Šä¼ æµç¨‹å®ç°

### 1. OSSä¸Šä¼ å™¨åŸºç±»

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Optional
import asyncio
import aiofiles
import json
from pathlib import Path

class OSSUploader(ABC):
    """OSSä¸Šä¼ å™¨æŠ½è±¡åŸºç±»"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.bucket_name = config.get('bucket_name')
        self.public_url_base = config.get('public_url_base')
        
    @abstractmethod
    async def upload_file(self, local_path: str, remote_path: str) -> str:
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
        pass
        
    @abstractmethod
    async def upload_json(self, data: Dict, remote_path: str) -> str:
        """ä¸Šä¼ JSONæ•°æ®"""
        pass
        
    async def upload_all(self, source_dir: str, **kwargs) -> Dict:
        """ä¸Šä¼ æ•´ä¸ªç›®å½•"""
        source_path = Path(source_dir)
        if not source_path.exists():
            raise FileNotFoundError(f"Source directory not found: {source_dir}")
            
        results = {
            'uploaded_files': [],
            'failed_files': [],
            'total_files': 0,
            'success_count': 0,
            'error_count': 0
        }
        
        # è·å–æ‰€æœ‰éœ€è¦ä¸Šä¼ çš„æ–‡ä»¶
        files_to_upload = []
        
        # ä¸Šä¼ articlesç›®å½•
        articles_dir = source_path / 'articles'
        if articles_dir.exists():
            for article_dir in articles_dir.iterdir():
                if article_dir.is_dir():
                    files_to_upload.extend(self._get_article_files(article_dir))
        
        # ä¸Šä¼ dataç›®å½•
        data_dir = source_path / 'data'
        if data_dir.exists():
            files_to_upload.extend(self._get_data_files(data_dir))
        
        results['total_files'] = len(files_to_upload)
        
        # å¹¶å‘ä¸Šä¼ 
        semaphore = asyncio.Semaphore(kwargs.get('concurrent', 5))
        tasks = [self._upload_with_semaphore(semaphore, file_info, results) 
                for file_info in files_to_upload]
        
        await asyncio.gather(*tasks, return_exceptions=True)
        
        return results
    
    def _get_article_files(self, article_dir: Path) -> List[Dict]:
        """è·å–æ–‡ç« ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶"""
        files = []
        article_id = article_dir.name
        
        for file_path in article_dir.rglob('*'):
            if file_path.is_file():
                relative_path = file_path.relative_to(article_dir.parent.parent)
                remote_path = f"articles/{article_id}/{file_path.name}"
                
                if file_path.parent.name == 'images':
                    remote_path = f"articles/{article_id}/images/{file_path.name}"
                
                files.append({
                    'local_path': str(file_path),
                    'remote_path': remote_path,
                    'type': 'article_file'
                })
        
        return files
    
    def _get_data_files(self, data_dir: Path) -> List[Dict]:
        """è·å–dataç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶"""
        files = []
        
        for file_path in data_dir.iterdir():
            if file_path.is_file() and file_path.suffix == '.json':
                files.append({
                    'local_path': str(file_path),
                    'remote_path': f"data/{file_path.name}",
                    'type': 'data_file'
                })
        
        return files
    
    async def _upload_with_semaphore(self, semaphore: asyncio.Semaphore, 
                                   file_info: Dict, results: Dict):
        """å¸¦ä¿¡å·é‡çš„ä¸Šä¼ """
        async with semaphore:
            try:
                if file_info['local_path'].endswith('.json'):
                    # JSONæ–‡ä»¶ç‰¹æ®Šå¤„ç†
                    async with aiofiles.open(file_info['local_path'], 'r', encoding='utf-8') as f:
                        data = json.loads(await f.read())
                    url = await self.upload_json(data, file_info['remote_path'])
                else:
                    # æ™®é€šæ–‡ä»¶ä¸Šä¼ 
                    url = await self.upload_file(file_info['local_path'], file_info['remote_path'])
                
                results['uploaded_files'].append({
                    'local_path': file_info['local_path'],
                    'remote_path': file_info['remote_path'],
                    'url': url
                })
                results['success_count'] += 1
                
            except Exception as e:
                results['failed_files'].append({
                    'local_path': file_info['local_path'],
                    'error': str(e)
                })
                results['error_count'] += 1
```

### 2. MinIOå®ç°

```python
from minio import Minio
from minio.error import S3Error
import io
import json
from typing import Dict

class MinIOUploader(OSSUploader):
    """MinIOä¸Šä¼ å™¨å®ç°"""
    
    def __init__(self, config: Dict):
        super().__init__(config)
        self.client = Minio(
            endpoint=config['endpoint'].replace('http://', '').replace('https://', ''),
            access_key=config['access_key'],
            secret_key=config['secret_key'],
            secure=config['endpoint'].startswith('https')
        )
        self._ensure_bucket_exists()
    
    def _ensure_bucket_exists(self):
        """ç¡®ä¿bucketå­˜åœ¨"""
        try:
            if not self.client.bucket_exists(self.bucket_name):
                self.client.make_bucket(self.bucket_name)
                # è®¾ç½®bucketä¸ºå…¬å¼€è¯»å–
                policy = {
                    "Version": "2012-10-17",
                    "Statement": [
                        {
                            "Effect": "Allow",
                            "Principal": {"AWS": "*"},
                            "Action": "s3:GetObject",
                            "Resource": f"arn:aws:s3:::{self.bucket_name}/*"
                        }
                    ]
                }
                self.client.set_bucket_policy(self.bucket_name, json.dumps(policy))
        except S3Error as e:
            print(f"Error creating bucket: {e}")
    
    async def upload_file(self, local_path: str, remote_path: str) -> str:
        """ä¸Šä¼ æ–‡ä»¶åˆ°MinIO"""
        try:
            # ç¡®å®šcontent type
            content_type = self._get_content_type(local_path)
            
            # ä¸Šä¼ æ–‡ä»¶
            self.client.fput_object(
                bucket_name=self.bucket_name,
                object_name=remote_path,
                file_path=local_path,
                content_type=content_type
            )
            
            # è¿”å›å…¬å¼€è®¿é—®URL
            return f"{self.public_url_base}/{remote_path}"
            
        except Exception as e:
            raise Exception(f"Failed to upload {local_path}: {str(e)}")
    
    async def upload_json(self, data: Dict, remote_path: str) -> str:
        """ä¸Šä¼ JSONæ•°æ®åˆ°MinIO"""
        try:
            json_str = json.dumps(data, ensure_ascii=False, indent=2)
            json_bytes = json_str.encode('utf-8')
            
            self.client.put_object(
                bucket_name=self.bucket_name,
                object_name=remote_path,
                data=io.BytesIO(json_bytes),
                length=len(json_bytes),
                content_type='application/json'
            )
            
            return f"{self.public_url_base}/{remote_path}"
            
        except Exception as e:
            raise Exception(f"Failed to upload JSON to {remote_path}: {str(e)}")
    
    def _get_content_type(self, file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šcontent type"""
        ext = file_path.lower().split('.')[-1]
        content_types = {
            'json': 'application/json',
            'md': 'text/markdown',
            'pdf': 'application/pdf',
            'png': 'image/png',
            'jpg': 'image/jpeg',
            'jpeg': 'image/jpeg',
            'gif': 'image/gif',
            'txt': 'text/plain'
        }
        return content_types.get(ext, 'application/octet-stream')
```

### 3. çˆ¬è™«é›†æˆç¤ºä¾‹

```python
import asyncio
import json
from pathlib import Path
from typing import Dict, List

class ArxivCrawler:
    """arXivçˆ¬è™«ç¤ºä¾‹"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.output_dir = Path(config.get('output_dir', 'crawled_data'))
        self.articles_dir = self.output_dir / 'articles'
        self.data_dir = self.output_dir / 'data'
        
        # åˆ›å»ºç›®å½•
        self.articles_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    async def crawl_papers(self, query: str, max_results: int = 10) -> List[Dict]:
        """çˆ¬å–è®ºæ–‡"""
        papers = []
        
        # è¿™é‡Œæ˜¯çˆ¬å–é€»è¾‘çš„ç¤ºä¾‹
        for i in range(max_results):
            paper = await self._fetch_paper(query, i)
            if paper:
                await self._save_paper(paper)
                papers.append(paper)
        
        # ä¿å­˜èšåˆæ•°æ®
        await self._save_aggregated_data(papers)
        
        return papers
    
    async def _fetch_paper(self, query: str, index: int) -> Dict:
        """è·å–å•ç¯‡è®ºæ–‡æ•°æ®"""
        # æ¨¡æ‹Ÿçˆ¬å–é€»è¾‘
        paper = {
            'id': f'2508.0563{index}v1',
            'title': f'Sample Paper {index}',
            'abstract': f'This is a sample abstract for paper {index}',
            'authors': [f'Author {index}A', f'Author {index}B'],
            'published_date': '2025-08-09',
            'updated_date': '2025-08-09',
            'categories': ['cs.AI', 'cs.LG'],
            'arxiv_url': f'https://arxiv.org/abs/2508.0563{index}',
            'pdf_url': f'https://arxiv.org/pdf/2508.0563{index}.pdf'
        }
        return paper
    
    async def _save_paper(self, paper: Dict):
        """ä¿å­˜å•ç¯‡è®ºæ–‡"""
        paper_id = paper['id']
        paper_dir = self.articles_dir / paper_id
        paper_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜metadata.json
        metadata = paper.copy()
        metadata.update({
            'local_pdf_path': f'articles/{paper_id}/paper.pdf',
            'content_hash': 'sample_hash',
            'processed_date': '2025-08-09T16:20:00Z',
            'wordcount': 8500,
            'local_images': [],
            'oss_urls': {},
            'elasticsearch_indexed': False
        })
        
        metadata_path = paper_dir / 'metadata.json'
        async with aiofiles.open(metadata_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(metadata, ensure_ascii=False, indent=2))
        
        # ä¿å­˜content.md
        content = self._generate_markdown_content(paper)
        content_path = paper_dir / 'content.md'
        async with aiofiles.open(content_path, 'w', encoding='utf-8') as f:
            await f.write(content)
    
    def _generate_markdown_content(self, paper: Dict) -> str:
        """ç”ŸæˆMarkdownå†…å®¹"""
        return f"""# {paper['title']}

## åŸºæœ¬ä¿¡æ¯
- **arXiv ID**: {paper['id']}
- **å‘å¸ƒæ—¥æœŸ**: {paper['published_date']}
- **ä½œè€…**: {', '.join(paper['authors'])}
- **åˆ†ç±»**: {', '.join(paper['categories'])}

## æ‘˜è¦
{paper['abstract']}

## é“¾æ¥
- [arXivé“¾æ¥]({paper['arxiv_url']})
- [PDFä¸‹è½½]({paper['pdf_url']})
"""
    
    async def _save_aggregated_data(self, papers: List[Dict]):
        """ä¿å­˜èšåˆæ•°æ®"""
        # ä¿å­˜papers_metadata.json
        metadata_path = self.data_dir / 'papers_metadata.json'
        async with aiofiles.open(metadata_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(papers, ensure_ascii=False, indent=2))
        
        # ä¿å­˜crawl_stats.json
        stats = {
            'total_papers': len(papers),
            'crawl_date': '2025-08-09T16:20:00Z',
            'success_count': len(papers),
            'error_count': 0
        }
        stats_path = self.data_dir / 'crawl_stats.json'
        async with aiofiles.open(stats_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(stats, ensure_ascii=False, indent=2))
```

### 4. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```python
import asyncio
import json
from pathlib import Path

async def main():
    """å®Œæ•´çš„çˆ¬è™«+ä¸Šä¼ æµç¨‹ç¤ºä¾‹"""
    
    # 1. é…ç½®
    config = {
        'output_dir': 'crawled_data',
        'oss': {
            'provider': 'minio',
            'endpoint': 'http://localhost:9000',
            'access_key': 'minioadmin',
            'secret_key': 'minioadmin',
            'bucket_name': 'arxiv-papers',
            'public_url_base': 'http://localhost:9000/arxiv-papers'
        }
    }
    
    # 2. çˆ¬å–æ•°æ®
    crawler = ArxivCrawler(config)
    papers = await crawler.crawl_papers('machine learning', max_results=5)
    print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(papers)} ç¯‡è®ºæ–‡")
    
    # 3. ä¸Šä¼ åˆ°OSS
    uploader = MinIOUploader(config['oss'])
    results = await uploader.upload_all('crawled_data', concurrent=5)
    
    print(f"ğŸ“¤ ä¸Šä¼ å®Œæˆ:")
    print(f"  âœ… æˆåŠŸ: {results['success_count']} ä¸ªæ–‡ä»¶")
    print(f"  âŒ å¤±è´¥: {results['error_count']} ä¸ªæ–‡ä»¶")
    
    # 4. æ˜¾ç¤ºç¤ºä¾‹URL
    if results['uploaded_files']:
        print("\nğŸ”— ç¤ºä¾‹è®¿é—®URL:")
        for file_info in results['uploaded_files'][:3]:
            print(f"  {file_info['url']}")

if __name__ == '__main__':
    asyncio.run(main())
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†

```python
# é‡è¯•æœºåˆ¶
async def upload_with_retry(self, upload_func, *args, max_retries=3):
    for attempt in range(max_retries):
        try:
            return await upload_func(*args)
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

# æ–­ç‚¹ç»­ä¼ 
async def resume_upload(self, progress_file='upload_progress.json'):
    if Path(progress_file).exists():
        async with aiofiles.open(progress_file, 'r') as f:
            progress = json.loads(await f.read())
        return progress.get('uploaded_files', [])
    return []
```

### 2. æ€§èƒ½ä¼˜åŒ–

```python
# å¹¶å‘æ§åˆ¶
semaphore = asyncio.Semaphore(5)  # é™åˆ¶å¹¶å‘æ•°

# åˆ†å—ä¸Šä¼ å¤§æ–‡ä»¶
async def upload_large_file(self, file_path, chunk_size=8*1024*1024):
    # å®ç°åˆ†å—ä¸Šä¼ é€»è¾‘
    pass

# å‹ç¼©ä¸Šä¼ 
import gzip
async def upload_compressed(self, data, remote_path):
    compressed_data = gzip.compress(data.encode('utf-8'))
    # ä¸Šä¼ å‹ç¼©æ•°æ®
```

### 3. ç›‘æ§å’Œæ—¥å¿—

```python
import logging
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ä¸Šä¼ è¿›åº¦è·Ÿè¸ª
class UploadProgressTracker:
    def __init__(self):
        self.start_time = datetime.now()
        self.uploaded_count = 0
        self.total_count = 0
    
    def update(self, uploaded_count, total_count):
        self.uploaded_count = uploaded_count
        self.total_count = total_count
        progress = (uploaded_count / total_count) * 100
        elapsed = (datetime.now() - self.start_time).total_seconds()
        logger.info(f"ä¸Šä¼ è¿›åº¦: {progress:.1f}% ({uploaded_count}/{total_count}), è€—æ—¶: {elapsed:.1f}s")
```

## ğŸ”§ é…ç½®æ¨¡æ¿

### config.json å®Œæ•´æ¨¡æ¿

```json
{
  "crawler": {
    "output_dir": "crawled_data",
    "max_concurrent_requests": 10,
    "request_delay": 1,
    "user_agent": "Academic Paper Crawler 1.0",
    "timeout": 30
  },
  "storage": {
    "local_storage_enabled": true,
    "compress_content": false,
    "backup_enabled": true,
    "cleanup_old_files": true,
    "retention_days": 30
  },
  "oss": {
    "provider": "minio",
    "endpoint": "http://localhost:9000",
    "access_key": "minioadmin",
    "secret_key": "minioadmin",
    "bucket_name": "academic-papers",
    "public_url_base": "http://localhost:9000/academic-papers",
    "upload_config": {
      "max_concurrent_uploads": 5,
      "retry_attempts": 3,
      "chunk_size": 8388608,
      "enable_compression": true,
      "progress_tracking": true
    }
  },
  "logging": {
    "level": "INFO",
    "file": "crawler.log",
    "max_size": "10MB",
    "backup_count": 5
  }
}
```

## ğŸ“š å…¶ä»–çˆ¬è™«é¡¹ç›®é›†æˆæŒ‡å—

### 1. å¿«é€Ÿé›†æˆæ­¥éª¤

1. **å¤åˆ¶æ ¸å¿ƒæ¨¡å—**:
   ```bash
   cp -r src/arxiv_system/oss your_project/src/
   cp -r src/arxiv_system/utils your_project/src/
   ```

2. **å®‰è£…ä¾èµ–**:
   ```bash
   pip install aiohttp aiofiles minio oss2
   ```

3. **ä¿®æ”¹é…ç½®**:
   - æ›´æ–° `config.json` ä¸­çš„OSSé…ç½®
   - è°ƒæ•´å­˜å‚¨ç›®å½•ç»“æ„

4. **å®ç°çˆ¬è™«æ¥å£**:
   ```python
   class YourCrawler:
       async def crawl(self):
           # å®ç°ä½ çš„çˆ¬å–é€»è¾‘
           pass
       
       async def save_data(self, data):
           # ä½¿ç”¨æ ‡å‡†æ ¼å¼ä¿å­˜æ•°æ®
           pass
   ```

### 2. æ•°æ®æ ¼å¼é€‚é…

æ ¹æ®ä½ çš„æ•°æ®æºè°ƒæ•´ `metadata.json` æ ¼å¼:

```python
# ç¤ºä¾‹ï¼šæ–°é—»çˆ¬è™«é€‚é…
def adapt_news_metadata(news_data):
    return {
        'id': news_data['url_hash'],
        'title': news_data['title'],
        'content': news_data['content'],
        'author': news_data['author'],
        'published_date': news_data['date'],
        'source': news_data['source'],
        'url': news_data['url'],
        'tags': news_data['tags'],
        'local_images': news_data['images'],
        'processed_date': datetime.now().isoformat(),
        'oss_urls': {},
        'elasticsearch_indexed': False
    }
```

### 3. æ‰©å±•å»ºè®®

- **æ•°æ®åº“é›†æˆ**: æ·»åŠ PostgreSQL/MongoDBæ”¯æŒ
- **æœç´¢å¼•æ“**: é›†æˆElasticsearchè¿›è¡Œå…¨æ–‡æœç´¢
- **APIæ¥å£**: æä¾›RESTful APIè®¿é—®æ•°æ®
- **ç›‘æ§é¢æ¿**: æ·»åŠ Webç•Œé¢ç›‘æ§çˆ¬å–çŠ¶æ€
- **å®šæ—¶ä»»åŠ¡**: ä½¿ç”¨APSchedulerå®ç°å®šæ—¶çˆ¬å–

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœåœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥å‚è€ƒ:

1. **æ—¥å¿—æ–‡ä»¶**: æŸ¥çœ‹è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
2. **é…ç½®æ£€æŸ¥**: ç¡®è®¤OSSé…ç½®æ­£ç¡®
3. **ç½‘ç»œè¿æ¥**: æµ‹è¯•OSSæœåŠ¡è¿é€šæ€§
4. **æƒé™è®¾ç½®**: ç¡®è®¤OSSè®¿é—®æƒé™

**ç®€å•æ¥è¯´**: è¿™å¥—æ¶æ„å°±æ˜¯æŠŠçˆ¬è™«ã€å­˜å‚¨ã€ä¸Šä¼ ä¸‰ä¸ªéƒ¨åˆ†å®Œå…¨åˆ†å¼€ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½å¯ä»¥ç‹¬ç«‹ä½¿ç”¨å’Œæ›¿æ¢ã€‚æ•°æ®ç”¨æ ‡å‡†æ ¼å¼å­˜å‚¨ï¼Œä¸Šä¼ åˆ°äº‘ç«¯åå¯ä»¥éšæ—¶è®¿é—®ã€‚å…¶ä»–é¡¹ç›®åªè¦æŒ‰ç…§è¿™ä¸ªæ ¼å¼æ¥ï¼Œå°±èƒ½ç›´æ¥å¤ç”¨æ‰€æœ‰çš„å­˜å‚¨å’Œä¸Šä¼ åŠŸèƒ½ï¼ğŸš€