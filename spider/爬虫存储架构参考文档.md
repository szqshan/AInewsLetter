# 🚀 arXiv爬虫存储架构参考文档

> 基于nlpSp1der架构的现代化爬虫存储解决方案

## 📋 目录

- [存储架构概述](#存储架构概述)
- [文件存储结构](#文件存储结构)
- [OSS云存储配置](#oss云存储配置)
- [数据模型设计](#数据模型设计)
- [上传流程实现](#上传流程实现)
- [示例代码](#示例代码)
- [最佳实践](#最佳实践)

## 🏗️ 存储架构概述

### 核心设计理念

1. **分层存储**: 本地存储 + 云存储双重保障
2. **模块化设计**: 爬虫、存储、上传模块完全解耦
3. **异步处理**: 全异步架构，支持高并发
4. **标准化格式**: 统一的数据模型，便于跨项目复用

### 技术栈

- **爬虫引擎**: asyncio + aiohttp
- **本地存储**: JSON + Markdown + PDF
- **云存储**: MinIO / 阿里云OSS
- **数据处理**: 异步文件操作

## 📁 文件存储结构

```
crawled_data/
├── articles/                    # 文章详细数据
│   └── {paper_id}/             # 按论文ID组织
│       ├── metadata.json       # 论文元数据
│       ├── content.md          # 论文内容(Markdown)
│       ├── paper.pdf           # 原始PDF文件
│       └── images/             # 图片资源
│           ├── figure_1.png
│           └── figure_2.png
└── data/                       # 聚合数据
    ├── papers_metadata.json    # 所有论文元数据汇总
    ├── processed_papers.json   # 处理状态记录
    └── crawl_stats.json        # 爬取统计信息
```

### 目录结构说明

- **articles/**: 按论文ID组织的详细数据，每篇论文一个独立目录
- **data/**: 聚合数据和统计信息，便于批量处理和分析
- **images/**: 论文中的图片资源，支持多种格式

## ☁️ OSS云存储配置

### MinIO配置示例

```json
{
  "oss": {
    "provider": "minio",
    "endpoint": "http://localhost:9000",
    "access_key": "minioadmin",
    "secret_key": "minioadmin",
    "bucket_name": "arxiv-papers",
    "public_url_base": "http://localhost:9000",
    "upload_config": {
      "max_concurrent_uploads": 5,
      "retry_attempts": 3,
      "chunk_size": 8388608
    }
  }
}
```

### 阿里云OSS配置示例

```json
{
  "oss": {
    "provider": "aliyun",
    "endpoint": "https://oss-cn-hangzhou.aliyuncs.com",
    "access_key": "your_access_key",
    "secret_key": "your_secret_key",
    "bucket_name": "your-bucket-name",
    "public_url_base": "https://your-bucket-name.oss-cn-hangzhou.aliyuncs.com",
    "upload_config": {
      "max_concurrent_uploads": 10,
      "retry_attempts": 3,
      "chunk_size": 16777216
    }
  }
}
```

## 📊 数据模型设计

### metadata.json 标准格式

```json
{
  "id": "2508.05635v1",
  "title": "论文标题",
  "abstract": "论文摘要",
  "authors": ["作者1", "作者2"],
  "published_date": "2025-08-09",
  "updated_date": "2025-08-09",
  "categories": ["cs.AI", "cs.LG"],
  "arxiv_url": "https://arxiv.org/abs/2508.05635",
  "pdf_url": "https://arxiv.org/pdf/2508.05635.pdf",
  "local_pdf_path": "articles/2508.05635v1/paper.pdf",
  "content_hash": "sha256_hash",
  "processed_date": "2025-08-09T16:20:00Z",
  "wordcount": 8500,
  "local_images": ["figure_1.png", "figure_2.png"],
  "oss_urls": {
    "metadata": "http://localhost:9000/bucket/articles/2508.05635v1/metadata.json",
    "content": "http://localhost:9000/bucket/articles/2508.05635v1/content.md",
    "pdf": "http://localhost:9000/bucket/articles/2508.05635v1/paper.pdf"
  },
  "elasticsearch_indexed": false
}
```

### content.md 标准格式

```markdown
# 论文标题

## 基本信息
- **arXiv ID**: 2508.05635v1
- **发布日期**: 2025-08-09
- **作者**: 作者1, 作者2
- **分类**: cs.AI, cs.LG

## 摘要
论文摘要内容...

## 主要内容
论文的主要内容和关键点...

## 图片
![Figure 1](images/figure_1.png)
![Figure 2](images/figure_2.png)

## 链接
- [arXiv链接](https://arxiv.org/abs/2508.05635)
- [PDF下载](https://arxiv.org/pdf/2508.05635.pdf)
```

## 🔄 上传流程实现

### 1. OSS上传器基类

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Optional
import asyncio
import aiofiles
import json
from pathlib import Path

class OSSUploader(ABC):
    """OSS上传器抽象基类"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.bucket_name = config.get('bucket_name')
        self.public_url_base = config.get('public_url_base')
        
    @abstractmethod
    async def upload_file(self, local_path: str, remote_path: str) -> str:
        """上传单个文件"""
        pass
        
    @abstractmethod
    async def upload_json(self, data: Dict, remote_path: str) -> str:
        """上传JSON数据"""
        pass
        
    async def upload_all(self, source_dir: str, **kwargs) -> Dict:
        """上传整个目录"""
        source_path = Path(source_dir)
        if not source_path.exists():
            raise FileNotFoundError(f"Source directory not found: {source_dir}")
            
        results = {
            'uploaded_files': [],
            'failed_files': [],
            'total_files': 0,
            'success_count': 0,
            'error_count': 0
        }
        
        # 获取所有需要上传的文件
        files_to_upload = []
        
        # 上传articles目录
        articles_dir = source_path / 'articles'
        if articles_dir.exists():
            for article_dir in articles_dir.iterdir():
                if article_dir.is_dir():
                    files_to_upload.extend(self._get_article_files(article_dir))
        
        # 上传data目录
        data_dir = source_path / 'data'
        if data_dir.exists():
            files_to_upload.extend(self._get_data_files(data_dir))
        
        results['total_files'] = len(files_to_upload)
        
        # 并发上传
        semaphore = asyncio.Semaphore(kwargs.get('concurrent', 5))
        tasks = [self._upload_with_semaphore(semaphore, file_info, results) 
                for file_info in files_to_upload]
        
        await asyncio.gather(*tasks, return_exceptions=True)
        
        return results
    
    def _get_article_files(self, article_dir: Path) -> List[Dict]:
        """获取文章目录下的所有文件"""
        files = []
        article_id = article_dir.name
        
        for file_path in article_dir.rglob('*'):
            if file_path.is_file():
                relative_path = file_path.relative_to(article_dir.parent.parent)
                remote_path = f"articles/{article_id}/{file_path.name}"
                
                if file_path.parent.name == 'images':
                    remote_path = f"articles/{article_id}/images/{file_path.name}"
                
                files.append({
                    'local_path': str(file_path),
                    'remote_path': remote_path,
                    'type': 'article_file'
                })
        
        return files
    
    def _get_data_files(self, data_dir: Path) -> List[Dict]:
        """获取data目录下的所有文件"""
        files = []
        
        for file_path in data_dir.iterdir():
            if file_path.is_file() and file_path.suffix == '.json':
                files.append({
                    'local_path': str(file_path),
                    'remote_path': f"data/{file_path.name}",
                    'type': 'data_file'
                })
        
        return files
    
    async def _upload_with_semaphore(self, semaphore: asyncio.Semaphore, 
                                   file_info: Dict, results: Dict):
        """带信号量的上传"""
        async with semaphore:
            try:
                if file_info['local_path'].endswith('.json'):
                    # JSON文件特殊处理
                    async with aiofiles.open(file_info['local_path'], 'r', encoding='utf-8') as f:
                        data = json.loads(await f.read())
                    url = await self.upload_json(data, file_info['remote_path'])
                else:
                    # 普通文件上传
                    url = await self.upload_file(file_info['local_path'], file_info['remote_path'])
                
                results['uploaded_files'].append({
                    'local_path': file_info['local_path'],
                    'remote_path': file_info['remote_path'],
                    'url': url
                })
                results['success_count'] += 1
                
            except Exception as e:
                results['failed_files'].append({
                    'local_path': file_info['local_path'],
                    'error': str(e)
                })
                results['error_count'] += 1
```

### 2. MinIO实现

```python
from minio import Minio
from minio.error import S3Error
import io
import json
from typing import Dict

class MinIOUploader(OSSUploader):
    """MinIO上传器实现"""
    
    def __init__(self, config: Dict):
        super().__init__(config)
        self.client = Minio(
            endpoint=config['endpoint'].replace('http://', '').replace('https://', ''),
            access_key=config['access_key'],
            secret_key=config['secret_key'],
            secure=config['endpoint'].startswith('https')
        )
        self._ensure_bucket_exists()
    
    def _ensure_bucket_exists(self):
        """确保bucket存在"""
        try:
            if not self.client.bucket_exists(self.bucket_name):
                self.client.make_bucket(self.bucket_name)
                # 设置bucket为公开读取
                policy = {
                    "Version": "2012-10-17",
                    "Statement": [
                        {
                            "Effect": "Allow",
                            "Principal": {"AWS": "*"},
                            "Action": "s3:GetObject",
                            "Resource": f"arn:aws:s3:::{self.bucket_name}/*"
                        }
                    ]
                }
                self.client.set_bucket_policy(self.bucket_name, json.dumps(policy))
        except S3Error as e:
            print(f"Error creating bucket: {e}")
    
    async def upload_file(self, local_path: str, remote_path: str) -> str:
        """上传文件到MinIO"""
        try:
            # 确定content type
            content_type = self._get_content_type(local_path)
            
            # 上传文件
            self.client.fput_object(
                bucket_name=self.bucket_name,
                object_name=remote_path,
                file_path=local_path,
                content_type=content_type
            )
            
            # 返回公开访问URL
            return f"{self.public_url_base}/{remote_path}"
            
        except Exception as e:
            raise Exception(f"Failed to upload {local_path}: {str(e)}")
    
    async def upload_json(self, data: Dict, remote_path: str) -> str:
        """上传JSON数据到MinIO"""
        try:
            json_str = json.dumps(data, ensure_ascii=False, indent=2)
            json_bytes = json_str.encode('utf-8')
            
            self.client.put_object(
                bucket_name=self.bucket_name,
                object_name=remote_path,
                data=io.BytesIO(json_bytes),
                length=len(json_bytes),
                content_type='application/json'
            )
            
            return f"{self.public_url_base}/{remote_path}"
            
        except Exception as e:
            raise Exception(f"Failed to upload JSON to {remote_path}: {str(e)}")
    
    def _get_content_type(self, file_path: str) -> str:
        """根据文件扩展名确定content type"""
        ext = file_path.lower().split('.')[-1]
        content_types = {
            'json': 'application/json',
            'md': 'text/markdown',
            'pdf': 'application/pdf',
            'png': 'image/png',
            'jpg': 'image/jpeg',
            'jpeg': 'image/jpeg',
            'gif': 'image/gif',
            'txt': 'text/plain'
        }
        return content_types.get(ext, 'application/octet-stream')
```

### 3. 爬虫集成示例

```python
import asyncio
import json
from pathlib import Path
from typing import Dict, List

class ArxivCrawler:
    """arXiv爬虫示例"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.output_dir = Path(config.get('output_dir', 'crawled_data'))
        self.articles_dir = self.output_dir / 'articles'
        self.data_dir = self.output_dir / 'data'
        
        # 创建目录
        self.articles_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    async def crawl_papers(self, query: str, max_results: int = 10) -> List[Dict]:
        """爬取论文"""
        papers = []
        
        # 这里是爬取逻辑的示例
        for i in range(max_results):
            paper = await self._fetch_paper(query, i)
            if paper:
                await self._save_paper(paper)
                papers.append(paper)
        
        # 保存聚合数据
        await self._save_aggregated_data(papers)
        
        return papers
    
    async def _fetch_paper(self, query: str, index: int) -> Dict:
        """获取单篇论文数据"""
        # 模拟爬取逻辑
        paper = {
            'id': f'2508.0563{index}v1',
            'title': f'Sample Paper {index}',
            'abstract': f'This is a sample abstract for paper {index}',
            'authors': [f'Author {index}A', f'Author {index}B'],
            'published_date': '2025-08-09',
            'updated_date': '2025-08-09',
            'categories': ['cs.AI', 'cs.LG'],
            'arxiv_url': f'https://arxiv.org/abs/2508.0563{index}',
            'pdf_url': f'https://arxiv.org/pdf/2508.0563{index}.pdf'
        }
        return paper
    
    async def _save_paper(self, paper: Dict):
        """保存单篇论文"""
        paper_id = paper['id']
        paper_dir = self.articles_dir / paper_id
        paper_dir.mkdir(exist_ok=True)
        
        # 保存metadata.json
        metadata = paper.copy()
        metadata.update({
            'local_pdf_path': f'articles/{paper_id}/paper.pdf',
            'content_hash': 'sample_hash',
            'processed_date': '2025-08-09T16:20:00Z',
            'wordcount': 8500,
            'local_images': [],
            'oss_urls': {},
            'elasticsearch_indexed': False
        })
        
        metadata_path = paper_dir / 'metadata.json'
        async with aiofiles.open(metadata_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(metadata, ensure_ascii=False, indent=2))
        
        # 保存content.md
        content = self._generate_markdown_content(paper)
        content_path = paper_dir / 'content.md'
        async with aiofiles.open(content_path, 'w', encoding='utf-8') as f:
            await f.write(content)
    
    def _generate_markdown_content(self, paper: Dict) -> str:
        """生成Markdown内容"""
        return f"""# {paper['title']}

## 基本信息
- **arXiv ID**: {paper['id']}
- **发布日期**: {paper['published_date']}
- **作者**: {', '.join(paper['authors'])}
- **分类**: {', '.join(paper['categories'])}

## 摘要
{paper['abstract']}

## 链接
- [arXiv链接]({paper['arxiv_url']})
- [PDF下载]({paper['pdf_url']})
"""
    
    async def _save_aggregated_data(self, papers: List[Dict]):
        """保存聚合数据"""
        # 保存papers_metadata.json
        metadata_path = self.data_dir / 'papers_metadata.json'
        async with aiofiles.open(metadata_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(papers, ensure_ascii=False, indent=2))
        
        # 保存crawl_stats.json
        stats = {
            'total_papers': len(papers),
            'crawl_date': '2025-08-09T16:20:00Z',
            'success_count': len(papers),
            'error_count': 0
        }
        stats_path = self.data_dir / 'crawl_stats.json'
        async with aiofiles.open(stats_path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(stats, ensure_ascii=False, indent=2))
```

### 4. 完整使用示例

```python
import asyncio
import json
from pathlib import Path

async def main():
    """完整的爬虫+上传流程示例"""
    
    # 1. 配置
    config = {
        'output_dir': 'crawled_data',
        'oss': {
            'provider': 'minio',
            'endpoint': 'http://localhost:9000',
            'access_key': 'minioadmin',
            'secret_key': 'minioadmin',
            'bucket_name': 'arxiv-papers',
            'public_url_base': 'http://localhost:9000/arxiv-papers'
        }
    }
    
    # 2. 爬取数据
    crawler = ArxivCrawler(config)
    papers = await crawler.crawl_papers('machine learning', max_results=5)
    print(f"✅ 爬取完成，共获取 {len(papers)} 篇论文")
    
    # 3. 上传到OSS
    uploader = MinIOUploader(config['oss'])
    results = await uploader.upload_all('crawled_data', concurrent=5)
    
    print(f"📤 上传完成:")
    print(f"  ✅ 成功: {results['success_count']} 个文件")
    print(f"  ❌ 失败: {results['error_count']} 个文件")
    
    # 4. 显示示例URL
    if results['uploaded_files']:
        print("\n🔗 示例访问URL:")
        for file_info in results['uploaded_files'][:3]:
            print(f"  {file_info['url']}")

if __name__ == '__main__':
    asyncio.run(main())
```

## 🎯 最佳实践

### 1. 错误处理

```python
# 重试机制
async def upload_with_retry(self, upload_func, *args, max_retries=3):
    for attempt in range(max_retries):
        try:
            return await upload_func(*args)
        except Exception as e:
            if attempt == max_retries - 1:
                raise e
            await asyncio.sleep(2 ** attempt)  # 指数退避

# 断点续传
async def resume_upload(self, progress_file='upload_progress.json'):
    if Path(progress_file).exists():
        async with aiofiles.open(progress_file, 'r') as f:
            progress = json.loads(await f.read())
        return progress.get('uploaded_files', [])
    return []
```

### 2. 性能优化

```python
# 并发控制
semaphore = asyncio.Semaphore(5)  # 限制并发数

# 分块上传大文件
async def upload_large_file(self, file_path, chunk_size=8*1024*1024):
    # 实现分块上传逻辑
    pass

# 压缩上传
import gzip
async def upload_compressed(self, data, remote_path):
    compressed_data = gzip.compress(data.encode('utf-8'))
    # 上传压缩数据
```

### 3. 监控和日志

```python
import logging
from datetime import datetime

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# 上传进度跟踪
class UploadProgressTracker:
    def __init__(self):
        self.start_time = datetime.now()
        self.uploaded_count = 0
        self.total_count = 0
    
    def update(self, uploaded_count, total_count):
        self.uploaded_count = uploaded_count
        self.total_count = total_count
        progress = (uploaded_count / total_count) * 100
        elapsed = (datetime.now() - self.start_time).total_seconds()
        logger.info(f"上传进度: {progress:.1f}% ({uploaded_count}/{total_count}), 耗时: {elapsed:.1f}s")
```

## 🔧 配置模板

### config.json 完整模板

```json
{
  "crawler": {
    "output_dir": "crawled_data",
    "max_concurrent_requests": 10,
    "request_delay": 1,
    "user_agent": "Academic Paper Crawler 1.0",
    "timeout": 30
  },
  "storage": {
    "local_storage_enabled": true,
    "compress_content": false,
    "backup_enabled": true,
    "cleanup_old_files": true,
    "retention_days": 30
  },
  "oss": {
    "provider": "minio",
    "endpoint": "http://localhost:9000",
    "access_key": "minioadmin",
    "secret_key": "minioadmin",
    "bucket_name": "academic-papers",
    "public_url_base": "http://localhost:9000/academic-papers",
    "upload_config": {
      "max_concurrent_uploads": 5,
      "retry_attempts": 3,
      "chunk_size": 8388608,
      "enable_compression": true,
      "progress_tracking": true
    }
  },
  "logging": {
    "level": "INFO",
    "file": "crawler.log",
    "max_size": "10MB",
    "backup_count": 5
  }
}
```

## 📚 其他爬虫项目集成指南

### 1. 快速集成步骤

1. **复制核心模块**:
   ```bash
   cp -r src/arxiv_system/oss your_project/src/
   cp -r src/arxiv_system/utils your_project/src/
   ```

2. **安装依赖**:
   ```bash
   pip install aiohttp aiofiles minio oss2
   ```

3. **修改配置**:
   - 更新 `config.json` 中的OSS配置
   - 调整存储目录结构

4. **实现爬虫接口**:
   ```python
   class YourCrawler:
       async def crawl(self):
           # 实现你的爬取逻辑
           pass
       
       async def save_data(self, data):
           # 使用标准格式保存数据
           pass
   ```

### 2. 数据格式适配

根据你的数据源调整 `metadata.json` 格式:

```python
# 示例：新闻爬虫适配
def adapt_news_metadata(news_data):
    return {
        'id': news_data['url_hash'],
        'title': news_data['title'],
        'content': news_data['content'],
        'author': news_data['author'],
        'published_date': news_data['date'],
        'source': news_data['source'],
        'url': news_data['url'],
        'tags': news_data['tags'],
        'local_images': news_data['images'],
        'processed_date': datetime.now().isoformat(),
        'oss_urls': {},
        'elasticsearch_indexed': False
    }
```

### 3. 扩展建议

- **数据库集成**: 添加PostgreSQL/MongoDB支持
- **搜索引擎**: 集成Elasticsearch进行全文搜索
- **API接口**: 提供RESTful API访问数据
- **监控面板**: 添加Web界面监控爬取状态
- **定时任务**: 使用APScheduler实现定时爬取

---

## 📞 技术支持

如果在使用过程中遇到问题，可以参考:

1. **日志文件**: 查看详细的错误信息
2. **配置检查**: 确认OSS配置正确
3. **网络连接**: 测试OSS服务连通性
4. **权限设置**: 确认OSS访问权限

**简单来说**: 这套架构就是把爬虫、存储、上传三个部分完全分开，每个部分都可以独立使用和替换。数据用标准格式存储，上传到云端后可以随时访问。其他项目只要按照这个格式来，就能直接复用所有的存储和上传功能！🚀