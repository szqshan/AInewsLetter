# Hugging Face Daily Papers çˆ¬è™«æŠ€æœ¯æ–‡æ¡£

## ç›®æ ‡ç½‘ç«™ä¿¡æ¯

- **ç½‘ç«™åç§°**: Hugging Face Daily Papers
- **ç½‘ç«™åœ°å€**: https://huggingface.co/papers
- **ç½‘ç«™ç±»å‹**: AIè®ºæ–‡èšåˆå¹³å°
- **æ•°æ®æ›´æ–°é¢‘ç‡**: æ¯æ—¥æ›´æ–°
- **è®¿é—®é™åˆ¶**: ç›¸å¯¹å®½æ¾ï¼Œä½†éœ€è¦åˆç†æ§åˆ¶é¢‘ç‡

## çˆ¬è™«æ–¹æ¡ˆæ¦‚è¿°

### æŠ€æœ¯æ¶æ„
- **çˆ¬è™«ç±»å‹**: API + ç½‘é¡µçˆ¬è™«æ··åˆ
- **ä¸»è¦æŠ€æœ¯**: Python + requests + BeautifulSoup4
- **æ•°æ®æ ¼å¼**: JSON â†’ Markdown
- **ç‰¹è‰²åŠŸèƒ½**: æ”¯æŒè®ºæ–‡è¶‹åŠ¿åˆ†æå’Œçƒ­åº¦è¯„ä¼°

### æ ¸å¿ƒåŠŸèƒ½
1. **æ¯æ—¥è®ºæ–‡**: è·å–æ¯æ—¥æ¨èçš„AIè®ºæ–‡
2. **çƒ­é—¨è®ºæ–‡**: åŸºäºç‚¹èµæ•°å’Œè¯„è®ºæ•°æ’åº
3. **åˆ†ç±»ç­›é€‰**: æŒ‰AIé¢†åŸŸåˆ†ç±»ç­›é€‰è®ºæ–‡
4. **ä½œè€…ä¿¡æ¯**: æå–è®ºæ–‡ä½œè€…å’Œæœºæ„
5. **è®ºæ–‡æ‘˜è¦**: è·å–è®ºæ–‡æ‘˜è¦å’Œå…³é”®ä¿¡æ¯

## çˆ¬å–æ–¹å¼è¯¦è§£

### 1. é¡µé¢URLç»“æ„

#### ä¸»è¦é¡µé¢
```
# æ¯æ—¥è®ºæ–‡é¦–é¡µ
https://huggingface.co/papers

# æŒ‰æ—¥æœŸæµè§ˆ
https://huggingface.co/papers?date=2024-01-15

# æŒ‰åˆ†ç±»æµè§ˆ
https://huggingface.co/papers?topic=computer-vision
https://huggingface.co/papers?topic=natural-language-processing
https://huggingface.co/papers?topic=machine-learning
```

#### URLå‚æ•°è¯´æ˜
- `date`: æŒ‡å®šæ—¥æœŸ (YYYY-MM-DDæ ¼å¼)
- `topic`: è®ºæ–‡åˆ†ç±»
- `sort`: æ’åºæ–¹å¼ (trending, recent, discussed)
- `p`: åˆ†é¡µå‚æ•°

### 2. é¡µé¢ç»“æ„åˆ†æ

#### è®ºæ–‡å¡ç‰‡ç»“æ„
```html
<article class="flex flex-col overflow-hidden rounded-xl border">
  <div class="flex flex-1 flex-col justify-between p-6">
    <div class="flex-1">
      <div class="flex items-center gap-x-2 text-xs">
        <time datetime="2024-01-15">Jan 15</time>
        <span class="text-gray-500">â€¢</span>
        <span class="text-gray-500">Computer Vision</span>
      </div>
      
      <div class="group mt-3">
        <h3 class="text-xl font-semibold text-gray-900">
          <a href="/papers/2401.xxxxx">
            è®ºæ–‡æ ‡é¢˜
          </a>
        </h3>
        <p class="mt-3 line-clamp-3 text-sm text-gray-500">
          è®ºæ–‡æ‘˜è¦å†…å®¹...
        </p>
      </div>
      
      <div class="mt-6 flex items-center">
        <div class="flex-shrink-0">
          <span class="text-sm font-medium text-gray-900">
            ä½œè€…å§“å
          </span>
        </div>
        <div class="ml-auto flex items-center space-x-4">
          <button class="flex items-center space-x-1">
            <span class="text-sm text-gray-500">ğŸ‘ 42</span>
          </button>
          <button class="flex items-center space-x-1">
            <span class="text-sm text-gray-500">ğŸ’¬ 8</span>
          </button>
        </div>
      </div>
    </div>
  </div>
</article>
```

#### å…³é”®CSSé€‰æ‹©å™¨
```python
SELECTORS = {
    'paper_cards': 'article.flex.flex-col.overflow-hidden.rounded-xl.border',
    'title': 'h3.text-xl.font-semibold a',
    'abstract': 'p.mt-3.line-clamp-3.text-sm.text-gray-500',
    'authors': '.flex-shrink-0 .text-sm.font-medium.text-gray-900',
    'date': 'time[datetime]',
    'category': '.text-gray-500:not(time)',
    'likes': 'button .text-sm.text-gray-500:contains("ğŸ‘")',
    'comments': 'button .text-sm.text-gray-500:contains("ğŸ’¬")',
    'paper_link': 'h3 a[href^="/papers/"]',
    'next_page': 'a[rel="next"]'
}
```

### 3. æ•°æ®æå–ç®—æ³•

#### è®ºæ–‡åŸºæœ¬ä¿¡æ¯æå–
```python
def extract_paper_info(card_element):
    paper_info = {}
    
    # æå–æ ‡é¢˜
    title_element = card_element.select_one('h3.text-xl.font-semibold a')
    if title_element:
        paper_info['title'] = title_element.get_text(strip=True)
        paper_info['paper_url'] = urljoin(base_url, title_element.get('href'))
    
    # æå–æ‘˜è¦
    abstract_element = card_element.select_one('p.mt-3.line-clamp-3')
    if abstract_element:
        paper_info['abstract'] = abstract_element.get_text(strip=True)
    
    # æå–å‘å¸ƒæ—¥æœŸ
    date_element = card_element.select_one('time[datetime]')
    if date_element:
        paper_info['date'] = date_element.get('datetime')
        paper_info['display_date'] = date_element.get_text(strip=True)
    
    # æå–åˆ†ç±»
    category_elements = card_element.select('.text-gray-500')
    for element in category_elements:
        text = element.get_text(strip=True)
        if text not in ['â€¢'] and not text.startswith('ğŸ‘') and not text.startswith('ğŸ’¬'):
            if element.name != 'time':
                paper_info['category'] = text
                break
    
    return paper_info
```

#### ä½œè€…ä¿¡æ¯æå–
```python
def extract_authors(card_element):
    authors = []
    author_elements = card_element.select('.flex-shrink-0 .text-sm.font-medium.text-gray-900')
    
    for author_element in author_elements:
        author_name = author_element.get_text(strip=True)
        if author_name:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä½œè€…é“¾æ¥
            author_link = author_element.find_parent('a')
            author_info = {
                'name': author_name,
                'profile_url': urljoin(base_url, author_link.get('href')) if author_link else None
            }
            authors.append(author_info)
    
    return authors
```

#### äº’åŠ¨æ•°æ®æå–
```python
def extract_engagement_metrics(card_element):
    metrics = {
        'likes': 0,
        'comments': 0
    }
    
    # æå–ç‚¹èµæ•°
    like_elements = card_element.select('button .text-sm.text-gray-500')
    for element in like_elements:
        text = element.get_text(strip=True)
        if 'ğŸ‘' in text:
            like_match = re.search(r'ğŸ‘\s*(\d+)', text)
            if like_match:
                metrics['likes'] = int(like_match.group(1))
        elif 'ğŸ’¬' in text:
            comment_match = re.search(r'ğŸ’¬\s*(\d+)', text)
            if comment_match:
                metrics['comments'] = int(comment_match.group(1))
    
    return metrics
```

### 4. è®ºæ–‡è¯¦æƒ…é¡µçˆ¬å–

#### è¯¦æƒ…é¡µURLæ ¼å¼
```
https://huggingface.co/papers/2401.xxxxx
```

#### è¯¦æƒ…é¡µæ•°æ®æå–
```python
def extract_paper_details(paper_url):
    response = requests.get(paper_url, headers=HEADERS)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    details = {}
    
    # æå–arXiv ID
    arxiv_link = soup.select_one('a[href*="arxiv.org"]')
    if arxiv_link:
        arxiv_url = arxiv_link.get('href')
        arxiv_id_match = re.search(r'arxiv\.org/abs/([\d\.]+)', arxiv_url)
        if arxiv_id_match:
            details['arxiv_id'] = arxiv_id_match.group(1)
            details['arxiv_url'] = arxiv_url
    
    # æå–PDFé“¾æ¥
    pdf_link = soup.select_one('a[href*=".pdf"]')
    if pdf_link:
        details['pdf_url'] = pdf_link.get('href')
    
    # æå–GitHubé“¾æ¥
    github_link = soup.select_one('a[href*="github.com"]')
    if github_link:
        details['github_url'] = github_link.get('href')
    
    # æå–å®Œæ•´æ‘˜è¦
    abstract_element = soup.select_one('.prose .text-gray-700')
    if abstract_element:
        details['full_abstract'] = abstract_element.get_text(strip=True)
    
    return details
```

## æ•°æ®å¤„ç†ä¸åˆ†æ

### 1. çƒ­åº¦è¯„åˆ†ç®—æ³•
```python
def calculate_trending_score(paper):
    # åŸºäºå¤šä¸ªç»´åº¦è®¡ç®—çƒ­åº¦åˆ†æ•°
    factors = {
        'likes': paper.get('likes', 0),
        'comments': paper.get('comments', 0),
        'recency': calculate_recency_factor(paper.get('date')),
        'category_weight': get_category_weight(paper.get('category'))
    }
    
    # æƒé‡é…ç½®
    weights = {
        'likes': 0.4,
        'comments': 0.3,
        'recency': 0.2,
        'category_weight': 0.1
    }
    
    score = 0
    for factor, value in factors.items():
        score += value * weights[factor]
    
    return min(score, 10)  # é™åˆ¶æœ€é«˜åˆ†ä¸º10

def calculate_recency_factor(date_str):
    """è®¡ç®—æ—¶æ•ˆæ€§å› å­"""
    if not date_str:
        return 0
    
    paper_date = datetime.strptime(date_str, '%Y-%m-%d')
    days_ago = (datetime.now() - paper_date).days
    
    # è¶Šæ–°çš„è®ºæ–‡åˆ†æ•°è¶Šé«˜
    if days_ago <= 1:
        return 10
    elif days_ago <= 7:
        return 8
    elif days_ago <= 30:
        return 5
    else:
        return 2

def get_category_weight(category):
    """ä¸åŒåˆ†ç±»çš„æƒé‡"""
    category_weights = {
        'Computer Vision': 1.2,
        'Natural Language Processing': 1.2,
        'Machine Learning': 1.1,
        'Robotics': 1.0,
        'Audio': 0.9,
        'Tabular': 0.8
    }
    return category_weights.get(category, 1.0)
```

### 2. åˆ†ç±»æ ‡å‡†åŒ–
```python
CATEGORY_MAPPING = {
    'Computer Vision': 'CV',
    'Natural Language Processing': 'NLP',
    'Machine Learning': 'ML',
    'Multimodal': 'MM',
    'Robotics': 'Robotics',
    'Audio': 'Audio',
    'Tabular': 'Tabular',
    'Reinforcement Learning': 'RL'
}

def normalize_category(category):
    return CATEGORY_MAPPING.get(category, 'Other')
```

### 3. æ•°æ®å»é‡ç­–ç•¥
```python
def deduplicate_papers(papers):
    seen_titles = set()
    seen_arxiv_ids = set()
    unique_papers = []
    
    for paper in papers:
        # åŸºäºæ ‡é¢˜å»é‡
        title_key = paper.get('title', '').lower().strip()
        arxiv_id = paper.get('arxiv_id')
        
        is_duplicate = False
        
        # æ£€æŸ¥arXiv IDé‡å¤
        if arxiv_id and arxiv_id in seen_arxiv_ids:
            is_duplicate = True
        
        # æ£€æŸ¥æ ‡é¢˜ç›¸ä¼¼åº¦
        if not is_duplicate and title_key:
            for seen_title in seen_titles:
                if calculate_title_similarity(title_key, seen_title) > 0.9:
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            unique_papers.append(paper)
            seen_titles.add(title_key)
            if arxiv_id:
                seen_arxiv_ids.add(arxiv_id)
    
    return unique_papers

def calculate_title_similarity(title1, title2):
    """è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦"""
    from difflib import SequenceMatcher
    return SequenceMatcher(None, title1, title2).ratio()
```

## åçˆ¬è™«åº”å¯¹ç­–ç•¥

### 1. è¯·æ±‚å¤´é…ç½®
```python
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Cache-Control': 'max-age=0'
}
```

### 2. è®¿é—®é¢‘ç‡æ§åˆ¶
```python
RATE_LIMIT_CONFIG = {
    'requests_per_minute': 30,  # æ¯åˆ†é’Ÿæœ€å¤š30ä¸ªè¯·æ±‚
    'delay_between_requests': 2,  # è¯·æ±‚é—´éš”2ç§’
    'random_delay_range': (1, 5),  # éšæœºå»¶è¿Ÿ1-5ç§’
    'max_retries': 3,
    'backoff_factor': 1.5
}

class RateLimiter:
    def __init__(self, config):
        self.config = config
        self.request_times = []
    
    def wait_if_needed(self):
        now = time.time()
        
        # æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•
        cutoff_time = now - 60  # 1åˆ†é’Ÿå‰
        self.request_times = [t for t in self.request_times if t > cutoff_time]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é¢‘ç‡é™åˆ¶
        if len(self.request_times) >= self.config['requests_per_minute']:
            sleep_time = 60 - (now - self.request_times[0])
            if sleep_time > 0:
                time.sleep(sleep_time)
        
        # æ·»åŠ éšæœºå»¶è¿Ÿ
        min_delay, max_delay = self.config['random_delay_range']
        delay = random.uniform(min_delay, max_delay)
        time.sleep(delay)
        
        # è®°å½•è¯·æ±‚æ—¶é—´
        self.request_times.append(now)
```

### 3. é”™è¯¯å¤„ç†ä¸é‡è¯•
```python
def robust_request(url, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=HEADERS, timeout=30)
            
            if response.status_code == 200:
                return response
            elif response.status_code == 429:  # Too Many Requests
                wait_time = 2 ** attempt * 60  # æŒ‡æ•°é€€é¿
                logger.warning(f"Rate limited, waiting {wait_time} seconds")
                time.sleep(wait_time)
            elif response.status_code >= 500:  # Server Error
                logger.warning(f"Server error {response.status_code}, retrying...")
                time.sleep(2 ** attempt)
            else:
                logger.error(f"HTTP {response.status_code}: {response.text}")
                break
                
        except requests.exceptions.RequestException as e:
            logger.warning(f"Request failed (attempt {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
    
    return None
```

## é…ç½®å‚æ•°

### çˆ¬å–é…ç½®
```python
HUGGINGFACE_CONFIG = {
    'base_url': 'https://huggingface.co/papers',
    'max_pages': 5,
    'papers_per_page': 20,
    'date_range': 7,  # çˆ¬å–æœ€è¿‘7å¤©çš„è®ºæ–‡
    'categories': [
        'Computer Vision',
        'Natural Language Processing',
        'Machine Learning',
        'Multimodal',
        'Robotics'
    ],
    'min_engagement': {
        'likes': 5,
        'comments': 1
    }
}
```

### è´¨é‡è¿‡æ»¤é…ç½®
```python
QUALITY_FILTERS = {
    'min_abstract_length': 100,
    'required_fields': ['title', 'abstract', 'authors'],
    'exclude_keywords': ['survey', 'review', 'tutorial'],
    'min_trending_score': 3.0,
    'max_days_old': 30
}
```

## æ•°æ®è¾“å‡ºæ ¼å¼

### JSONæ ¼å¼
```json
{
  "paper_id": "hf_2024_01_15_001",
  "title": "Attention Is All You Need for Video Understanding",
  "abstract": "We present a novel approach...",
  "authors": [
    {
      "name": "John Doe",
      "profile_url": "https://huggingface.co/johndoe"
    }
  ],
  "category": "Computer Vision",
  "date": "2024-01-15",
  "engagement": {
    "likes": 42,
    "comments": 8
  },
  "urls": {
    "huggingface": "https://huggingface.co/papers/2401.xxxxx",
    "arxiv": "https://arxiv.org/abs/2401.xxxxx",
    "pdf": "https://arxiv.org/pdf/2401.xxxxx.pdf",
    "github": "https://github.com/author/repo"
  },
  "trending_score": 8.5,
  "quality_score": 7.2
}
```

## å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 1. é¡µé¢åŠ è½½ä¸å®Œæ•´
**é—®é¢˜**: åŠ¨æ€å†…å®¹æœªå®Œå…¨åŠ è½½
**è§£å†³**: 
- å¢åŠ é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´
- æ£€æŸ¥æ˜¯å¦éœ€è¦JavaScriptæ¸²æŸ“
- ä½¿ç”¨Seleniumä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ

### 2. åˆ†é¡µå¤„ç†å¤±è´¥
**é—®é¢˜**: æ— æ³•æ­£ç¡®è¯†åˆ«ä¸‹ä¸€é¡µé“¾æ¥
**è§£å†³**: 
- å¤šç§åˆ†é¡µè¯†åˆ«ç­–ç•¥
- åŸºäºURLå‚æ•°çš„åˆ†é¡µ
- æ‰‹åŠ¨æ„é€ åˆ†é¡µURL

### 3. æ•°æ®æ ¼å¼å˜åŒ–
**é—®é¢˜**: Hugging Faceæ›´æ–°é¡µé¢ç»“æ„
**è§£å†³**: 
- å®šæœŸæ£€æŸ¥CSSé€‰æ‹©å™¨
- å®ç°å¤šå¥—è§£æè§„åˆ™
- æ·»åŠ ç»“æ„å˜åŒ–æ£€æµ‹

### 4. é‡å¤æ•°æ®é—®é¢˜
**é—®é¢˜**: åŒä¸€è®ºæ–‡åœ¨ä¸åŒé¡µé¢å‡ºç°
**è§£å†³**: 
- åŸºäºarXiv IDå»é‡
- æ ‡é¢˜ç›¸ä¼¼åº¦æ£€æµ‹
- ç»´æŠ¤å·²çˆ¬å–è®ºæ–‡åˆ—è¡¨

## ç»´æŠ¤å»ºè®®

### å®šæœŸæ£€æŸ¥é¡¹ç›®
1. **é¡µé¢ç»“æ„**: ç›‘æ§Hugging Faceé¡µé¢å˜åŒ–
2. **æ•°æ®è´¨é‡**: æ£€æŸ¥çˆ¬å–æ•°æ®çš„å®Œæ•´æ€§
3. **æ€§èƒ½ç›‘æ§**: è·Ÿè¸ªçˆ¬å–é€Ÿåº¦å’ŒæˆåŠŸç‡
4. **åˆ†ç±»æ›´æ–°**: å…³æ³¨æ–°å¢çš„è®ºæ–‡åˆ†ç±»

### ä¼˜åŒ–æ–¹å‘
1. **æ™ºèƒ½åˆ†ç±»**: åŸºäºå†…å®¹çš„è‡ªåŠ¨åˆ†ç±»
2. **è¶‹åŠ¿é¢„æµ‹**: é¢„æµ‹è®ºæ–‡çƒ­åº¦è¶‹åŠ¿
3. **ä¸ªæ€§åŒ–æ¨è**: åŸºäºç”¨æˆ·å…´è¶£çš„è®ºæ–‡æ¨è
4. **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒä¸­æ–‡æ‘˜è¦ç¿»è¯‘

## ç›¸å…³èµ„æº

- [Hugging Face Papers](https://huggingface.co/papers)
- [Hugging Face APIæ–‡æ¡£](https://huggingface.co/docs)
- [arXiv API](https://arxiv.org/help/api)
- [è®ºæ–‡è´¨é‡è¯„ä¼°æ ‡å‡†](https://www.nature.com/articles/d41586-019-01643-3)