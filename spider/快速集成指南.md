# 🚀 爬虫存储架构快速集成指南

> 5分钟快速集成现代化爬虫存储方案

## 📋 核心概念

### 存储结构
```
crawled_data/
├── articles/{item_id}/     # 每个条目一个目录
│   ├── metadata.json      # 标准元数据
│   ├── content.md         # 内容文件
│   └── attachments/       # 附件(图片/PDF等)
└── data/                  # 聚合数据
    ├── items_metadata.json
    └── crawl_stats.json
```

### OSS配置
```json
{
  "oss": {
    "provider": "minio",
    "endpoint": "http://localhost:9000",
    "access_key": "minioadmin",
    "secret_key": "minioadmin",
    "bucket_name": "your-project-name",
    "public_url_base": "http://localhost:9000/your-project-name"
  }
}
```

## 🔧 快速集成

### 1. 复制核心文件
```bash
# 复制OSS上传模块
cp -r src/arxiv_system/oss your_project/src/
cp -r src/arxiv_system/utils your_project/src/

# 复制配置文件
cp config.json your_project/
```

### 2. 安装依赖
```bash
pip install aiohttp aiofiles minio pathlib asyncio
```

### 3. 基础爬虫模板
```python
import asyncio
import json
from pathlib import Path
from datetime import datetime

class BaseCrawler:
    def __init__(self, config):
        self.config = config
        self.output_dir = Path(config.get('output_dir', 'crawled_data'))
        self.articles_dir = self.output_dir / 'articles'
        self.data_dir = self.output_dir / 'data'
        
        # 创建目录
        self.articles_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    async def save_item(self, item_data):
        """保存单个条目 - 核心方法"""
        item_id = item_data['id']
        item_dir = self.articles_dir / item_id
        item_dir.mkdir(exist_ok=True)
        
        # 1. 保存metadata.json
        metadata = self._build_metadata(item_data)
        await self._save_json(item_dir / 'metadata.json', metadata)
        
        # 2. 保存content.md
        content = self._build_content(item_data)
        await self._save_text(item_dir / 'content.md', content)
        
        return metadata
    
    def _build_metadata(self, item_data):
        """构建标准元数据格式 - 需要根据项目调整"""
        return {
            'id': item_data['id'],
            'title': item_data['title'],
            'content': item_data.get('content', ''),
            'author': item_data.get('author', ''),
            'published_date': item_data.get('date', ''),
            'source_url': item_data.get('url', ''),
            'processed_date': datetime.now().isoformat(),
            'local_attachments': [],
            'oss_urls': {},
            'indexed': False
        }
    
    def _build_content(self, item_data):
        """构建Markdown内容 - 需要根据项目调整"""
        return f"""# {item_data['title']}

## 基本信息
- **ID**: {item_data['id']}
- **作者**: {item_data.get('author', 'Unknown')}
- **日期**: {item_data.get('date', 'Unknown')}
- **来源**: {item_data.get('url', 'Unknown')}

## 内容
{item_data.get('content', '')}
"""
    
    async def _save_json(self, path, data):
        """保存JSON文件"""
        import aiofiles
        async with aiofiles.open(path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, ensure_ascii=False, indent=2))
    
    async def _save_text(self, path, content):
        """保存文本文件"""
        import aiofiles
        async with aiofiles.open(path, 'w', encoding='utf-8') as f:
            await f.write(content)
```

### 4. OSS上传器使用
```python
from src.oss.wrapper import OSSUploader

async def upload_to_oss(config):
    """上传到OSS"""
    uploader = OSSUploader(config['oss'])
    
    async with uploader:
        results = await uploader.upload_all(
            source_dir='crawled_data',
            concurrent=5,
            resume=True
        )
    
    print(f"✅ 上传完成: {results['success_count']} 个文件")
    print(f"❌ 失败: {results['error_count']} 个文件")
    
    return results
```

### 5. 完整使用示例
```python
import asyncio

class YourCrawler(BaseCrawler):
    async def crawl(self, query, max_results=10):
        """实现你的爬取逻辑"""
        items = []
        
        for i in range(max_results):
            # 这里实现你的具体爬取逻辑
            item_data = {
                'id': f'item_{i}',
                'title': f'Sample Item {i}',
                'content': f'This is content for item {i}',
                'author': f'Author {i}',
                'date': '2025-08-09',
                'url': f'https://example.com/item/{i}'
            }
            
            # 保存数据
            metadata = await self.save_item(item_data)
            items.append(metadata)
        
        # 保存聚合数据
        await self._save_json(self.data_dir / 'items_metadata.json', items)
        
        return items

async def main():
    # 配置
    config = {
        'output_dir': 'crawled_data',
        'oss': {
            'provider': 'minio',
            'endpoint': 'http://localhost:9000',
            'access_key': 'minioadmin',
            'secret_key': 'minioadmin',
            'bucket_name': 'your-project',
            'public_url_base': 'http://localhost:9000/your-project'
        }
    }
    
    # 爬取
    crawler = YourCrawler(config)
    items = await crawler.crawl('your_query', max_results=5)
    print(f"爬取完成: {len(items)} 个条目")
    
    # 上传
    results = await upload_to_oss(config)
    print("上传完成!")

if __name__ == '__main__':
    asyncio.run(main())
```

## 🎯 项目适配指南

### 新闻爬虫适配
```python
def _build_metadata(self, item_data):
    return {
        'id': item_data['url_hash'],
        'title': item_data['title'],
        'summary': item_data['summary'],
        'author': item_data['author'],
        'published_date': item_data['date'],
        'source': item_data['source'],
        'category': item_data['category'],
        'tags': item_data['tags'],
        'source_url': item_data['url'],
        'processed_date': datetime.now().isoformat(),
        'local_images': item_data.get('images', []),
        'oss_urls': {},
        'indexed': False
    }
```

### 社交媒体适配
```python
def _build_metadata(self, item_data):
    return {
        'id': item_data['post_id'],
        'content': item_data['text'],
        'author': item_data['username'],
        'author_id': item_data['user_id'],
        'published_date': item_data['created_at'],
        'platform': 'twitter',  # or 'reddit', 'linkedin'
        'likes': item_data.get('likes', 0),
        'shares': item_data.get('shares', 0),
        'comments': item_data.get('comments', 0),
        'hashtags': item_data.get('hashtags', []),
        'mentions': item_data.get('mentions', []),
        'source_url': item_data['url'],
        'processed_date': datetime.now().isoformat(),
        'local_media': item_data.get('media', []),
        'oss_urls': {},
        'indexed': False
    }
```

### 电商产品适配
```python
def _build_metadata(self, item_data):
    return {
        'id': item_data['product_id'],
        'title': item_data['name'],
        'description': item_data['description'],
        'brand': item_data['brand'],
        'category': item_data['category'],
        'price': item_data['price'],
        'currency': item_data['currency'],
        'rating': item_data.get('rating', 0),
        'reviews_count': item_data.get('reviews_count', 0),
        'availability': item_data.get('availability', 'unknown'),
        'source_url': item_data['url'],
        'processed_date': datetime.now().isoformat(),
        'local_images': item_data.get('images', []),
        'oss_urls': {},
        'indexed': False
    }
```

## 📊 OSS访问示例

上传完成后，你的数据可以通过以下URL访问:

```
# 元数据
http://localhost:9000/your-bucket/articles/item_1/metadata.json

# 内容
http://localhost:9000/your-bucket/articles/item_1/content.md

# 聚合数据
http://localhost:9000/your-bucket/data/items_metadata.json
```

## 🔧 常用配置

### MinIO本地部署
```bash
# 下载MinIO
wget https://dl.min.io/server/minio/release/windows-amd64/minio.exe

# 启动MinIO
.\minio.exe server .\minio-data --console-address ":9001"

# 访问控制台: http://localhost:9001
# 用户名/密码: minioadmin/minioadmin
```

### 阿里云OSS配置
```json
{
  "oss": {
    "provider": "aliyun",
    "endpoint": "https://oss-cn-hangzhou.aliyuncs.com",
    "access_key": "your_access_key",
    "secret_key": "your_secret_key",
    "bucket_name": "your-bucket-name",
    "public_url_base": "https://your-bucket-name.oss-cn-hangzhou.aliyuncs.com"
  }
}
```

## 🚨 注意事项

1. **ID唯一性**: 确保每个条目的ID唯一
2. **文件名安全**: 避免特殊字符，使用URL安全的文件名
3. **并发控制**: 根据目标网站调整并发数
4. **错误处理**: 实现重试机制和异常处理
5. **数据备份**: 定期备份重要数据

## 📞 快速问题解决

### 常见问题

**Q: OSS连接失败?**
A: 检查endpoint、access_key、secret_key配置

**Q: 上传速度慢?**
A: 调整concurrent参数，增加并发数

**Q: 文件重复上传?**
A: 使用resume=True参数启用断点续传

**Q: 内存占用过高?**
A: 减少并发数，增加处理间隔

---

**简单来说**: 复制文件 → 改配置 → 实现爬取逻辑 → 调用保存方法 → 上传OSS，就这么简单！🎉