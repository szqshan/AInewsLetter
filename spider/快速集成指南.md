# ğŸš€ çˆ¬è™«å­˜å‚¨æ¶æ„å¿«é€Ÿé›†æˆæŒ‡å—

> 5åˆ†é’Ÿå¿«é€Ÿé›†æˆç°ä»£åŒ–çˆ¬è™«å­˜å‚¨æ–¹æ¡ˆ

## ğŸ“‹ æ ¸å¿ƒæ¦‚å¿µ

### å­˜å‚¨ç»“æ„
```
crawled_data/
â”œâ”€â”€ articles/{item_id}/     # æ¯ä¸ªæ¡ç›®ä¸€ä¸ªç›®å½•
â”‚   â”œâ”€â”€ metadata.json      # æ ‡å‡†å…ƒæ•°æ®
â”‚   â”œâ”€â”€ content.md         # å†…å®¹æ–‡ä»¶
â”‚   â””â”€â”€ attachments/       # é™„ä»¶(å›¾ç‰‡/PDFç­‰)
â””â”€â”€ data/                  # èšåˆæ•°æ®
    â”œâ”€â”€ items_metadata.json
    â””â”€â”€ crawl_stats.json
```

### OSSé…ç½®
```json
{
  "oss": {
    "provider": "minio",
    "endpoint": "http://localhost:9000",
    "access_key": "minioadmin",
    "secret_key": "minioadmin",
    "bucket_name": "your-project-name",
    "public_url_base": "http://localhost:9000/your-project-name"
  }
}
```

## ğŸ”§ å¿«é€Ÿé›†æˆ

### 1. å¤åˆ¶æ ¸å¿ƒæ–‡ä»¶
```bash
# å¤åˆ¶OSSä¸Šä¼ æ¨¡å—
cp -r src/arxiv_system/oss your_project/src/
cp -r src/arxiv_system/utils your_project/src/

# å¤åˆ¶é…ç½®æ–‡ä»¶
cp config.json your_project/
```

### 2. å®‰è£…ä¾èµ–
```bash
pip install aiohttp aiofiles minio pathlib asyncio
```

### 3. åŸºç¡€çˆ¬è™«æ¨¡æ¿
```python
import asyncio
import json
from pathlib import Path
from datetime import datetime

class BaseCrawler:
    def __init__(self, config):
        self.config = config
        self.output_dir = Path(config.get('output_dir', 'crawled_data'))
        self.articles_dir = self.output_dir / 'articles'
        self.data_dir = self.output_dir / 'data'
        
        # åˆ›å»ºç›®å½•
        self.articles_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    async def save_item(self, item_data):
        """ä¿å­˜å•ä¸ªæ¡ç›® - æ ¸å¿ƒæ–¹æ³•"""
        item_id = item_data['id']
        item_dir = self.articles_dir / item_id
        item_dir.mkdir(exist_ok=True)
        
        # 1. ä¿å­˜metadata.json
        metadata = self._build_metadata(item_data)
        await self._save_json(item_dir / 'metadata.json', metadata)
        
        # 2. ä¿å­˜content.md
        content = self._build_content(item_data)
        await self._save_text(item_dir / 'content.md', content)
        
        return metadata
    
    def _build_metadata(self, item_data):
        """æ„å»ºæ ‡å‡†å…ƒæ•°æ®æ ¼å¼ - éœ€è¦æ ¹æ®é¡¹ç›®è°ƒæ•´"""
        return {
            'id': item_data['id'],
            'title': item_data['title'],
            'content': item_data.get('content', ''),
            'author': item_data.get('author', ''),
            'published_date': item_data.get('date', ''),
            'source_url': item_data.get('url', ''),
            'processed_date': datetime.now().isoformat(),
            'local_attachments': [],
            'oss_urls': {},
            'indexed': False
        }
    
    def _build_content(self, item_data):
        """æ„å»ºMarkdownå†…å®¹ - éœ€è¦æ ¹æ®é¡¹ç›®è°ƒæ•´"""
        return f"""# {item_data['title']}

## åŸºæœ¬ä¿¡æ¯
- **ID**: {item_data['id']}
- **ä½œè€…**: {item_data.get('author', 'Unknown')}
- **æ—¥æœŸ**: {item_data.get('date', 'Unknown')}
- **æ¥æº**: {item_data.get('url', 'Unknown')}

## å†…å®¹
{item_data.get('content', '')}
"""
    
    async def _save_json(self, path, data):
        """ä¿å­˜JSONæ–‡ä»¶"""
        import aiofiles
        async with aiofiles.open(path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, ensure_ascii=False, indent=2))
    
    async def _save_text(self, path, content):
        """ä¿å­˜æ–‡æœ¬æ–‡ä»¶"""
        import aiofiles
        async with aiofiles.open(path, 'w', encoding='utf-8') as f:
            await f.write(content)
```

### 4. OSSä¸Šä¼ å™¨ä½¿ç”¨
```python
from src.oss.wrapper import OSSUploader

async def upload_to_oss(config):
    """ä¸Šä¼ åˆ°OSS"""
    uploader = OSSUploader(config['oss'])
    
    async with uploader:
        results = await uploader.upload_all(
            source_dir='crawled_data',
            concurrent=5,
            resume=True
        )
    
    print(f"âœ… ä¸Šä¼ å®Œæˆ: {results['success_count']} ä¸ªæ–‡ä»¶")
    print(f"âŒ å¤±è´¥: {results['error_count']} ä¸ªæ–‡ä»¶")
    
    return results
```

### 5. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
```python
import asyncio

class YourCrawler(BaseCrawler):
    async def crawl(self, query, max_results=10):
        """å®ç°ä½ çš„çˆ¬å–é€»è¾‘"""
        items = []
        
        for i in range(max_results):
            # è¿™é‡Œå®ç°ä½ çš„å…·ä½“çˆ¬å–é€»è¾‘
            item_data = {
                'id': f'item_{i}',
                'title': f'Sample Item {i}',
                'content': f'This is content for item {i}',
                'author': f'Author {i}',
                'date': '2025-08-09',
                'url': f'https://example.com/item/{i}'
            }
            
            # ä¿å­˜æ•°æ®
            metadata = await self.save_item(item_data)
            items.append(metadata)
        
        # ä¿å­˜èšåˆæ•°æ®
        await self._save_json(self.data_dir / 'items_metadata.json', items)
        
        return items

async def main():
    # é…ç½®
    config = {
        'output_dir': 'crawled_data',
        'oss': {
            'provider': 'minio',
            'endpoint': 'http://localhost:9000',
            'access_key': 'minioadmin',
            'secret_key': 'minioadmin',
            'bucket_name': 'your-project',
            'public_url_base': 'http://localhost:9000/your-project'
        }
    }
    
    # çˆ¬å–
    crawler = YourCrawler(config)
    items = await crawler.crawl('your_query', max_results=5)
    print(f"çˆ¬å–å®Œæˆ: {len(items)} ä¸ªæ¡ç›®")
    
    # ä¸Šä¼ 
    results = await upload_to_oss(config)
    print("ä¸Šä¼ å®Œæˆ!")

if __name__ == '__main__':
    asyncio.run(main())
```

## ğŸ¯ é¡¹ç›®é€‚é…æŒ‡å—

### æ–°é—»çˆ¬è™«é€‚é…
```python
def _build_metadata(self, item_data):
    return {
        'id': item_data['url_hash'],
        'title': item_data['title'],
        'summary': item_data['summary'],
        'author': item_data['author'],
        'published_date': item_data['date'],
        'source': item_data['source'],
        'category': item_data['category'],
        'tags': item_data['tags'],
        'source_url': item_data['url'],
        'processed_date': datetime.now().isoformat(),
        'local_images': item_data.get('images', []),
        'oss_urls': {},
        'indexed': False
    }
```

### ç¤¾äº¤åª’ä½“é€‚é…
```python
def _build_metadata(self, item_data):
    return {
        'id': item_data['post_id'],
        'content': item_data['text'],
        'author': item_data['username'],
        'author_id': item_data['user_id'],
        'published_date': item_data['created_at'],
        'platform': 'twitter',  # or 'reddit', 'linkedin'
        'likes': item_data.get('likes', 0),
        'shares': item_data.get('shares', 0),
        'comments': item_data.get('comments', 0),
        'hashtags': item_data.get('hashtags', []),
        'mentions': item_data.get('mentions', []),
        'source_url': item_data['url'],
        'processed_date': datetime.now().isoformat(),
        'local_media': item_data.get('media', []),
        'oss_urls': {},
        'indexed': False
    }
```

### ç”µå•†äº§å“é€‚é…
```python
def _build_metadata(self, item_data):
    return {
        'id': item_data['product_id'],
        'title': item_data['name'],
        'description': item_data['description'],
        'brand': item_data['brand'],
        'category': item_data['category'],
        'price': item_data['price'],
        'currency': item_data['currency'],
        'rating': item_data.get('rating', 0),
        'reviews_count': item_data.get('reviews_count', 0),
        'availability': item_data.get('availability', 'unknown'),
        'source_url': item_data['url'],
        'processed_date': datetime.now().isoformat(),
        'local_images': item_data.get('images', []),
        'oss_urls': {},
        'indexed': False
    }
```

## ğŸ“Š OSSè®¿é—®ç¤ºä¾‹

ä¸Šä¼ å®Œæˆåï¼Œä½ çš„æ•°æ®å¯ä»¥é€šè¿‡ä»¥ä¸‹URLè®¿é—®:

```
# å…ƒæ•°æ®
http://localhost:9000/your-bucket/articles/item_1/metadata.json

# å†…å®¹
http://localhost:9000/your-bucket/articles/item_1/content.md

# èšåˆæ•°æ®
http://localhost:9000/your-bucket/data/items_metadata.json
```

## ğŸ”§ å¸¸ç”¨é…ç½®

### MinIOæœ¬åœ°éƒ¨ç½²
```bash
# ä¸‹è½½MinIO
wget https://dl.min.io/server/minio/release/windows-amd64/minio.exe

# å¯åŠ¨MinIO
.\minio.exe server .\minio-data --console-address ":9001"

# è®¿é—®æ§åˆ¶å°: http://localhost:9001
# ç”¨æˆ·å/å¯†ç : minioadmin/minioadmin
```

### é˜¿é‡Œäº‘OSSé…ç½®
```json
{
  "oss": {
    "provider": "aliyun",
    "endpoint": "https://oss-cn-hangzhou.aliyuncs.com",
    "access_key": "your_access_key",
    "secret_key": "your_secret_key",
    "bucket_name": "your-bucket-name",
    "public_url_base": "https://your-bucket-name.oss-cn-hangzhou.aliyuncs.com"
  }
}
```

## ğŸš¨ æ³¨æ„äº‹é¡¹

1. **IDå”¯ä¸€æ€§**: ç¡®ä¿æ¯ä¸ªæ¡ç›®çš„IDå”¯ä¸€
2. **æ–‡ä»¶åå®‰å…¨**: é¿å…ç‰¹æ®Šå­—ç¬¦ï¼Œä½¿ç”¨URLå®‰å…¨çš„æ–‡ä»¶å
3. **å¹¶å‘æ§åˆ¶**: æ ¹æ®ç›®æ ‡ç½‘ç«™è°ƒæ•´å¹¶å‘æ•°
4. **é”™è¯¯å¤„ç†**: å®ç°é‡è¯•æœºåˆ¶å’Œå¼‚å¸¸å¤„ç†
5. **æ•°æ®å¤‡ä»½**: å®šæœŸå¤‡ä»½é‡è¦æ•°æ®

## ğŸ“ å¿«é€Ÿé—®é¢˜è§£å†³

### å¸¸è§é—®é¢˜

**Q: OSSè¿æ¥å¤±è´¥?**
A: æ£€æŸ¥endpointã€access_keyã€secret_keyé…ç½®

**Q: ä¸Šä¼ é€Ÿåº¦æ…¢?**
A: è°ƒæ•´concurrentå‚æ•°ï¼Œå¢åŠ å¹¶å‘æ•°

**Q: æ–‡ä»¶é‡å¤ä¸Šä¼ ?**
A: ä½¿ç”¨resume=Trueå‚æ•°å¯ç”¨æ–­ç‚¹ç»­ä¼ 

**Q: å†…å­˜å ç”¨è¿‡é«˜?**
A: å‡å°‘å¹¶å‘æ•°ï¼Œå¢åŠ å¤„ç†é—´éš”

---

**ç®€å•æ¥è¯´**: å¤åˆ¶æ–‡ä»¶ â†’ æ”¹é…ç½® â†’ å®ç°çˆ¬å–é€»è¾‘ â†’ è°ƒç”¨ä¿å­˜æ–¹æ³• â†’ ä¸Šä¼ OSSï¼Œå°±è¿™ä¹ˆç®€å•ï¼ğŸ‰