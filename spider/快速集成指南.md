# ğŸš€ çˆ¬è™«å­˜å‚¨æ¶æ„å¿«é€Ÿé›†æˆæŒ‡å—

> åŸºäºarXivçˆ¬è™«çš„ä¼ä¸šçº§ä¸‰å±‚å­˜å‚¨æ¶æ„ï¼Œæä¾›æ ‡å‡†åŒ–çš„å­˜å‚¨è§£å†³æ–¹æ¡ˆ

## ğŸ“‹ æ ¸å¿ƒæ¦‚å¿µ

### ä¸‰å±‚å­˜å‚¨æ¶æ„
- **æœ¬åœ°å­˜å‚¨**: åŸå§‹æ•°æ®ä¿å­˜åœ¨ `crawled_data/` ç›®å½•ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
- **MinIOå¯¹è±¡å­˜å‚¨**: ä¼ä¸šçº§å¯¹è±¡å­˜å‚¨ï¼Œæ”¯æŒå…¬å¼€è®¿é—®å’ŒRESTful API
- **PostgreSQLæ•°æ®åº“**: å…ƒæ•°æ®å­˜å‚¨ï¼Œæ”¯æŒå¤æ‚æŸ¥è¯¢å’Œå…³ç³»ç®¡ç†
- **Elasticsearchæœç´¢**: å…¨æ–‡æ£€ç´¢å¼•æ“ï¼Œæ”¯æŒé«˜æ€§èƒ½æœç´¢å’Œåˆ†æ

### å­˜å‚¨ç»“æ„
```
crawled_data/
â”œâ”€â”€ articles/{item_id}/     # æ¯ä¸ªæ¡ç›®ä¸€ä¸ªç›®å½•
â”‚   â”œâ”€â”€ metadata.json      # æ ‡å‡†å…ƒæ•°æ®
â”‚   â”œâ”€â”€ content.md         # å†…å®¹æ–‡ä»¶
â”‚   â””â”€â”€ attachments/       # é™„ä»¶(å›¾ç‰‡/PDFç­‰)
â””â”€â”€ data/                  # èšåˆæ•°æ®
    â”œâ”€â”€ items_metadata.json
    â””â”€â”€ crawl_stats.json
```

### MinIOé…ç½®ï¼ˆæœ€æ–°ï¼‰
```json
{
  "minio": {
    "endpoint": "http://localhost:9000",
    "access_key": "minioadmin",
    "secret_key": "minioadmin",
    "bucket_name": "arxiv-papers",
    "public_url_base": "http://localhost:9000/arxiv-papers",
    "secure": false
  },
  "postgres": {
    "host": "localhost",
    "port": 5432,
    "database": "arxiv_db",
    "user": "postgres",
    "password": "password"
  },
  "elasticsearch": {
    "host": "localhost",
    "port": 9200,
    "index": "arxiv_papers"
  }
}
```

## ğŸ”§ å¿«é€Ÿé›†æˆ

### 1. å¤åˆ¶æ ¸å¿ƒæ–‡ä»¶
```bash
# å¤åˆ¶arXivå­˜å‚¨æ¶æ„æ ¸å¿ƒæ–‡ä»¶
cp -r src/arxiv_system/storage your_project/src/
cp -r src/arxiv_system/utils your_project/src/
cp -r src/arxiv_system/oss your_project/src/

# å¤åˆ¶é…ç½®æ–‡ä»¶
cp config.json your_project/
```

### 2. å®‰è£…ä¾èµ–
```bash
pip install aiohttp aiofiles minio pathlib asyncio psycopg2-binary elasticsearch
# æ ¸å¿ƒä¾èµ–åŒ…æ‹¬ï¼š
# - minio: MinIOå®¢æˆ·ç«¯
# - psycopg2-binary: PostgreSQLè¿æ¥å™¨
# - elasticsearch: Elasticsearchå®¢æˆ·ç«¯
# - asyncio: å¼‚æ­¥å¤„ç†
# - aiofiles: å¼‚æ­¥æ–‡ä»¶æ“ä½œ
```

### 3. åŸºç¡€çˆ¬è™«æ¨¡æ¿
```python
import asyncio
import json
from pathlib import Path
from datetime import datetime

class BaseCrawler:
    def __init__(self, config):
        self.config = config
        self.output_dir = Path(config.get('output_dir', 'crawled_data'))
        self.articles_dir = self.output_dir / 'articles'
        self.data_dir = self.output_dir / 'data'
        
        # åˆ›å»ºç›®å½•
        self.articles_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir.mkdir(parents=True, exist_ok=True)
    
    async def save_item(self, item_data):
        """ä¿å­˜å•ä¸ªæ¡ç›® - æ ¸å¿ƒæ–¹æ³•"""
        item_id = item_data['id']
        item_dir = self.articles_dir / item_id
        item_dir.mkdir(exist_ok=True)
        
        # 1. ä¿å­˜metadata.json
        metadata = self._build_metadata(item_data)
        await self._save_json(item_dir / 'metadata.json', metadata)
        
        # 2. ä¿å­˜content.md
        content = self._build_content(item_data)
        await self._save_text(item_dir / 'content.md', content)
        
        return metadata
    
    def _build_metadata(self, item_data):
        """æ„å»ºæ ‡å‡†å…ƒæ•°æ®æ ¼å¼ - éœ€è¦æ ¹æ®é¡¹ç›®è°ƒæ•´"""
        return {
            'id': item_data['id'],
            'title': item_data['title'],
            'content': item_data.get('content', ''),
            'author': item_data.get('author', ''),
            'published_date': item_data.get('date', ''),
            'source_url': item_data.get('url', ''),
            'processed_date': datetime.now().isoformat(),
            'local_attachments': [],
            'oss_urls': {},
            'indexed': False
        }
    
    def _build_content(self, item_data):
        """æ„å»ºMarkdownå†…å®¹ - éœ€è¦æ ¹æ®é¡¹ç›®è°ƒæ•´"""
        return f"""# {item_data['title']}

## åŸºæœ¬ä¿¡æ¯
- **ID**: {item_data['id']}
- **ä½œè€…**: {item_data.get('author', 'Unknown')}
- **æ—¥æœŸ**: {item_data.get('date', 'Unknown')}
- **æ¥æº**: {item_data.get('url', 'Unknown')}

## å†…å®¹
{item_data.get('content', '')}
"""
    
    async def _save_json(self, path, data):
        """ä¿å­˜JSONæ–‡ä»¶"""
        import aiofiles
        async with aiofiles.open(path, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, ensure_ascii=False, indent=2))
    
    async def _save_text(self, path, content):
        """ä¿å­˜æ–‡æœ¬æ–‡ä»¶"""
        import aiofiles
        async with aiofiles.open(path, 'w', encoding='utf-8') as f:
            await f.write(content)
```

### 4. MinIOè¿æ¥å™¨ä½¿ç”¨ï¼ˆæœ€æ–°APIï¼‰
```python
from src.oss.wrapper import OSSUploader
from src.storage.minio_connector import MinIOConnector
from src.storage.postgres_manager import PostgresManager
from src.storage.elasticsearch_manager import ElasticsearchManager

async def upload_to_storage(config):
    """ä¸Šä¼ åˆ°ä¸‰å±‚å­˜å‚¨æ¶æ„"""
    # åˆå§‹åŒ–ä¸‰å±‚å­˜å‚¨
    minio = MinIOConnector(config['minio'])
    postgres = PostgresManager(config['postgres'])
    elasticsearch = ElasticsearchManager(config['elasticsearch'])
    
    # ä¼ ç»ŸOSSä¸Šä¼ å™¨ï¼ˆå…¼å®¹æ€§ï¼‰
    uploader = OSSUploader(config.get('oss', config['minio']))
    
    async with uploader:
        results = await uploader.upload_all(
            source_dir='crawled_data',
            concurrent=5,
            resume=True
        )
    
    print(f"âœ… ä¸Šä¼ å®Œæˆ: {results['success_count']} ä¸ªæ–‡ä»¶")
    print(f"âŒ å¤±è´¥: {results['error_count']} ä¸ªæ–‡ä»¶")
    
    return results
```

### 5. å®Œæ•´ä½¿ç”¨ç¤ºä¾‹ï¼ˆä¸‰å±‚å­˜å‚¨æ¶æ„ï¼‰
```python
import asyncio
from src.storage.minio_connector import MinIOConnector
from src.storage.postgres_manager import PostgresManager
from src.storage.elasticsearch_manager import ElasticsearchManager

class ModernCrawler(BaseCrawler):
    def __init__(self, config):
        super().__init__(config)
        # åˆå§‹åŒ–ä¸‰å±‚å­˜å‚¨
        self.minio = MinIOConnector(config['minio'])
        self.postgres = PostgresManager(config['postgres'])
        self.elasticsearch = ElasticsearchManager(config['elasticsearch'])
    
    async def crawl_and_store(self, query, max_results=10):
        """çˆ¬å–å¹¶å­˜å‚¨åˆ°ä¸‰å±‚æ¶æ„"""
        items = []
        
        for i in range(max_results):
            # è¿™é‡Œå®ç°ä½ çš„å…·ä½“çˆ¬å–é€»è¾‘
            item_data = {
                'id': f'item_{i}',
                'title': f'Sample Item {i}',
                'content': f'This is content for item {i}',
                'author': f'Author {i}',
                'date': '2025-08-09',
                'url': f'https://example.com/item/{i}'
            }
            
            # ä¿å­˜åˆ°ä¸‰å±‚å­˜å‚¨
            metadata = await self.save_item_to_storage(item_data)
            items.append(metadata)
        
        return items
    
    async def save_item_to_storage(self, item_data):
        """ä¿å­˜åˆ°ä¸‰å±‚å­˜å‚¨æ¶æ„"""
        # 1. æœ¬åœ°å­˜å‚¨
        metadata = await self.save_item(item_data)
        
        # 2. MinIOå¯¹è±¡å­˜å‚¨
        item_dir = self.articles_dir / item_data['id']
        minio_url = await self.minio.upload_file(
            item_dir / 'content.md', 
            f"articles/{item_data['id']}/content.md"
        )
        
        # 3. PostgreSQLå…ƒæ•°æ®
        await self.postgres.insert_paper({
            'id': item_data['id'],
            'title': item_data['title'],
            'author': item_data['author'],
            'minio_url': minio_url,
            'status': 'completed'
        })
        
        # 4. Elasticsearchå…¨æ–‡ç´¢å¼•
        await self.elasticsearch.index_document({
            'id': item_data['id'],
            'title': item_data['title'],
            'content': item_data['content'],
            'minio_url': minio_url
        })
        
        metadata['minio_url'] = minio_url
        return metadata

async def main():
    # é…ç½®
    config = {
        'output_dir': 'crawled_data',
        'minio': {
            'endpoint': 'http://localhost:9000',
            'access_key': 'minioadmin',
            'secret_key': 'minioadmin',
            'bucket_name': 'arxiv-papers',
            'public_url_base': 'http://localhost:9000/arxiv-papers'
        },
        'postgres': {
            'host': 'localhost',
            'port': 5432,
            'database': 'arxiv_db',
            'user': 'postgres',
            'password': 'password'
        },
        'elasticsearch': {
            'host': 'localhost',
            'port': 9200,
            'index': 'arxiv_papers'
        }
    }
    
    # çˆ¬å–å¹¶å­˜å‚¨
    crawler = ModernCrawler(config)
    items = await crawler.crawl_and_store('your_query', max_results=5)
    print(f"âœ… çˆ¬å–å¹¶å­˜å‚¨å®Œæˆ: {len(items)} ä¸ªæ¡ç›®")
    
    # éªŒè¯å­˜å‚¨
    print("ğŸ” éªŒè¯ä¸‰å±‚å­˜å‚¨...")
    for item in items:
        print(f"  - {item['title']}: {item.get('minio_url', 'N/A')}")

if __name__ == '__main__':
    asyncio.run(main())
```

## ğŸ¯ é¡¹ç›®é€‚é…æŒ‡å—

### æ–°é—»çˆ¬è™«é€‚é…ï¼ˆåŸºäºarXivä¸‰å±‚æ¶æ„ï¼‰
```python
from src.storage.minio_connector import MinIOConnector
from src.storage.postgres_manager import PostgresManager
from src.storage.elasticsearch_manager import ElasticsearchManager

class NewsSpider(BaseCrawler):
    def __init__(self, config):
        super().__init__(config)
        self.minio = MinIOConnector(config['minio'])
        self.postgres = PostgresManager(config['postgres'])
        self.elasticsearch = ElasticsearchManager(config['elasticsearch'])
    
    async def crawl_news(self, source_url):
        """æ–°é—»çˆ¬å–å¹¶å­˜å‚¨åˆ°ä¸‰å±‚æ¶æ„"""
        news_data = await self.fetch_news(source_url)
        await self.store_news(news_data)
    
    async def store_news(self, news_data):
        """å­˜å‚¨æ–°é—»åˆ°ä¸‰å±‚æ¶æ„"""
        # 1. æœ¬åœ°å­˜å‚¨
        local_path = await self.save_local(news_data)
        
        # 2. MinIOå­˜å‚¨
        minio_url = await self.minio.upload_file(
            local_path, f"news/{news_data['id']}.md"
        )
        
        # 3. PostgreSQLå…ƒæ•°æ®
        await self.postgres.insert_news({
            'id': news_data['id'],
            'title': news_data['title'],
            'source': news_data['source'],
            'minio_url': minio_url
        })
        
        # 4. Elasticsearchç´¢å¼•
        await self.elasticsearch.index_document({
            'id': news_data['id'],
            'title': news_data['title'],
            'content': news_data['content'],
            'category': 'news'
        })

    def _build_metadata(self, item_data):
        return {
            'id': item_data['url_hash'],
            'title': item_data['title'],
            'summary': item_data['summary'],
            'author': item_data['author'],
            'published_date': item_data['date'],
            'source': item_data['source'],
            'category': item_data['category'],
            'tags': item_data['tags'],
            'source_url': item_data['url'],
            'processed_date': datetime.now().isoformat(),
            'local_images': item_data.get('images', []),
            'oss_urls': {},
            'indexed': False
        }
```

### ç¤¾äº¤åª’ä½“é€‚é…ï¼ˆåŸºäºarXivä¸‰å±‚æ¶æ„ï¼‰
```python
class SocialMediaSpider(BaseCrawler):
    def __init__(self, config):
        super().__init__(config)
        self.minio = MinIOConnector(config['minio'])
        self.postgres = PostgresManager(config['postgres'])
        self.elasticsearch = ElasticsearchManager(config['elasticsearch'])
    
    async def crawl_posts(self, platform, user_id):
        """ç¤¾äº¤åª’ä½“çˆ¬å–å¹¶å­˜å‚¨"""
        posts = await self.fetch_posts(platform, user_id)
        
        for post in posts:
            await self.store_post(post, platform)
    
    async def store_post(self, post_data, platform):
        """å­˜å‚¨ç¤¾äº¤åª’ä½“å†…å®¹"""
        # å­˜å‚¨åˆ°ä¸‰å±‚æ¶æ„ï¼ˆç±»ä¼¼arXivæ¨¡å¼ï¼‰
        local_path = await self.save_local(post_data)
        minio_url = await self.minio.upload_file(
            local_path, f"social/{platform}/{post_data['id']}.md"
        )
        
        await self.postgres.insert_post({
            'id': post_data['id'],
            'platform': platform,
            'author': post_data['author'],
            'minio_url': minio_url
        })
        
        await self.elasticsearch.index_document({
            'id': post_data['id'],
            'content': post_data['content'],
            'platform': platform,
            'category': 'social_media'
        })

    def _build_metadata(self, item_data):
        return {
            'id': item_data['post_id'],
            'content': item_data['text'],
            'author': item_data['username'],
            'author_id': item_data['user_id'],
            'published_date': item_data['created_at'],
            'platform': 'twitter',  # or 'reddit', 'linkedin'
            'likes': item_data.get('likes', 0),
            'shares': item_data.get('shares', 0),
            'comments': item_data.get('comments', 0),
            'hashtags': item_data.get('hashtags', []),
            'mentions': item_data.get('mentions', []),
            'source_url': item_data['url'],
            'processed_date': datetime.now().isoformat(),
            'local_media': item_data.get('media', []),
            'oss_urls': {},
            'indexed': False
        }
```

### ç”µå•†äº§å“é€‚é…ï¼ˆåŸºäºarXivä¸‰å±‚æ¶æ„ï¼‰
```python
class ProductSpider(BaseCrawler):
    def __init__(self, config):
        super().__init__(config)
        self.minio = MinIOConnector(config['minio'])
        self.postgres = PostgresManager(config['postgres'])
        self.elasticsearch = ElasticsearchManager(config['elasticsearch'])
    
    async def crawl_products(self, category):
        """ç”µå•†äº§å“çˆ¬å–å¹¶å­˜å‚¨"""
        products = await self.fetch_products(category)
        
        for product in products:
            await self.store_product(product, category)
    
    async def store_product(self, product_data, category):
        """å­˜å‚¨äº§å“ä¿¡æ¯åˆ°ä¸‰å±‚æ¶æ„"""
        local_path = await self.save_local(product_data)
        minio_url = await self.minio.upload_file(
            local_path, f"products/{category}/{product_data['id']}.md"
        )
        
        await self.postgres.insert_product({
            'id': product_data['id'],
            'name': product_data['name'],
            'category': category,
            'price': product_data['price'],
            'minio_url': minio_url
        })
        
        await self.elasticsearch.index_document({
            'id': product_data['id'],
            'name': product_data['name'],
            'description': product_data['description'],
            'category': 'product'
        })

    def _build_metadata(self, item_data):
        return {
            'id': item_data['product_id'],
            'title': item_data['name'],
            'description': item_data['description'],
            'brand': item_data['brand'],
            'category': item_data['category'],
            'price': item_data['price'],
            'currency': item_data['currency'],
            'rating': item_data.get('rating', 0),
            'reviews_count': item_data.get('reviews_count', 0),
            'availability': item_data.get('availability', 'unknown'),
            'source_url': item_data['url'],
            'processed_date': datetime.now().isoformat(),
            'local_images': item_data.get('images', []),
            'oss_urls': {},
            'indexed': False
        }
```

## ğŸ“Š ä¸‰å±‚å­˜å‚¨è®¿é—®ç¤ºä¾‹

### MinIOå¯¹è±¡å­˜å‚¨è®¿é—®
```python
from src.storage.minio_connector import MinIOConnector

async def access_minio_files():
    config = {
        'endpoint': 'localhost:9000',
        'access_key': 'minioadmin',
        'secret_key': 'minioadmin',
        'bucket_name': 'arxiv-papers',
        'secure': False
    }
    
    minio = MinIOConnector(config)
    async with minio:
        # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
        files = await minio.list_files()
        print(f"ğŸ“ æ€»å…± {len(files)} ä¸ªæ–‡ä»¶")
        
        # æŒ‰å‰ç¼€è¿‡æ»¤
        papers = await minio.list_files(prefix="papers/")
        print(f"ğŸ“„ è®ºæ–‡æ–‡ä»¶: {len(papers)} ä¸ª")
        
        # ç”Ÿæˆå…¬å¼€è®¿é—®é“¾æ¥
        public_url = await minio.get_public_url("papers/arxiv_2024_001.md")
        print(f"ğŸŒ å…¬å¼€é“¾æ¥: {public_url}")
        
        # ç”Ÿæˆé¢„ç­¾åä¸´æ—¶é“¾æ¥ï¼ˆ1å°æ—¶æœ‰æ•ˆï¼‰
        temp_url = await minio.get_presigned_url(
            "papers/arxiv_2024_001.md", 
            expires=3600
        )
        print(f"ğŸ”— ä¸´æ—¶é“¾æ¥: {temp_url}")
```

### PostgreSQLæ•°æ®æŸ¥è¯¢
```python
from src.storage.postgres_manager import PostgresManager

async def query_postgres():
    postgres = PostgresManager(config['postgres'])
    
    # æŸ¥è¯¢ç‰¹å®šä½œè€…çš„è®ºæ–‡
    papers = await postgres.get_papers_by_author("å¼ ä¸‰")
    print(f"ğŸ‘¨â€ğŸ“ ä½œè€…å¼ ä¸‰çš„è®ºæ–‡: {len(papers)} ç¯‡")
    
    # æŸ¥è¯¢æœ€è¿‘çš„è®ºæ–‡
    recent_papers = await postgres.get_recent_papers(days=7)
    print(f"ğŸ“… æœ€è¿‘7å¤©çš„è®ºæ–‡: {len(recent_papers)} ç¯‡")
    
    # æŒ‰çŠ¶æ€æŸ¥è¯¢
    completed_papers = await postgres.get_papers_by_status("completed")
    print(f"âœ… å·²å®Œæˆçš„è®ºæ–‡: {len(completed_papers)} ç¯‡")
```

### Elasticsearchå…¨æ–‡æœç´¢
```python
from src.storage.elasticsearch_manager import ElasticsearchManager

async def search_elasticsearch():
    es = ElasticsearchManager(config['elasticsearch'])
    
    # å…¨æ–‡æœç´¢
    results = await es.search("æœºå™¨å­¦ä¹ ", size=10)
    print(f"ğŸ” æœç´¢åˆ° {results['hits']['total']['value']} ç¯‡ç›¸å…³è®ºæ–‡")
    
    # é«˜çº§æœç´¢
    advanced_results = await es.advanced_search({
        "query": {
            "bool": {
                "must": [
                    {"match": {"title": "æ·±åº¦å­¦ä¹ "}},
                    {"range": {"created_at": {"gte": "2024-01-01"}}}
                ]
            }
        }
    })
    print(f"ğŸ¯ é«˜çº§æœç´¢ç»“æœ: {len(advanced_results['hits']['hits'])} ç¯‡")
```

### ç›´æ¥URLè®¿é—®ç¤ºä¾‹
ä¸Šä¼ å®Œæˆåï¼Œä½ çš„æ•°æ®å¯ä»¥é€šè¿‡ä»¥ä¸‹URLè®¿é—®:

```
# MinIOå…¬å¼€è®¿é—®
http://localhost:9000/arxiv-papers/articles/item_1/metadata.json
http://localhost:9000/arxiv-papers/articles/item_1/content.md
http://localhost:9000/arxiv-papers/data/items_metadata.json

# Elasticsearchæœç´¢API
http://localhost:9200/arxiv_papers/_search?q=title:æœºå™¨å­¦ä¹ 

# PostgreSQLé€šè¿‡APIè®¿é—®ï¼ˆéœ€è¦å®ç°REST APIï¼‰
http://localhost:8000/api/papers?author=å¼ ä¸‰
http://localhost:8000/api/papers/recent?days=7
```

## ğŸ”§ å¸¸ç”¨é…ç½®

### å®Œæ•´ä¸‰å±‚å­˜å‚¨éƒ¨ç½²
```bash
# ä½¿ç”¨Docker Composeä¸€é”®éƒ¨ç½²
cd your_project
wget https://raw.githubusercontent.com/your-repo/docker-compose.yml
docker-compose up -d

# éªŒè¯æœåŠ¡çŠ¶æ€
docker-compose ps
```

### MinIOæœ¬åœ°éƒ¨ç½²ï¼ˆå•ç‹¬ï¼‰
```bash
# å¯åŠ¨MinIOæœåŠ¡
docker run -d --name minio \
  -p 9000:9000 -p 9001:9001 \
  -e "MINIO_ROOT_USER=minioadmin" \
  -e "MINIO_ROOT_PASSWORD=minioadmin" \
  -v minio_data:/data \
  minio/minio server /data --console-address ":9001"

# è®¿é—®ç®¡ç†ç•Œé¢: http://localhost:9001
```

### PostgreSQLé…ç½®
```bash
# å¯åŠ¨PostgreSQL
docker run -d --name postgres \
  -p 5432:5432 \
  -e POSTGRES_DB=arxiv_db \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=password \
  -v postgres_data:/var/lib/postgresql/data \
  postgres:15

# åˆ›å»ºæ•°æ®è¡¨
psql -h localhost -U postgres -d arxiv_db -f schema.sql
```

### Elasticsearché…ç½®
```bash
# å¯åŠ¨Elasticsearch
docker run -d --name elasticsearch \
  -p 9200:9200 \
  -e "discovery.type=single-node" \
  -e "xpack.security.enabled=false" \
  -v es_data:/usr/share/elasticsearch/data \
  elasticsearch:8.8.0

# åˆ›å»ºç´¢å¼•
curl -X PUT "localhost:9200/arxiv_papers" -H 'Content-Type: application/json' -d'
{
  "mappings": {
    "properties": {
      "title": {"type": "text", "analyzer": "standard"},
      "content": {"type": "text", "analyzer": "standard"},
      "authors": {"type": "keyword"},
      "created_at": {"type": "date"}
    }
  }
}'
```

### ç”Ÿäº§ç¯å¢ƒé…ç½®
```json
{
  "minio": {
    "endpoint": "minio.your-domain.com:9000",
    "access_key": "${MINIO_ACCESS_KEY}",
    "secret_key": "${MINIO_SECRET_KEY}",
    "bucket_name": "arxiv-papers-prod",
    "secure": true
  },
  "postgres": {
    "host": "postgres.your-domain.com",
    "port": 5432,
    "database": "arxiv_prod",
    "user": "${POSTGRES_USER}",
    "password": "${POSTGRES_PASSWORD}",
    "ssl_mode": "require"
  },
  "elasticsearch": {
    "host": "elasticsearch.your-domain.com",
    "port": 9200,
    "index": "arxiv_papers_prod",
    "auth": {
      "username": "${ES_USERNAME}",
      "password": "${ES_PASSWORD}"
    }
  }
}
```

### é˜¿é‡Œäº‘OSSé…ç½®ï¼ˆå…¼å®¹æ€§æ”¯æŒï¼‰
```json
{
  "oss": {
    "provider": "aliyun",
    "endpoint": "oss-cn-beijing.aliyuncs.com",
    "access_key_id": "${ALIYUN_ACCESS_KEY}",
    "access_key_secret": "${ALIYUN_SECRET_KEY}",
    "bucket_name": "your-bucket-name",
    "public_url_base": "https://your-bucket-name.oss-cn-beijing.aliyuncs.com"
  }
}
```

### Docker Composeéƒ¨ç½²ï¼ˆæ¨èï¼‰
```yaml
version: '3.8'
services:
  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
  
  postgres:
    image: postgres:15
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: arxiv_db
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
  
  elasticsearch:
    image: elasticsearch:8.8.0
    ports:
      - "9200:9200"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - es_data:/usr/share/elasticsearch/data

volumes:
  minio_data:
  postgres_data:
  es_data:
```

## ğŸš¨ æ³¨æ„äº‹é¡¹

### ä¸‰å±‚å­˜å‚¨ä¸€è‡´æ€§
- **æ•°æ®åŒæ­¥**: ç¡®ä¿æœ¬åœ°ã€MinIOã€PostgreSQLã€Elasticsearchå››å±‚æ•°æ®ä¸€è‡´
- **äº‹åŠ¡å¤„ç†**: ä½¿ç”¨äº‹åŠ¡ç¡®ä¿å¤šå±‚å­˜å‚¨çš„åŸå­æ€§æ“ä½œ
- **é”™è¯¯å›æ»š**: ä»»ä¸€å±‚å­˜å‚¨å¤±è´¥æ—¶ï¼Œå›æ»šå·²å®Œæˆçš„æ“ä½œ

### IDå”¯ä¸€æ€§ï¼ˆé‡è¦ï¼ï¼‰
- ç¡®ä¿æ¯ä¸ªçˆ¬å–é¡¹ç›®çš„IDåœ¨æ‰€æœ‰å­˜å‚¨å±‚å…¨å±€å”¯ä¸€
- æ¨èæ ¼å¼: `{source}_{timestamp}_{hash}`
- ç¤ºä¾‹: `arxiv_20240129_a1b2c3d4`

### æ–‡ä»¶åå®‰å…¨
- é¿å…ä½¿ç”¨ç‰¹æ®Šå­—ç¬¦: `/ \ : * ? " < > |`
- MinIOå¯¹è±¡åæ¨èæ ¼å¼: `category/subcategory/id.extension`
- ç¤ºä¾‹: `papers/arxiv/arxiv_2024_001.md`

### å¹¶å‘æ§åˆ¶ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰
```python
# æ¨èå¹¶å‘é…ç½®
CONCURRENCY_CONFIG = {
    'max_concurrent_crawls': 10,
    'max_concurrent_uploads': 5,
    'max_db_connections': 20,
    'elasticsearch_bulk_size': 100
}
```

### é”™è¯¯å¤„ç†å’Œé‡è¯•
```python
# å®ç°æŒ‡æ•°é€€é¿é‡è¯•
import asyncio
from functools import wraps

def retry_with_backoff(max_retries=3, base_delay=1):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    delay = base_delay * (2 ** attempt)
                    await asyncio.sleep(delay)
            return wrapper
    return decorator
```

### æ•°æ®å¤‡ä»½ç­–ç•¥
- **MinIOå¤‡ä»½**: é…ç½®è·¨åŒºåŸŸå¤åˆ¶æˆ–å®šæœŸå¯¼å‡º
- **PostgreSQLå¤‡ä»½**: ä½¿ç”¨pg_dumpå®šæœŸå¤‡ä»½
- **Elasticsearchå¤‡ä»½**: ä½¿ç”¨å¿«ç…§åŠŸèƒ½
- **æœ¬åœ°å¤‡ä»½**: å®šæœŸå‹ç¼©å¹¶ä¸Šä¼ åˆ°äº‘å­˜å‚¨

### ç›‘æ§å’Œå‘Šè­¦
```python
# å¥åº·æ£€æŸ¥ç¤ºä¾‹
async def health_check():
    checks = {
        'minio': await minio.health_check(),
        'postgres': await postgres.health_check(),
        'elasticsearch': await elasticsearch.health_check()
    }
    
    failed = [k for k, v in checks.items() if not v]
    if failed:
        logger.error(f"âŒ å­˜å‚¨æœåŠ¡å¼‚å¸¸: {failed}")
        # å‘é€å‘Šè­¦é€šçŸ¥
    else:
        logger.info("âœ… æ‰€æœ‰å­˜å‚¨æœåŠ¡æ­£å¸¸")
```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®
- **æ‰¹é‡æ“ä½œ**: ä½¿ç”¨æ‰¹é‡æ’å…¥å’Œæ‰¹é‡ç´¢å¼•
- **è¿æ¥æ± **: é…ç½®åˆé€‚çš„æ•°æ®åº“è¿æ¥æ± å¤§å°
- **ç¼“å­˜ç­–ç•¥**: å¯¹é¢‘ç¹æŸ¥è¯¢çš„æ•°æ®è¿›è¡Œç¼“å­˜
- **ç´¢å¼•ä¼˜åŒ–**: ä¸ºå¸¸ç”¨æŸ¥è¯¢å­—æ®µåˆ›å»ºç´¢å¼•

### å®‰å…¨æ³¨æ„äº‹é¡¹
- **æ•æ„Ÿä¿¡æ¯**: ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨å¯†é’¥ï¼Œä¸è¦ç¡¬ç¼–ç 
- **ç½‘ç»œå®‰å…¨**: ç”Ÿäº§ç¯å¢ƒå¯ç”¨SSL/TLS
- **è®¿é—®æ§åˆ¶**: é…ç½®é€‚å½“çš„ç”¨æˆ·æƒé™å’Œè®¿é—®ç­–ç•¥
- **æ•°æ®åŠ å¯†**: æ•æ„Ÿæ•°æ®å­˜å‚¨æ—¶è¿›è¡ŒåŠ å¯†

## ğŸ“ å¿«é€Ÿé—®é¢˜è§£å†³

### å¸¸è§é—®é¢˜

**Q: MinIOè¿æ¥å¤±è´¥?**
A: æ£€æŸ¥endpointã€access_keyã€secret_keyé…ç½®ï¼Œç¡®ä¿MinIOæœåŠ¡æ­£åœ¨è¿è¡Œ

**Q: PostgreSQLè¿æ¥è¶…æ—¶?**
A: æ£€æŸ¥æ•°æ®åº“æœåŠ¡çŠ¶æ€ï¼Œè°ƒæ•´è¿æ¥æ± é…ç½®å’Œè¶…æ—¶è®¾ç½®

**Q: Elasticsearchç´¢å¼•å¤±è´¥?**
A: æ£€æŸ¥ç´¢å¼•æ˜ å°„é…ç½®ï¼Œç¡®ä¿å­—æ®µç±»å‹åŒ¹é…

**Q: ä¸‰å±‚å­˜å‚¨æ•°æ®ä¸ä¸€è‡´?**
A: å®ç°äº‹åŠ¡å›æ»šæœºåˆ¶ï¼Œä½¿ç”¨å¥åº·æ£€æŸ¥ç›‘æ§å„å±‚çŠ¶æ€

**Q: ä¸Šä¼ é€Ÿåº¦æ…¢?**
A: è°ƒæ•´concurrentå‚æ•°ï¼Œä¼˜åŒ–ç½‘ç»œé…ç½®ï¼Œä½¿ç”¨æ‰¹é‡æ“ä½œ

**Q: å†…å­˜å ç”¨è¿‡é«˜?**
A: å‡å°‘å¹¶å‘æ•°ï¼Œä½¿ç”¨æµå¼å¤„ç†ï¼ŒåŠæ—¶é‡Šæ”¾èµ„æº

---

## ğŸ‰ æ€»ç»“

è¿™ä¸ªå¿«é€Ÿé›†æˆæŒ‡å—åŸºäº**arXivçˆ¬è™«çš„ä¼ä¸šçº§ä¸‰å±‚å­˜å‚¨æ¶æ„**ï¼Œæä¾›äº†ï¼š

âœ… **æ ‡å‡†åŒ–å­˜å‚¨æ–¹æ¡ˆ**: MinIO + PostgreSQL + Elasticsearch  
âœ… **ç”Ÿäº§å°±ç»ªé…ç½®**: Docker Composeä¸€é”®éƒ¨ç½²  
âœ… **å®Œæ•´ä»£ç ç¤ºä¾‹**: ä»çˆ¬å–åˆ°å­˜å‚¨çš„å…¨æµç¨‹  
âœ… **å¤šåœºæ™¯é€‚é…**: æ–°é—»ã€ç¤¾äº¤åª’ä½“ã€ç”µå•†ç­‰  
âœ… **æœ€ä½³å®è·µ**: é”™è¯¯å¤„ç†ã€ç›‘æ§ã€å®‰å…¨ç­‰  

**çœŸç‰›é€¼ï¼** æœ‰äº†è¿™å¥—æ¶æ„ï¼Œä½ çš„çˆ¬è™«é¡¹ç›®å°±èƒ½å¿«é€Ÿå…·å¤‡ä¼ä¸šçº§çš„å­˜å‚¨èƒ½åŠ›ï¼ğŸš€