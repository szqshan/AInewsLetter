#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»æ§åˆ¶è„šæœ¬
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰çˆ¬è™«çš„æ‰§è¡Œ
"""

import os
import sys
import argparse
import asyncio
import subprocess
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from shared.config import PLATFORM_CONFIGS, ensure_directories
from shared.utils import save_json, generate_filename
from shared.quality_scorer import QualityScorer

class SpiderManager:
    """
    çˆ¬è™«ç®¡ç†å™¨
    ç»Ÿä¸€ç®¡ç†æ‰€æœ‰å¹³å°çš„çˆ¬è™«
    """
    
    def __init__(self):
        self.quality_scorer = QualityScorer()
        self.results = {
            "academic_papers": [],
            "ai_news": [],
            "ai_tools": []
        }
        
        # ç¡®ä¿ç›®å½•ç»“æ„å­˜åœ¨
        ensure_directories()
    
    async def run_all_spiders(self, categories: List[str] = None):
        """
        è¿è¡Œæ‰€æœ‰çˆ¬è™«
        
        Args:
            categories: è¦è¿è¡Œçš„åˆ†ç±»åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè¿è¡Œæ‰€æœ‰
        """
        print("Starting AI Content Aggregation Spider System!")
        print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        if not categories:
            categories = ["academic_papers", "ai_news", "ai_tools"]
        
        tasks = []
        
        if "academic_papers" in categories:
            tasks.append(self._run_academic_spiders())
        
        if "ai_news" in categories:
            tasks.append(self._run_news_spiders())
        
        if "ai_tools" in categories:
            tasks.append(self._run_tools_spiders())
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰çˆ¬è™«
        await asyncio.gather(*tasks)
        
        # å¤„ç†å’Œä¿å­˜ç»“æœ
        await self._process_results()
        
        print("\nAll spider tasks completed!")
        print(f"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    async def _run_academic_spiders(self):
        """
        è¿è¡Œå­¦æœ¯è®ºæ–‡çˆ¬è™«
        """
        print("\nStarting academic papers crawling...")
        
        # arXivçˆ¬è™«
        try:
            sys.path.append(os.path.join(project_root, 'academic_papers', 'arxiv'))
            import subprocess
            result = subprocess.run([sys.executable, os.path.join(project_root, 'academic_papers', 'arxiv', 'spider.py')], 
                                  capture_output=True, text=True, cwd=project_root)
            if result.returncode == 0:
                print("Success: arXiv crawler completed")
            else:
                print(f"Error: arXiv crawler failed: {result.stderr}")
        except Exception as e:
            print(f"Error: arXiv crawler failed: {e}")
        
        # Google Scholarçˆ¬è™«
        try:
            result = subprocess.run([sys.executable, os.path.join(project_root, 'academic_papers', 'google_scholar', 'spider.py')], 
                                  capture_output=True, text=True, cwd=project_root)
            if result.returncode == 0:
                print("Success: Google Scholar crawler completed")
            else:
                print(f"Error: Google Scholar crawler failed: {result.stderr}")
        except Exception as e:
            print(f"Error: Google Scholar crawler failed: {e}")
        
        # Papers with Codeçˆ¬è™«
        try:
            from academic_papers.papers_with_code.spider import PapersWithCodeSpider
            pwc_spider = PapersWithCodeSpider()
            pwc_results = await pwc_spider.crawl()
            self.results["academic_papers"].extend(pwc_results)
            print(f"Success: Papers with Code: Got {len(pwc_results)} papers")
        except ImportError:
            print("Warning: Papers with Code crawler module not found, skipping")
        except Exception as e:
            print(f"Error: Papers with Code crawler failed: {e}")
        
        # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–å­¦æœ¯å¹³å°çˆ¬è™«...
    
    async def _run_news_spiders(self):
        """
        è¿è¡ŒAIæ–°é—»çˆ¬è™«
        """
        print("\nStarting AI news crawling...")
        
        # Hugging Face Daily Papers
        try:
            result = subprocess.run([sys.executable, os.path.join(project_root, 'ai_news', 'huggingface_daily', 'spider.py')], 
                                  capture_output=True, text=True, cwd=project_root)
            if result.returncode == 0:
                print("Success: Hugging Face crawler completed")
            else:
                print(f"Error: Hugging Face crawler failed: {result.stderr}")
        except Exception as e:
            print(f"Error: Hugging Face crawler failed: {e}")
        
        # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–æ–°é—»å¹³å°çˆ¬è™«...
    
    async def _run_tools_spiders(self):
        """
        è¿è¡ŒAIå·¥å…·çˆ¬è™«
        """
        print("\nStarting AI tools crawling...")
        
        # GitHub Trending
        try:
            result = subprocess.run([sys.executable, os.path.join(project_root, 'ai_tools', 'github_trending', 'spider.py')], 
                                  capture_output=True, text=True, cwd=project_root)
            if result.returncode == 0:
                print("Success: GitHub Trending crawler completed")
            else:
                print(f"Error: GitHub Trending crawler failed: {result.stderr}")
        except Exception as e:
            print(f"Error: GitHub Trending crawler failed: {e}")
        
        # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–å·¥å…·å¹³å°çˆ¬è™«...
    
    async def _process_results(self):
        """
        å¤„ç†å’Œä¿å­˜çˆ¬å–ç»“æœ
        """
        print("\nStarting to process crawling results...")
        
        # è´¨é‡è¯„ä¼°å’Œæ’åº
        for category, items in self.results.items():
            if not items:
                continue
            
            print(f"\nEvaluating quality for {category}...")
            
            for item in items:
                if category == "academic_papers":
                    scores = self.quality_scorer.score_paper(item)
                elif category == "ai_news":
                    scores = self.quality_scorer.score_news(item)
                elif category == "ai_tools":
                    scores = self.quality_scorer.score_tool(item)
                
                item["quality_scores"] = scores
            
            # æŒ‰è´¨é‡åˆ†æ•°æ’åº
            items.sort(key=lambda x: x["quality_scores"]["total_score"], reverse=True)
            
            print(f"Success: {category}: Evaluated {len(items)} items")
        
        # ä¿å­˜ç»“æœ
        await self._save_results()
    
    async def _save_results(self):
        """
        ä¿å­˜çˆ¬å–ç»“æœåˆ°æ–‡ä»¶
        """
        print("\nSaving results to files...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for category, items in self.results.items():
            if not items:
                continue
            
            # ä¿å­˜JSONæ ¼å¼
            json_filename = f"{category}_{timestamp}.json"
            json_filepath = project_root / "data" / "processed" / json_filename
            save_json(items, str(json_filepath))
            
            # ä¿å­˜Markdownæ ¼å¼
            md_content = self._generate_markdown_report(category, items)
            md_filename = f"{category}_{timestamp}.md"
            md_filepath = project_root / "data" / "exports" / md_filename
            
            with open(md_filepath, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            print(f"Success: {category}: JSON and Markdown files saved")
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary_report = self._generate_summary_report()
        summary_filepath = project_root / "data" / "exports" / f"summary_report_{timestamp}.md"
        
        with open(summary_filepath, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        print(f"Success: Summary report saved: {summary_filepath}")
    
    def _generate_markdown_report(self, category: str, items: List[Dict]) -> str:
        """
        ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š
        
        Args:
            category: åˆ†ç±»åç§°
            items: é¡¹ç›®åˆ—è¡¨
        
        Returns:
            Markdownå†…å®¹
        """
        title_map = {
            "academic_papers": "Academic Papers Report",
            "ai_news": "AI News Report",
            "ai_tools": "AI Tools Report"
        }
        
        md_content = f"# {title_map.get(category, category)}\n\n"
        md_content += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        md_content += f"æ€»è®¡: {len(items)} ä¸ªé¡¹ç›®\n\n"
        
        # æŒ‰è´¨é‡ç­‰çº§åˆ†ç»„
        quality_groups = {}
        for item in items:
            level = item["quality_scores"]["quality_level"]
            if level not in quality_groups:
                quality_groups[level] = []
            quality_groups[level].append(item)
        
        for level in ["ä¼˜ç§€", "è‰¯å¥½", "ä¸€èˆ¬", "è¾ƒå·®", "å¾ˆå·®"]:
            if level not in quality_groups:
                continue
            
            md_content += f"## {level} ({len(quality_groups[level])} ä¸ª)\n\n"
            
            for i, item in enumerate(quality_groups[level], 1):
                md_content += f"### {i}. {item.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n\n"
                md_content += f"**è´¨é‡åˆ†æ•°**: {item['quality_scores']['total_score']}/10\n\n"
                
                if category == "academic_papers":
                    md_content += f"**ä½œè€…**: {', '.join(item.get('authors', []))}\n\n"
                    md_content += f"**å‘å¸ƒæ—¥æœŸ**: {item.get('published', 'æœªçŸ¥')}\n\n"
                    md_content += f"**é“¾æ¥**: [{item.get('id', '')}]({item.get('link', '')})\n\n"
                    md_content += f"**æ‘˜è¦**: {item.get('summary', '')[:200]}...\n\n"
                
                elif category == "ai_news":
                    md_content += f"**æ¥æº**: {item.get('source', 'æœªçŸ¥')}\n\n"
                    md_content += f"**å‘å¸ƒæ—¥æœŸ**: {item.get('published', 'æœªçŸ¥')}\n\n"
                    md_content += f"**é“¾æ¥**: [æŸ¥çœ‹åŸæ–‡]({item.get('link', '')})\n\n"
                
                elif category == "ai_tools":
                    md_content += f"**æè¿°**: {item.get('description', '')}\n\n"
                    md_content += f"**GitHub**: [æŸ¥çœ‹é¡¹ç›®]({item.get('github_url', '')})\n\n"
                    popularity = item.get('popularity', {})
                    md_content += f"**Stars**: {popularity.get('github_stars', 0)}\n\n"
                
                md_content += "---\n\n"
        
        return md_content
    
    def _generate_summary_report(self) -> str:
        """
        ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        
        Returns:
            æ±‡æ€»æŠ¥å‘Šå†…å®¹
        """
        md_content = "# ğŸ¤– AIå†…å®¹èšåˆæŠ¥å‘Š\n\n"
        md_content += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_papers = len(self.results["academic_papers"])
        total_news = len(self.results["ai_news"])
        total_tools = len(self.results["ai_tools"])
        
        md_content += "## Statistical Overview\n\n"
        md_content += f"- Academic Papers: {total_papers} items\n"
        md_content += f"- AI News: {total_news} items\n"
        md_content += f"- AI Tools: {total_tools} items\n\n"
        
        # è´¨é‡åˆ†å¸ƒ
        md_content += "## ğŸ† è´¨é‡åˆ†å¸ƒ\n\n"
        
        for category, items in self.results.items():
            if not items:
                continue
            
            quality_dist = {}
            for item in items:
                level = item["quality_scores"]["quality_level"]
                quality_dist[level] = quality_dist.get(level, 0) + 1
            
            category_name = {
                "academic_papers": "å­¦æœ¯è®ºæ–‡",
                "ai_news": "AIæ–°é—»",
                "ai_tools": "AIå·¥å…·"
            }[category]
            
            md_content += f"### {category_name}\n\n"
            for level in ["ä¼˜ç§€", "è‰¯å¥½", "ä¸€èˆ¬", "è¾ƒå·®", "å¾ˆå·®"]:
                count = quality_dist.get(level, 0)
                if count > 0:
                    md_content += f"- {level}: {count} ä¸ª\n"
            md_content += "\n"
        
        # æ¨èå†…å®¹
        md_content += "## â­ ä»Šæ—¥æ¨è\n\n"
        
        all_items = []
        for category, items in self.results.items():
            for item in items:
                item["category"] = category
                all_items.append(item)
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åºï¼Œå–å‰10
        top_items = sorted(all_items, key=lambda x: x["quality_scores"]["total_score"], reverse=True)[:10]
        
        for i, item in enumerate(top_items, 1):
            category_emoji = {
                "academic_papers": "[PAPER]",
                "ai_news": "[NEWS]",
                "ai_tools": "[TOOL]"
            }
            
            emoji = category_emoji.get(item["category"], "[ITEM]")
            md_content += f"### {i}. {emoji} {item.get('title', 'æœªçŸ¥æ ‡é¢˜')}\n\n"
            md_content += f"**è´¨é‡åˆ†æ•°**: {item['quality_scores']['total_score']}/10\n\n"
            md_content += f"**åˆ†ç±»**: {item['category']}\n\n"
            
            if item.get('link'):
                md_content += f"**é“¾æ¥**: [æŸ¥çœ‹è¯¦æƒ…]({item['link']})\n\n"
            
            md_content += "---\n\n"
        
        return md_content

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description="AIå†…å®¹èšåˆçˆ¬è™«ç³»ç»Ÿ")
    parser.add_argument(
        "--categories",
        nargs="*",
        choices=["academic_papers", "ai_news", "ai_tools"],
        help="è¦è¿è¡Œçš„çˆ¬è™«åˆ†ç±»"
    )
    parser.add_argument(
        "--config",
        type=str,
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºçˆ¬è™«ç®¡ç†å™¨
    manager = SpiderManager()
    
    # è¿è¡Œçˆ¬è™«
    asyncio.run(manager.run_all_spiders(args.categories))

if __name__ == "__main__":
    main()
