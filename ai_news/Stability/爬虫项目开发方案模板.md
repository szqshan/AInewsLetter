# ğŸ•·ï¸ ç½‘ç«™çˆ¬è™«é¡¹ç›®å¼€å‘æ–¹æ¡ˆæ¨¡æ¿

æ³¨æ„ï¼æ•°æ®æå–ç­–ç•¥å¿…é¡»ä¸ç”¨æˆ·ç¡®è®¤åï¼Œå¹¶ä¸”æ–¹æ¡ˆå–å¾—ç”¨æˆ·åŒæ„åå†å¼€å§‹æ ¸å¿ƒä»£ç çš„å¼€å‘ã€‚

## ğŸ¯ ç¬¬ä¸€é˜¶æ®µï¼šéœ€æ±‚åˆ†æä¸æ–¹æ¡ˆåˆ¶å®š

### 1.1 éœ€æ±‚æ¾„æ¸…
- **ç›®æ ‡ç½‘ç«™**: {ç½‘ç«™URL}
- **æ•°æ®ç±»å‹**: {æ–°é—»/æ–‡ç« /äº§å“/æ•°æ®ç­‰}
- **æ ¸å¿ƒåŠŸèƒ½**: 
  - [ ] æ•°æ®çˆ¬å–
  - [ ] åª’ä½“æ–‡ä»¶ä¸‹è½½
  - [ ] æœ¬åœ°å­˜å‚¨
  - [ ] å®šæ—¶è°ƒåº¦
- **æ•°æ®é‡çº§**: {é¢„ä¼°æ•°æ®é‡}
- **æ›´æ–°é¢‘ç‡**: {æ¯æ—¥/æ¯å‘¨/å®æ—¶}

### 1.2 æŠ€æœ¯é€‰å‹å†³ç­–
```yaml
çˆ¬è™«æŠ€æœ¯:
  - HTMLè§£æ: BeautifulSoup4 + requests (ç¨³å®šæ€§ä¼˜å…ˆ)
  - JSæ¸²æŸ“: Playwright (å¤æ‚é¡µé¢)
  - å¼‚æ­¥å¤„ç†: asyncio (é«˜æ€§èƒ½éœ€æ±‚)

æ•°æ®å­˜å‚¨:
  - æœ¬åœ°å­˜å‚¨: æ ‡å‡†åŒ–ç›®å½•ç»“æ„

å®šæ—¶è°ƒåº¦:
  - Linux: Cron
  - Windows: ä»»åŠ¡è®¡åˆ’ç¨‹åº
  - é«˜çº§: APScheduler
```

### 1.3 ç›®å½•ç»“æ„è®¾è®¡ï¼Œcrawled_dataç›®å½•ä¸‹çš„ç›®å½•ç»“æ„å¿…é¡»ä¸¥æ ¼éµå¾ªï¼Œå…¶ä»–ç›®å½•ç»“æ„å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè®¾è®¡ã€‚


```
{é¡¹ç›®å}/
â”œâ”€â”€ ğŸ“„ spider.py                    # ä¸»çˆ¬è™«è„šæœ¬
â”œâ”€â”€ ğŸ“„ run_crawler.py               # ä¸€é”®è¿è¡Œè„šæœ¬
â”œâ”€â”€ ğŸ“„ config.json                  # é…ç½®æ–‡ä»¶
â”œâ”€â”€ ğŸ“„ requirements.txt             # ä¾èµ–åŒ…
â”œâ”€â”€ ğŸ“„ README.md                    # è¯´æ˜æ–‡æ¡£
â””â”€â”€ ğŸ“Š crawled_data/                # æœ¬åœ°æ•°æ®å­˜å‚¨
    â””â”€â”€ {æ•°æ®ç±»å‹}/                 # æ•°æ®åˆ†ç±»ç›®å½•
        â””â”€â”€ {å”¯ä¸€æ ‡è¯†}/             # æŒ‰ID/slugç»„ç»‡
            â”œâ”€â”€ content.md          # Markdownå†…å®¹
            â”œâ”€â”€ metadata.json       # å…ƒæ•°æ®JSON
            â””â”€â”€ media/              # åª’ä½“æ–‡ä»¶ç›®å½•
```

---

## ğŸ” ç¬¬äºŒé˜¶æ®µï¼šç›®æ ‡ç½‘ç«™åˆ†æ

### 2.1 é¡µé¢ç»“æ„åˆ†æå·¥å…·

#### 2.1.1 ä½¿ç”¨Playwrightåˆ†æé¡µé¢
```python
# 1. å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
browser.navigate(target_url)

# 2. è·å–é¡µé¢å¿«ç…§
browser.snapshot()

# 3. åˆ†æé¡µé¢å…ƒç´ 
browser.evaluate("é¡µé¢åˆ†æJavaScript")

# 4. æ£€æŸ¥ç½‘ç»œè¯·æ±‚
browser.network_requests()
```

#### 2.1.2 å…³é”®ä¿¡æ¯æå–
```yaml
é¡µé¢æ¶æ„åˆ†æ:
  - åˆ—è¡¨é¡µé¢: {URLæ¨¡å¼å’Œé€‰æ‹©å™¨}
  - è¯¦æƒ…é¡µé¢: {URLæ¨¡å¼å’Œå†…å®¹ç»“æ„}
  - åˆ†é¡µæœºåˆ¶: {ç¿»é¡µæ–¹å¼}
  - åª’ä½“æ–‡ä»¶: {å›¾ç‰‡/è§†é¢‘ç­‰èµ„æº}

æŠ€æœ¯æ¶æ„è¯†åˆ«:
  - å‰ç«¯æ¡†æ¶: {React/Vue/åŸç”Ÿç­‰}
  - æ¸²æŸ“æ–¹å¼: {SSR/CSR/é™æ€}
  - APIæ¥å£: {REST/GraphQL/æ— }
  - åçˆ¬æªæ–½: {éªŒè¯ç /é™é¢‘/JSæ£€æµ‹}
```
#### 2.1.3 ä½¿ç”¨playwrightæŸ¥çœ‹ä½¿ç”¨èƒ½è·å–åˆ°åç«¯çš„APIæ¥å£ï¼Œç›´æ¥è°ƒç”¨æ¥å£è·å–æ•°æ®


### 2.2 æ•°æ®æå–ç­–ç•¥
```yaml
CSSé€‰æ‹©å™¨è§„åˆ’:
  åˆ—è¡¨é¡µé¢:
    - æ¡ç›®é“¾æ¥: "a[href*='/article/']"
    - æ ‡é¢˜: ".title"
    - åˆ†ç±»: ".category"
    - æ—¥æœŸ: ".date"
    - æ‘˜è¦: ".summary"
  
  è¯¦æƒ…é¡µé¢:
    - æ ‡é¢˜: "h1.title"
    - æ­£æ–‡: ".content"
    - å›¾ç‰‡: "img"
    - å…ƒæ•°æ®: ".meta"
```

---

## ğŸ› ï¸ ç¬¬ä¸‰é˜¶æ®µï¼šæ ¸å¿ƒä»£ç å¼€å‘ï¼Œæ³¨æ„ï¼æ•°æ®æå–ç­–ç•¥å¿…é¡»ä¸ç”¨æˆ·ç¡®è®¤åï¼Œå¹¶ä¸”æ–¹æ¡ˆå–å¾—ç”¨æˆ·åŒæ„åå†å¼€å§‹æ ¸å¿ƒä»£ç çš„å¼€å‘ã€‚

### 3.1 çˆ¬è™«ä¸»è„šæœ¬ (spider.py)

#### 3.1.1 åŸºç¡€æ¶æ„
```python
class {WebsiteName}Spider:
    def __init__(self, config_file="config.json"):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.base_url = "{ç½‘ç«™åŸºç¡€URL}"
        self.target_url = "{ç›®æ ‡é¡µé¢URL}"
        self.session = requests.Session()
        self.config = self.load_config(config_file)
        self.setup_directories()
        
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        
    def get_article_list(self):
        """è·å–æ–‡ç« åˆ—è¡¨"""
        
    def crawl_article(self, article_url):
        """çˆ¬å–å•ç¯‡æ–‡ç« """
        
    def download_media(self, media_url, save_path):
        """ä¸‹è½½åª’ä½“æ–‡ä»¶"""
        
    def save_article(self, data, article_id):
        """ä¿å­˜æ–‡ç« æ•°æ®"""
        
    def run(self, max_articles=None):
        """æ‰§è¡Œçˆ¬è™«"""
```

#### 3.1.2 æ ¸å¿ƒåŠŸèƒ½å®ç°
```python
# æ•°æ®æå–
def extract_data(self, soup, url):
    """æå–é¡µé¢æ•°æ®"""
    return {
        'title': soup.select_one('h1').get_text().strip(),
        'content': self.clean_content(soup.select_one('.content')),
        'date': self.parse_date(soup.select_one('.date')),
        'category': soup.select_one('.category').get_text().strip(),
        'images': self.extract_images(soup),
        'url': url
    }

# å¢é‡æ›´æ–°æ£€æŸ¥
def is_article_exists(self, article_id):
    """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨"""
    return Path(f"crawled_data/{data_type}/{article_id}").exists()

# é”™è¯¯å¤„ç†å’Œé‡è¯•
def safe_request(self, url, max_retries=3):
    """å®‰å…¨çš„HTTPè¯·æ±‚"""
```


### 3.3 é…ç½®æ–‡ä»¶ (config.json)
```json
{
  "crawler": {
    "delay": 2,
    "timeout": 30,
    "max_retries": 3,
    "max_articles": 50,
    "user_agent": "æ ‡å‡†UserAgent"
  },
  "media": {
    "download_images": true,
    "download_videos": false,
    "image_timeout": 15,
    "max_file_size": 10485760,
    "allowed_extensions": [".jpg", ".png", ".gif"]
  },
  "storage": {
    "data_dir": "crawled_data",
    "data_type": "{æ•°æ®ç±»å‹åç§°}",
    "create_subdirs": true
  },
  "filter": {
    "skip_duplicates": true,
    "min_content_length": 100,
    "categories": []
  },
  "minio": {
    "bucket_name": "{é¡¹ç›®åç§°}",
    "api_endpoint": "localhost:9011"
  }
}
```

---

## ğŸ“Š ç¬¬å››é˜¶æ®µï¼šæ•°æ®æ ¼å¼æ ‡å‡†åŒ–

### 4.1 Markdownå†…å®¹æ ¼å¼ (content.md)
```markdown
# {æ–‡ç« æ ‡é¢˜}

**åˆ†ç±»**: {åˆ†ç±»}  
**å‘å¸ƒæ—¥æœŸ**: {æ—¥æœŸ}  
**æ¥æº**: {åŸå§‹URL}  
**çˆ¬å–æ—¶é—´**: {æ—¶é—´æˆ³}

---

{æ­£æ–‡å†…å®¹}

## ç›¸å…³åª’ä½“

![{æè¿°}](media/{æ–‡ä»¶å})
```

### 4.2 å…ƒæ•°æ®æ ¼å¼ (metadata.json)
```json
{
  "url": "åŸå§‹URL",
  "title": "æ–‡ç« æ ‡é¢˜",
  "category": "åˆ†ç±»",
  "date": "å‘å¸ƒæ—¥æœŸ",
  "content": "æ­£æ–‡å†…å®¹",
  "images": [
    {
      "url": "åŸå§‹å›¾ç‰‡URL",
      "alt": "å›¾ç‰‡æè¿°",
      "filename": "æœ¬åœ°æ–‡ä»¶å",
      "local_path": "æœ¬åœ°è·¯å¾„"
    }
  ],
  "crawl_time": "çˆ¬å–æ—¶é—´æˆ³",
  "slug": "æ–‡ç« æ ‡è¯†ç¬¦",
  "word_count": å­—æ•°ç»Ÿè®¡,
  "content_hash": "å†…å®¹å“ˆå¸Œ"
}
```

---

## ğŸš€ ç¬¬äº”é˜¶æ®µï¼šéƒ¨ç½²ä¸è¿ç»´

### 5.1 ä¾èµ–ç®¡ç† (requirements.txt)
```
# æ ¸å¿ƒçˆ¬è™«ä¾èµ–
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0

# æ•°æ®å¤„ç†
python-dateutil>=2.8.0

# å¯é€‰é«˜çº§åŠŸèƒ½
# playwright>=1.40.0  # å¤æ‚é¡µé¢
# aiohttp>=3.9.0      # å¼‚æ­¥è¯·æ±‚
```

### 5.2 è¿è¡Œè„šæœ¬æ¨¡æ¿

#### 5.2.1 ä¸€é”®è¿è¡Œè„šæœ¬ (run_crawler.py)
```python
#!/usr/bin/env python3
def main():
    parser = argparse.ArgumentParser(description='{é¡¹ç›®åç§°} ä¸€é”®çˆ¬è™«')
    parser.add_argument('--crawl-only', action='store_true')
    parser.add_argument('--upload-only', action='store_true')
    parser.add_argument('--max', type=int, default=20)
    parser.add_argument('--force', action='store_true')
    
    args = parser.parse_args()
    
    if not args.upload_only:
        # æ‰§è¡Œçˆ¬å–
        pass
        
    if not args.crawl_only:
        # æ‰§è¡Œä¸Šä¼ 
        pass
```


## âœ… ç¬¬å…­é˜¶æ®µï¼šæµ‹è¯•ä¸éªŒè¯

### 6.1 åŠŸèƒ½æµ‹è¯•æ¸…å•
- [ ] **åŸºç¡€çˆ¬å–**: èƒ½å¦æˆåŠŸè®¿é—®ç›®æ ‡ç½‘ç«™
- [ ] **æ•°æ®æå–**: å„å­—æ®µæ˜¯å¦æ­£ç¡®æå–
- [ ] **åª’ä½“ä¸‹è½½**: å›¾ç‰‡ç­‰æ–‡ä»¶æ˜¯å¦æ­£å¸¸ä¸‹è½½
- [ ] **å¢é‡æ›´æ–°**: é‡å¤è¿è¡Œæ˜¯å¦è·³è¿‡å·²æœ‰æ•°æ®
- [ ] **é”™è¯¯å¤„ç†**: ç½‘ç»œå¼‚å¸¸æ—¶æ˜¯å¦ä¼˜é›…å¤„ç†
- [ ] **æ•°æ®æ ¼å¼**: ç”Ÿæˆçš„æ–‡ä»¶æ ¼å¼æ˜¯å¦æ ‡å‡†
- [ ] **å­˜å‚¨é›†æˆ**: èƒ½å¦æ­£å¸¸ä¸Šä¼ åˆ°MinIOç³»ç»Ÿ

### 6.2 æµ‹è¯•å‘½ä»¤
```bash
# å°è§„æ¨¡æµ‹è¯•
python spider.py --max 3

# åŠŸèƒ½éªŒè¯
python run_crawler.py --crawl-only --max 5


# å®Œæ•´æµç¨‹æµ‹è¯•
python run_crawler.py --max 10
```

---