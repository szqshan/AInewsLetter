# 🕷️ 网站爬虫项目开发方案模板

注意！数据提取策略必须与用户确认后，并且方案取得用户同意后再开始核心代码的开发。

## 🎯 第一阶段：需求分析与方案制定

### 1.1 需求澄清
- **目标网站**: {网站URL}
- **数据类型**: {新闻/文章/产品/数据等}
- **核心功能**: 
  - [ ] 数据爬取
  - [ ] 媒体文件下载
  - [ ] 本地存储
  - [ ] 定时调度
- **数据量级**: {预估数据量}
- **更新频率**: {每日/每周/实时}

### 1.2 技术选型决策
```yaml
爬虫技术:
  - HTML解析: BeautifulSoup4 + requests (稳定性优先)
  - JS渲染: Playwright (复杂页面)
  - 异步处理: asyncio (高性能需求)

数据存储:
  - 本地存储: 标准化目录结构

定时调度:
  - Linux: Cron
  - Windows: 任务计划程序
  - 高级: APScheduler
```

### 1.3 目录结构设计，crawled_data目录下的目录结构必须严格遵循，其他目录结构可以根据实际情况进行设计。


```
{项目名}/
├── 📄 spider.py                    # 主爬虫脚本
├── 📄 run_crawler.py               # 一键运行脚本
├── 📄 config.json                  # 配置文件
├── 📄 requirements.txt             # 依赖包
├── 📄 README.md                    # 说明文档
└── 📊 crawled_data/                # 本地数据存储
    └── {数据类型}/                 # 数据分类目录
        └── {唯一标识}/             # 按ID/slug组织
            ├── content.md          # Markdown内容
            ├── metadata.json       # 元数据JSON
            └── media/              # 媒体文件目录
```

---

## 🔍 第二阶段：目标网站分析

### 2.1 页面结构分析工具

#### 2.1.1 使用Playwright分析页面
```python
# 1. 导航到目标页面
browser.navigate(target_url)

# 2. 获取页面快照
browser.snapshot()

# 3. 分析页面元素
browser.evaluate("页面分析JavaScript")

# 4. 检查网络请求
browser.network_requests()
```

#### 2.1.2 关键信息提取
```yaml
页面架构分析:
  - 列表页面: {URL模式和选择器}
  - 详情页面: {URL模式和内容结构}
  - 分页机制: {翻页方式}
  - 媒体文件: {图片/视频等资源}

技术架构识别:
  - 前端框架: {React/Vue/原生等}
  - 渲染方式: {SSR/CSR/静态}
  - API接口: {REST/GraphQL/无}
  - 反爬措施: {验证码/限频/JS检测}
```
#### 2.1.3 使用playwright查看使用能获取到后端的API接口，直接调用接口获取数据


### 2.2 数据提取策略
```yaml
CSS选择器规划:
  列表页面:
    - 条目链接: "a[href*='/article/']"
    - 标题: ".title"
    - 分类: ".category"
    - 日期: ".date"
    - 摘要: ".summary"
  
  详情页面:
    - 标题: "h1.title"
    - 正文: ".content"
    - 图片: "img"
    - 元数据: ".meta"
```

---

## 🛠️ 第三阶段：核心代码开发，注意！数据提取策略必须与用户确认后，并且方案取得用户同意后再开始核心代码的开发。

### 3.1 爬虫主脚本 (spider.py)

#### 3.1.1 基础架构
```python
class {WebsiteName}Spider:
    def __init__(self, config_file="config.json"):
        """初始化爬虫"""
        self.base_url = "{网站基础URL}"
        self.target_url = "{目标页面URL}"
        self.session = requests.Session()
        self.config = self.load_config(config_file)
        self.setup_directories()
        
    def setup_directories(self):
        """创建必要目录"""
        
    def get_article_list(self):
        """获取文章列表"""
        
    def crawl_article(self, article_url):
        """爬取单篇文章"""
        
    def download_media(self, media_url, save_path):
        """下载媒体文件"""
        
    def save_article(self, data, article_id):
        """保存文章数据"""
        
    def run(self, max_articles=None):
        """执行爬虫"""
```

#### 3.1.2 核心功能实现
```python
# 数据提取
def extract_data(self, soup, url):
    """提取页面数据"""
    return {
        'title': soup.select_one('h1').get_text().strip(),
        'content': self.clean_content(soup.select_one('.content')),
        'date': self.parse_date(soup.select_one('.date')),
        'category': soup.select_one('.category').get_text().strip(),
        'images': self.extract_images(soup),
        'url': url
    }

# 增量更新检查
def is_article_exists(self, article_id):
    """检查文章是否已存在"""
    return Path(f"crawled_data/{data_type}/{article_id}").exists()

# 错误处理和重试
def safe_request(self, url, max_retries=3):
    """安全的HTTP请求"""
```


### 3.3 配置文件 (config.json)
```json
{
  "crawler": {
    "delay": 2,
    "timeout": 30,
    "max_retries": 3,
    "max_articles": 50,
    "user_agent": "标准UserAgent"
  },
  "media": {
    "download_images": true,
    "download_videos": false,
    "image_timeout": 15,
    "max_file_size": 10485760,
    "allowed_extensions": [".jpg", ".png", ".gif"]
  },
  "storage": {
    "data_dir": "crawled_data",
    "data_type": "{数据类型名称}",
    "create_subdirs": true
  },
  "filter": {
    "skip_duplicates": true,
    "min_content_length": 100,
    "categories": []
  },
  "minio": {
    "bucket_name": "{项目名称}",
    "api_endpoint": "localhost:9011"
  }
}
```

---

## 📊 第四阶段：数据格式标准化

### 4.1 Markdown内容格式 (content.md)
```markdown
# {文章标题}

**分类**: {分类}  
**发布日期**: {日期}  
**来源**: {原始URL}  
**爬取时间**: {时间戳}

---

{正文内容}

## 相关媒体

![{描述}](media/{文件名})
```

### 4.2 元数据格式 (metadata.json)
```json
{
  "url": "原始URL",
  "title": "文章标题",
  "category": "分类",
  "date": "发布日期",
  "content": "正文内容",
  "images": [
    {
      "url": "原始图片URL",
      "alt": "图片描述",
      "filename": "本地文件名",
      "local_path": "本地路径"
    }
  ],
  "crawl_time": "爬取时间戳",
  "slug": "文章标识符",
  "word_count": 字数统计,
  "content_hash": "内容哈希"
}
```

---

## 🚀 第五阶段：部署与运维

### 5.1 依赖管理 (requirements.txt)
```
# 核心爬虫依赖
requests>=2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0

# 数据处理
python-dateutil>=2.8.0

# 可选高级功能
# playwright>=1.40.0  # 复杂页面
# aiohttp>=3.9.0      # 异步请求
```

### 5.2 运行脚本模板

#### 5.2.1 一键运行脚本 (run_crawler.py)
```python
#!/usr/bin/env python3
def main():
    parser = argparse.ArgumentParser(description='{项目名称} 一键爬虫')
    parser.add_argument('--crawl-only', action='store_true')
    parser.add_argument('--upload-only', action='store_true')
    parser.add_argument('--max', type=int, default=20)
    parser.add_argument('--force', action='store_true')
    
    args = parser.parse_args()
    
    if not args.upload_only:
        # 执行爬取
        pass
        
    if not args.crawl_only:
        # 执行上传
        pass
```


## ✅ 第六阶段：测试与验证

### 6.1 功能测试清单
- [ ] **基础爬取**: 能否成功访问目标网站
- [ ] **数据提取**: 各字段是否正确提取
- [ ] **媒体下载**: 图片等文件是否正常下载
- [ ] **增量更新**: 重复运行是否跳过已有数据
- [ ] **错误处理**: 网络异常时是否优雅处理
- [ ] **数据格式**: 生成的文件格式是否标准
- [ ] **存储集成**: 能否正常上传到MinIO系统

### 6.2 测试命令
```bash
# 小规模测试
python spider.py --max 3

# 功能验证
python run_crawler.py --crawl-only --max 5


# 完整流程测试
python run_crawler.py --max 10
```

---