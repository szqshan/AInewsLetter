#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Newsletter Crawler ä¸»å…¥å£ç¨‹åº

ç»Ÿä¸€çš„çˆ¬è™«æ‰§è¡Œå…¥å£ï¼Œæä¾›å¤šç§åŠŸèƒ½ï¼š
1. çˆ¬å–æ–‡ç« 
2. æ£€æŸ¥ç©ºå†…å®¹
3. é‡æ–°çˆ¬å–é—®é¢˜æ–‡ç« 
4. ä¸Šä¼ åˆ°OSS
"""

import argparse
import asyncio
import sys
import logging
from pathlib import Path

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from src.newsletter_system.crawler.newsletter_crawler import CrawlerConfig
from src.newsletter_system.utils.logger import setup_logger
from src.newsletter_system.utils.file_utils import load_json

# è®¾ç½®æ—¥å¿—
logger = setup_logger('main', level=logging.INFO)


def run_crawler(args):
    """è¿è¡Œçˆ¬è™«"""
    # ç›´æ¥è°ƒç”¨çˆ¬è™«
    from src.newsletter_system.crawler.newsletter_crawler import NewsletterCrawler, CrawlerConfig
    
    config = CrawlerConfig(
        output_dir=args.output,
        max_concurrent_articles=args.concurrent,
        max_concurrent_images=args.concurrent_images,
        batch_size=args.batch_size,
        api_delay=args.api_delay,
        article_delay=args.article_delay,
        enable_resume=not args.no_resume
    )
    
    async def run():
        print("ğŸš€ å¼€å§‹çˆ¬å–Newsletteræ–‡ç« ...")
        print(f"ğŸ”§ é…ç½®: å¹¶å‘{config.max_concurrent_articles}ç¯‡æ–‡ç« , {config.max_concurrent_images}å¼ å›¾ç‰‡")
        
        async with NewsletterCrawler(config) as crawler:
            stats = await crawler.crawl_all()
            
            print("\nâœ… çˆ¬å–å®Œæˆ!")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   - æ€»æ–‡ç« æ•°: {stats.get('total_articles', 0)}")
            print(f"   - å¤„ç†æˆåŠŸ: {stats.get('processed_articles', 0)}")
            print(f"   - ä¸‹è½½å›¾ç‰‡: {stats.get('total_images', 0)}")
            print(f"   - è¾“å‡ºç›®å½•: {stats.get('output_directory', '')}")
            
            return stats
    
    asyncio.run(run())


def upload_to_oss(args):
    """ä¸Šä¼ åˆ°OSS"""
    # åŠ è½½é…ç½®
    config = load_json('config.json')
    if not config or 'oss' not in config:
        logger.error("OSS configuration not found in config.json")
        return
    
    oss_config = config['oss']
    
    # è¦†ç›–é…ç½®ä¸­çš„å€¼ï¼ˆå¦‚æœå‘½ä»¤è¡Œæä¾›äº†ï¼‰
    if args.bucket:
        oss_config['bucket_name'] = args.bucket
    # è¦†ç›–ç«¯ç‚¹é…ç½®ï¼ˆè‹¥æä¾›ï¼‰
    if getattr(args, 'endpoint', None):
        oss_config['base_url'] = args.endpoint
    if getattr(args, 'public_base_url', None):
        oss_config['public_base_url'] = args.public_base_url
    
    # è¿è¡Œä¸Šä¼ 
    import asyncio
    from src.newsletter_system.oss import OSSUploader
    
    async def run_upload():
        base_dir = Path(args.source_dir or args.output)
        async with OSSUploader(oss_config) as uploader:
            stats = await uploader.upload_all(base_dir, resume=not args.no_resume)
            return stats
    
    stats = asyncio.run(run_upload())
    
    if stats['success']:
        print(f"\nâœ… ä¸Šä¼ æˆåŠŸ!")
        print(f"  æ–‡ä»¶æ•°: {stats['uploaded_files']}")
        print(f"  è€—æ—¶: {stats['elapsed_time_seconds']}ç§’")
        if stats.get('sample_urls'):
            print(f"\nç¤ºä¾‹URL:")
            for url in stats['sample_urls'][:3]:
                print(f"  {url}")
    else:
        print(f"\nâŒ ä¸Šä¼ å¤±è´¥: {stats.get('error', 'Unknown error')}")


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description="Newsletter Crawler - ç»Ÿä¸€æ‰§è¡Œå…¥å£",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # çˆ¬å–æ‰€æœ‰æ–‡ç« 
  python main.py crawl
  
  # ä¸Šä¼ åˆ°OSS
  python main.py upload --bucket my-bucket
  
  # çˆ¬å–å¹¶ä¸Šä¼ 
  python main.py crawl && python main.py upload
        """
    )
    
    # æ·»åŠ å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # çˆ¬è™«å‘½ä»¤
    crawl_parser = subparsers.add_parser('crawl', help='è¿è¡Œçˆ¬è™«')
    crawl_parser.add_argument('--concurrent', type=int, default=5, help='å¹¶å‘æ–‡ç« æ•°')
    crawl_parser.add_argument('--concurrent-images', type=int, default=20, help='å¹¶å‘å›¾ç‰‡æ•°')
    crawl_parser.add_argument('--batch-size', type=int, default=10, help='æ‰¹å¤„ç†å¤§å°')
    crawl_parser.add_argument('--api-delay', type=float, default=1.0, help='APIå»¶è¿Ÿ(ç§’)')
    crawl_parser.add_argument('--article-delay', type=float, default=0.5, help='æ–‡ç« å»¶è¿Ÿ(ç§’)')
    crawl_parser.add_argument('--no-resume', action='store_true', help='ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ')
    crawl_parser.add_argument('--output', default='crawled_data', help='è¾“å‡ºç›®å½•')
    
    # ä¸Šä¼ å‘½ä»¤
    upload_parser = subparsers.add_parser('upload', help='ä¸Šä¼ åˆ°OSS')
    upload_parser.add_argument('--bucket', help='è¦†ç›–é…ç½®ä¸­çš„bucketåç§°')
    upload_parser.add_argument('--no-resume', action='store_true', help='ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ')
    upload_parser.add_argument('--output', default='crawled_data', help='æ•°æ®ç›®å½•')
    # ç¨³å®šåˆ«åä¸è¦†ç›–é¡¹ï¼Œé¿å…åç»­æ”¹åŠ¨å½±å“
    upload_parser.add_argument('--source-dir', dest='source_dir', default=None, help='æ•°æ®ç›®å½•ï¼ˆåˆ«åï¼Œç­‰ä»·äº --outputï¼‰')
    upload_parser.add_argument('--endpoint', dest='endpoint', default=None, help='è¦†ç›–é…ç½®ä¸­çš„endpoint/base_url')
    upload_parser.add_argument('--public-base-url', dest='public_base_url', default=None, help='è¦†ç›–é…ç½®ä¸­çš„public_base_url')
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    # æ‰§è¡Œå¯¹åº”å‘½ä»¤
    try:
        if args.command == 'crawl':
            run_crawler(args)
        elif args.command == 'upload':
            upload_to_oss(args)
        else:
            parser.print_help()
            sys.exit(1)
    except KeyboardInterrupt:
        logger.info("\næ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        sys.exit(1)


if __name__ == '__main__':
    main()