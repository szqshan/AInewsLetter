#!/usr/bin/env python3
"""
Newsletter çˆ¬è™«è¿è¡Œè„šæœ¬ï¼ˆåŸºç¡€ç‰ˆï¼‰ã€‚

ç”¨é€”ï¼š
- é¢å‘å¸¸è§„ç«™ç‚¹/è½»åº¦é£æ§åœºæ™¯ï¼Œç›´æ¥åŸºäº `NewsletterCrawler` è¿›è¡ŒæŠ“å–ã€‚
- é€šè¿‡å‘½ä»¤è¡Œå‚æ•°é…ç½®è¾“å‡ºç›®å½•ã€å¹¶å‘ã€æ‰¹æ¬¡ä¸å»¶è¿Ÿç­‰å…³é”®è¡Œä¸ºã€‚

æç¤ºï¼š
- è‹¥é¢‘ç¹å‡ºç°é™æµæˆ–éªŒè¯ç ï¼Œå»ºè®®æ”¹ç”¨ `run_anti_detect.py`ã€‚
"""

import asyncio
import sys
import argparse
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from newsletter_system.crawler.newsletter_crawler import NewsletterCrawler, CrawlerConfig

async def run_crawler(config: CrawlerConfig):
    """è¿è¡Œçˆ¬è™«"""
    print("ğŸš€ å¼€å§‹çˆ¬å–Newsletteræ–‡ç« ...")
    print(f"ğŸ”§ é…ç½®: å¹¶å‘{config.max_concurrent_articles}ç¯‡æ–‡ç« , {config.max_concurrent_images}å¼ å›¾ç‰‡")
    
    async with NewsletterCrawler(config) as crawler:
        stats = await crawler.crawl_all()
        
        print("\nâœ… çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ€»æ–‡ç« æ•°: {stats.get('total_articles', 0)}")
        print(f"   - å¤„ç†æˆåŠŸ: {stats.get('processed_articles', 0)}")
        print(f"   - ä¸‹è½½å›¾ç‰‡: {stats.get('total_images', 0)}")
        print(f"   - è¾“å‡ºç›®å½•: {stats.get('output_directory', '')}")
        
        return stats

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Newsletterçˆ¬è™«ç³»ç»Ÿ")
    parser.add_argument("--output-dir", "-o", default="crawled_data", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max-concurrent-articles", type=int, default=5, help="æœ€å¤§å¹¶å‘æ–‡ç« å¤„ç†æ•°")
    parser.add_argument("--max-concurrent-images", type=int, default=20, help="æœ€å¤§å¹¶å‘å›¾ç‰‡ä¸‹è½½æ•°")
    parser.add_argument("--batch-size", type=int, default=10, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--no-resume", action="store_true", help="ç¦ç”¨æ–­ç‚¹ç»­ä¼ ")
    parser.add_argument("--api-delay", type=float, default=1.0, help="APIè¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("--article-delay", type=float, default=0.5, help="æ–‡ç« å¤„ç†é—´éš”ï¼ˆç§’ï¼‰")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = CrawlerConfig(
        output_dir=args.output_dir,
        max_concurrent_articles=args.max_concurrent_articles,
        max_concurrent_images=args.max_concurrent_images,
        batch_size=args.batch_size,
        enable_resume=not args.no_resume,
        api_delay=args.api_delay,
        article_delay=args.article_delay
    )
    
    # è¿è¡Œçˆ¬è™«
    await run_crawler(config)
    
    print("\nğŸ‰ çˆ¬è™«ä»»åŠ¡å®Œæˆ!")

if __name__ == "__main__":
    asyncio.run(main())