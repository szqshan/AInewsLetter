# Newsletter OSS ä¸Šä¼ ç³»ç»Ÿè®¾è®¡å’Œä½¿ç”¨æ–¹å¼

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ Newsletter çˆ¬è™«ç³»ç»Ÿçš„å¯¹è±¡å­˜å‚¨ (OSS/MinIO) ä¸Šä¼ åŠŸèƒ½çš„è®¾è®¡æ¶æ„å’Œä½¿ç”¨æ–¹æ³•ã€‚

## ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒç»„ä»¶

```
OSSä¸Šä¼ ç³»ç»Ÿ
â”œâ”€â”€ OSSUploader (ä¸»ä¸Šä¼ å™¨)
â”œâ”€â”€ OSSWrapper (æ¥å£é€‚é…å™¨) 
â”œâ”€â”€ MinIOå®¢æˆ·ç«¯ (åº•å±‚å­˜å‚¨)
â””â”€â”€ è¿›åº¦ç®¡ç† (æ–­ç‚¹ç»­ä¼ )
```

### æŠ€æœ¯æ ˆ
- **å­˜å‚¨åç«¯**: MinIO (S3å…¼å®¹å¯¹è±¡å­˜å‚¨)
- **Python SDK**: minio-py
- **å¹¶å‘æ§åˆ¶**: asyncio + ThreadPoolExecutor
- **è¿›åº¦æŒä¹…åŒ–**: JSONæ–‡ä»¶
- **é”™è¯¯é‡è¯•**: æŒ‡æ•°é€€é¿ç®—æ³•

## æ•°æ®å­˜å‚¨ç»“æ„

### å­˜å‚¨è·¯å¾„è®¾è®¡

```
bucket-name/
â””â”€â”€ articles/                    # æ–‡ç« æ ¹ç›®å½•
    â””â”€â”€ {source_id}/            # æ•°æ®æºæ ‡è¯† (nlp-elvissaravia)
        â””â”€â”€ {article_id}/       # æ–‡ç« IDç›®å½•
            â”œâ”€â”€ content.md      # Markdownå†…å®¹
            â”œâ”€â”€ metadata.json   # æ–‡ç« å…ƒæ•°æ®
            â””â”€â”€ images/         # å›¾ç‰‡ç›®å½•
                â”œâ”€â”€ cover.jpg   # å°é¢å›¾ç‰‡ (å¦‚æœæœ‰)
                â””â”€â”€ img_*.jpg   # å†…å®¹å›¾ç‰‡
```

### è·¯å¾„ç¤ºä¾‹

```
test-newsletter-upload/
â””â”€â”€ articles/
    â””â”€â”€ nlp-elvissaravia/
        â”œâ”€â”€ 169787925/
        â”‚   â”œâ”€â”€ content.md
        â”‚   â”œâ”€â”€ metadata.json
        â”‚   â””â”€â”€ images/
        â”‚       â”œâ”€â”€ img_0.jpg
        â”‚       â”œâ”€â”€ img_1.jpg
        â”‚       â””â”€â”€ img_2.jpg
        â”œâ”€â”€ 169783090/
        â”‚   â”œâ”€â”€ content.md
        â”‚   â”œâ”€â”€ metadata.json
        â”‚   â””â”€â”€ images/
        â”‚       â””â”€â”€ cover.jpg
        â””â”€â”€ ...
```

## é…ç½®è®¾ç½®

### config.json é…ç½®

```json
{
  "oss": {
    "base_url": "http://localhost:9011",           // MinIOæœåŠ¡åœ°å€
    "public_base_url": "http://60.205.160.74:9000", // å…¬ç½‘è®¿é—®åœ°å€
    "bucket_name": "newsletter-articles-nlp",      // é»˜è®¤å­˜å‚¨æ¡¶åç§°
    "source_id": "nlp-elvissaravia",               // æ•°æ®æºæ ‡è¯†
    "max_concurrent_uploads": 10,                  // æœ€å¤§å¹¶å‘ä¸Šä¼ æ•°
    "upload_timeout": 60,                          // ä¸Šä¼ è¶…æ—¶æ—¶é—´(ç§’)
    "retry_attempts": 3,                           // é‡è¯•æ¬¡æ•°
    "chunk_size": 8192                             // æ–‡ä»¶å—å¤§å°
  }
}
```

### ç¯å¢ƒå˜é‡

```bash
# MinIOè®¤è¯ä¿¡æ¯ (å¿…éœ€)
export MINIO_ACCESS_KEY="your-access-key"
export MINIO_SECRET_KEY="your-secret-key"

# å¯é€‰: è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„endpoint
export MINIO_ENDPOINT="http://localhost:9000"
```

## ä½¿ç”¨æ–¹å¼

### 1. åŸºæœ¬ä¸Šä¼ å‘½ä»¤

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®ä¸Šä¼ æ‰€æœ‰æ–‡ç« 
python3 main.py upload

# æŒ‡å®šè‡ªå®šä¹‰bucketåç§°
python3 main.py upload --bucket my-custom-bucket

# ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°ä¸Šä¼ æ‰€æœ‰æ–‡ä»¶
python3 main.py upload --no-resume
```

### 2. å‘½ä»¤è¡Œå‚æ•°

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `--bucket` | æŒ‡å®šbucketåç§° | `newsletter-articles-nlp` |
| `--no-resume` | ç¦ç”¨æ–­ç‚¹ç»­ä¼  | å¯ç”¨ |
| `--source-dir` | æŒ‡å®šæºç›®å½• | `crawled_data` |

### 3. Python ä»£ç è°ƒç”¨

```python
import asyncio
from src.newsletter_system.oss.oss_uploader import OSSUploader

async def upload_articles():
    # åˆ›å»ºä¸Šä¼ å™¨å®ä¾‹
    uploader = OSSUploader(
        endpoint='http://localhost:9000',
        access_key='your-access-key',
        secret_key='your-secret-key',
        bucket_name='my-bucket',
        source_id='nlp-elvissaravia'
    )
    
    # ä¸Šä¼ æŒ‡å®šç›®å½•çš„æ‰€æœ‰æ–‡ç« 
    await uploader.upload_all_articles('crawled_data')

# è¿è¡Œä¸Šä¼ 
asyncio.run(upload_articles())
```

### 4. å•æ–‡ç« ä¸Šä¼ 

```python
async def upload_single_article():
    uploader = OSSUploader(...)
    
    # ä¸Šä¼ å•ç¯‡æ–‡ç« 
    article_path = "crawled_data/articles/169787925_Top-AI-Papers-of-the-Week"
    success = await uploader.upload_article(article_path)
    
    if success:
        print("ä¸Šä¼ æˆåŠŸ")
    else:
        print("ä¸Šä¼ å¤±è´¥")
```

## æ ¸å¿ƒåŠŸèƒ½ç‰¹æ€§

### 1. è‡ªåŠ¨Bucketç®¡ç†

```python
# è‡ªåŠ¨åˆ›å»ºbucket
await uploader.ensure_bucket_exists()

# è‡ªåŠ¨è®¾ç½®å…¬å…±è¯»æƒé™
await uploader.set_bucket_policy_public()
```

**åŠŸèƒ½è¯´æ˜ï¼š**
- æ£€æŸ¥bucketæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»º
- è‡ªåŠ¨è®¾ç½®bucketä¸ºå…¬å…±è¯»å–æƒé™
- æ”¯æŒè·¨åŒºåŸŸbucketåˆ›å»º

### 2. æ™ºèƒ½æ–‡ä»¶å‘ç°

```python
# è‡ªåŠ¨æ‰«ææ–‡ç« ç›®å½•
articles = uploader.discover_articles("crawled_data/articles")

# æ¯ç¯‡æ–‡ç« åŒ…å«ï¼š
# - content.md (å¿…éœ€)
# - metadata.json (å¿…éœ€)  
# - images/ ç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡ (å¯é€‰)
```

**å‘ç°è§„åˆ™ï¼š**
- æ‰«æ `crawled_data/articles/` ä¸‹çš„æ‰€æœ‰å­ç›®å½•
- æ¯ä¸ªå­ç›®å½•ä»£è¡¨ä¸€ç¯‡æ–‡ç« 
- å¿…é¡»åŒ…å« `content.md` å’Œ `metadata.json`
- è‡ªåŠ¨åŒ…å« `images/` ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶

### 3. æ–­ç‚¹ç»­ä¼ æœºåˆ¶

```python
# è¿›åº¦æ–‡ä»¶ä½ç½®
progress_file = "crawled_data/upload_progress.json"

# è¿›åº¦æ•°æ®ç»“æ„
{
  "uploaded_articles": ["169787925", "169783090"],
  "failed_articles": {
    "169333505": "Connection timeout"
  },
  "upload_start_time": "2024-08-07T01:28:52",
  "last_update_time": "2024-08-07T01:30:52"
}
```

**å·¥ä½œåŸç†ï¼š**
- ä¸Šä¼ å‰æ£€æŸ¥è¿›åº¦æ–‡ä»¶ï¼Œè·³è¿‡å·²ä¸Šä¼ çš„æ–‡ç« 
- å®æ—¶æ›´æ–°è¿›åº¦ï¼Œæ”¯æŒä¸­æ–­åç»§ç»­
- è®°å½•å¤±è´¥åŸå› ï¼Œä¾¿äºé—®é¢˜æ’æŸ¥
- ä½¿ç”¨ `--no-resume` å¯å¿½ç•¥è¿›åº¦é‡æ–°ä¸Šä¼ 

### 4. å¹¶å‘ä¸Šä¼ æ§åˆ¶

```python
# ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
semaphore = asyncio.Semaphore(max_concurrent_uploads)

async def upload_with_semaphore(article_path):
    async with semaphore:
        return await self.upload_single_article(article_path)

# æ‰¹é‡å¹¶å‘ä¸Šä¼ 
tasks = [upload_with_semaphore(path) for path in article_paths]
results = await asyncio.gather(*tasks, return_exceptions=True)
```

**æ€§èƒ½ä¼˜åŒ–ï¼š**
- é»˜è®¤10ä¸ªå¹¶å‘ä¸Šä¼ ä»»åŠ¡
- è‡ªåŠ¨è´Ÿè½½å‡è¡¡ï¼Œé¿å…æœåŠ¡å™¨è¿‡è½½
- æ”¯æŒåŠ¨æ€è°ƒæ•´å¹¶å‘æ•°

### 5. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
# æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
async def upload_with_retry(self, file_path, object_name):
    for attempt in range(self.retry_attempts):
        try:
            await self.upload_file(file_path, object_name)
            return True
        except Exception as e:
            if attempt < self.retry_attempts - 1:
                delay = 2 ** attempt  # 1s, 2s, 4s
                await asyncio.sleep(delay)
                continue
            else:
                logger.error(f"Final upload failed: {e}")
                return False
```

**é”™è¯¯ç±»å‹å¤„ç†ï¼š**
- **ç½‘ç»œé”™è¯¯**: è‡ªåŠ¨é‡è¯•ï¼Œæœ€å¤š3æ¬¡
- **è®¤è¯é”™è¯¯**: ç«‹å³å¤±è´¥ï¼Œæ£€æŸ¥å¯†é’¥
- **æƒé™é”™è¯¯**: ç«‹å³å¤±è´¥ï¼Œæ£€æŸ¥bucketæƒé™  
- **ç©ºé—´ä¸è¶³**: ç«‹å³å¤±è´¥ï¼Œæ£€æŸ¥å­˜å‚¨é…é¢

## ç›‘æ§å’Œæ—¥å¿—

### 1. ä¸Šä¼ è¿›åº¦ç›‘æ§

```bash
# å®æ—¶æŸ¥çœ‹ä¸Šä¼ æ—¥å¿—
tail -f upload.log

# è¾“å‡ºç¤ºä¾‹:
# 2024-08-07 01:28:53 - INFO - ğŸª£ Setting up bucket: test-newsletter-upload
# 2024-08-07 01:28:53 - INFO - âœ… Created bucket: test-newsletter-upload
# 2024-08-07 01:28:53 - INFO - ğŸ“Š Found 180 articles to process
# 2024-08-07 01:28:55 - INFO - âœ… Successfully uploaded: 100722694_Top-ML-Papers-of-the-Week
```

### 2. è¿›åº¦ç»Ÿè®¡è„šæœ¬

```python
# æ£€æŸ¥ä¸Šä¼ è¿›åº¦
def check_upload_progress():
    with open('crawled_data/upload_progress.json') as f:
        progress = json.load(f)
    
    total_articles = len(os.listdir('crawled_data/articles'))
    uploaded = len(progress['uploaded_articles'])
    failed = len(progress['failed_articles'])
    
    print(f"ä¸Šä¼ è¿›åº¦: {uploaded}/{total_articles}")
    print(f"æˆåŠŸç‡: {uploaded/(uploaded+failed)*100:.1f}%")
    print(f"å¤±è´¥æ•°: {failed}")
```

### 3. å­˜å‚¨ç©ºé—´æ£€æŸ¥

```bash
# æ£€æŸ¥bucketå¤§å° (éœ€è¦mcå·¥å…·)
mc du minio/test-newsletter-upload

# æ£€æŸ¥æ–‡ä»¶æ•°é‡
mc ls --recursive minio/test-newsletter-upload | wc -l

# æ£€æŸ¥ä¸Šä¼ å®Œæ•´æ€§
python3 -c "
import json
with open('crawled_data/upload_progress.json') as f:
    progress = json.load(f)
print(f'å·²ä¸Šä¼ : {len(progress[\"uploaded_articles\"])} ç¯‡æ–‡ç« ')
print(f'å¤±è´¥: {len(progress[\"failed_articles\"])} ç¯‡æ–‡ç« ')
"
```

## æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶å‘å‚æ•°è°ƒä¼˜

```json
{
  "oss": {
    "max_concurrent_uploads": 20,    // é«˜æ€§èƒ½æœåŠ¡å™¨
    "upload_timeout": 120,          // å¤§æ–‡ä»¶è¶…æ—¶
    "chunk_size": 16384            // ç½‘ç»œä¼˜åŒ–
  }
}
```

**è°ƒä¼˜å»ºè®®ï¼š**
- **é«˜å¸¦å®½ç½‘ç»œ**: `max_concurrent_uploads: 20-50`
- **ä½å¸¦å®½ç½‘ç»œ**: `max_concurrent_uploads: 5-10`  
- **å¤§æ–‡ä»¶ä¸Šä¼ **: å¢åŠ  `upload_timeout`
- **å°æ–‡ä»¶ä¼˜åŒ–**: å‡å° `chunk_size`

### 2. ç½‘ç»œä¼˜åŒ–

```python
# å¯ç”¨è¿æ¥å¤ç”¨
client = Minio(
    endpoint,
    access_key=access_key,
    secret_key=secret_key,
    secure=secure,
    http_client=urllib3.PoolManager(
        maxsize=50,        # è¿æ¥æ± å¤§å°
        timeout=60,        # è¿æ¥è¶…æ—¶
        retries=3          # é‡è¯•æ¬¡æ•°
    )
)
```

### 3. å†…å­˜ä¼˜åŒ–

```python
# åˆ†æ‰¹ä¸Šä¼ ï¼Œé¿å…å†…å­˜æº¢å‡º
batch_size = 50
for i in range(0, len(articles), batch_size):
    batch = articles[i:i+batch_size]
    await upload_batch(batch)
    
    # æ‰¹æ¬¡é—´æš‚åœï¼Œé‡Šæ”¾èµ„æº
    await asyncio.sleep(1)
```

## æ•…éšœæ’æŸ¥

### 1. å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

| é”™è¯¯ç±»å‹ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|---------|------|----------|
| è®¤è¯å¤±è´¥ | `AccessDenied` | æ£€æŸ¥ `MINIO_ACCESS_KEY/SECRET_KEY` |
| ç½‘ç»œè¶…æ—¶ | `Connection timeout` | å¢åŠ  `upload_timeout`ï¼Œæ£€æŸ¥ç½‘ç»œ |
| Bucketä¸å­˜åœ¨ | `NoSuchBucket` | è‡ªåŠ¨åˆ›å»ºæˆ–æ‰‹åŠ¨åˆ›å»ºbucket |
| æƒé™ä¸è¶³ | `AccessDenied on PUT` | æ£€æŸ¥ç”¨æˆ·æƒé™ï¼Œè®¾ç½®bucket policy |
| æ–‡ä»¶ä¸å­˜åœ¨ | `FileNotFound` | æ£€æŸ¥æºæ–‡ä»¶è·¯å¾„ï¼Œé‡æ–°çˆ¬å– |

### 2. è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export MINIO_DEBUG=1
python3 main.py upload --bucket debug-test

# æµ‹è¯•å•ç¯‡æ–‡ç« ä¸Šä¼ 
python3 -c "
import asyncio
from src.newsletter_system.oss.oss_uploader import OSSUploader
uploader = OSSUploader(...)
asyncio.run(uploader.upload_article('crawled_data/articles/169787925_Top-AI-Papers-of-the-Week'))
"
```

### 3. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥

```python
# æ£€æŸ¥ä¸Šä¼ å®Œæ•´æ€§
async def verify_upload_integrity():
    uploader = OSSUploader(...)
    
    # è·å–æœ¬åœ°æ–‡ç« åˆ—è¡¨
    local_articles = uploader.discover_articles("crawled_data/articles")
    
    # æ£€æŸ¥è¿œç¨‹æ–‡ä»¶
    for article_id in local_articles:
        # æ£€æŸ¥content.md
        exists = await uploader.object_exists(f"articles/nlp-elvissaravia/{article_id}/content.md")
        if not exists:
            print(f"Missing: {article_id}/content.md")
        
        # æ£€æŸ¥metadata.json
        exists = await uploader.object_exists(f"articles/nlp-elvissaravia/{article_id}/metadata.json")
        if not exists:
            print(f"Missing: {article_id}/metadata.json")
```

## æœ€ä½³å®è·µ

### 1. ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

```bash
# 1. è®¾ç½®ç¯å¢ƒå˜é‡
export MINIO_ACCESS_KEY="production-key"
export MINIO_SECRET_KEY="production-secret"
export MINIO_ENDPOINT="https://oss.yourcompany.com"

# 2. ä½¿ç”¨ä¸“ç”¨bucket
python3 main.py upload --bucket newsletter-production

# 3. ç›‘æ§ä¸Šä¼ çŠ¶æ€
python3 main.py upload --bucket newsletter-production 2>&1 | tee upload.log

# 4. å®šæœŸå¤‡ä»½è¿›åº¦æ–‡ä»¶
cp crawled_data/upload_progress.json backups/upload_progress_$(date +%Y%m%d).json
```

### 2. æ•°æ®è¿ç§»

```bash
# ä»ä¸€ä¸ªbucketè¿ç§»åˆ°å¦ä¸€ä¸ªbucket
python3 -c "
import asyncio
from src.newsletter_system.oss.oss_uploader import OSSUploader

async def migrate():
    # ä»æºbucketä¸‹è½½
    source = OSSUploader(bucket_name='old-bucket')
    # ä¸Šä¼ åˆ°ç›®æ ‡bucket  
    target = OSSUploader(bucket_name='new-bucket')
    
    # å®ç°è¿ç§»é€»è¾‘
    await migrate_between_buckets(source, target)

asyncio.run(migrate())
"
```

### 3. è‡ªåŠ¨åŒ–è„šæœ¬

```bash
#!/bin/bash
# auto_upload.sh - è‡ªåŠ¨çˆ¬å–å’Œä¸Šä¼ è„šæœ¬

set -e

echo "å¼€å§‹çˆ¬å–..."
python3 main.py crawl

echo "å¼€å§‹ä¸Šä¼ ..."
python3 main.py upload --bucket newsletter-$(date +%Y%m)

echo "æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
rm -f crawled_data/upload_progress.json

echo "å®Œæˆ!"
```

## å®‰å…¨è€ƒè™‘

### 1. è®¿é—®æ§åˆ¶

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": "*",
      "Action": ["s3:GetObject"],
      "Resource": ["arn:aws:s3:::newsletter-public/*"]
    },
    {
      "Effect": "Deny", 
      "Principal": "*",
      "Action": ["s3:PutObject", "s3:DeleteObject"],
      "Resource": ["arn:aws:s3:::newsletter-public/*"]
    }
  ]
}
```

### 2. æ•°æ®åŠ å¯†

```python
# å¯ç”¨æœåŠ¡ç«¯åŠ å¯†
client.put_object(
    bucket_name,
    object_name,
    data,
    length,
    metadata={
        'x-amz-server-side-encryption': 'AES256'
    }
)
```

### 3. è®¿é—®æ—¥å¿—

```python
# è®°å½•æ‰€æœ‰ä¸Šä¼ æ“ä½œ
import logging

upload_logger = logging.getLogger('oss_upload')
upload_logger.info(f"Upload: {object_name} by {user_id}")
```

## é‡è¦ä¿®å¤è¯´æ˜

### å›¾ç‰‡URLå…¬å¼€è®¿é—®åœ°å€ä¿®å¤

**é—®é¢˜æè¿°ï¼š**
ä¹‹å‰ç‰ˆæœ¬ä¸­ï¼Œä¸Šä¼ åˆ°OSSåçš„å›¾ç‰‡URLä½¿ç”¨çš„æ˜¯åç«¯APIåœ°å€ï¼ˆå¦‚ `http://localhost:9011`ï¼‰ï¼Œå¯¼è‡´å¤–éƒ¨æ— æ³•ç›´æ¥è®¿é—®å›¾ç‰‡ã€‚

**ä¿®å¤å†…å®¹ï¼š**
1. **MinIOUploader æ„é€ å‡½æ•°**ï¼šæ·»åŠ  `public_base_url` å‚æ•°
2. **URLç”Ÿæˆé€»è¾‘**ï¼šæ‰€æœ‰ä¸Šä¼ æ–‡ä»¶çš„å…¬å¼€URLç°åœ¨ä½¿ç”¨ `public_base_url`
3. **é…ç½®é›†æˆ**ï¼šä» `config.json` ä¸­è¯»å– `public_base_url` é…ç½®
4. **å†…å®¹æ›¿æ¢**ï¼šæ–‡ç« å†…å®¹ä¸­çš„å›¾ç‰‡URLæ›¿æ¢ä¸ºå…¬å¼€è®¿é—®åœ°å€

**é…ç½®ç¤ºä¾‹ï¼š**
```json
{
  "oss": {
    "base_url": "http://localhost:9011",           // åç«¯APIåœ°å€ï¼ˆå†…éƒ¨è°ƒç”¨ï¼‰
    "public_base_url": "http://60.205.160.74:9000", // å…¬å¼€è®¿é—®åœ°å€ï¼ˆå¤–éƒ¨è®¿é—®ï¼‰
    "bucket_name": "newsletter-articles-nlp"
  }
}
```

**ä¿®å¤åæ•ˆæœï¼š**
- ä¸Šä¼ å‰ï¼šå›¾ç‰‡URLä¸º `images/img_0.jpg`ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
- ä¸Šä¼ åï¼šå›¾ç‰‡URLä¸º `http://60.205.160.74:9000/bucket-name/articles/article-id/images/img_0.jpg`ï¼ˆå…¬å¼€è®¿é—®URLï¼‰

**éªŒè¯æ–¹æ³•ï¼š**
```bash
# é‡æ–°ä¸Šä¼ æ–‡ç« ä»¥åº”ç”¨ä¿®å¤
python3 main.py upload --bucket test-public-fix --no-resume

# æ£€æŸ¥ä¸Šä¼ åçš„æ–‡ç« å†…å®¹
# æ‰€æœ‰å›¾ç‰‡URLåº”è¯¥ä½¿ç”¨ public_base_url
```

## æ›´æ–°æ—¥å¿—

### v2.2 (2024-08-07)
- ğŸ”§ **é‡å¤§ä¿®å¤**: ä¿®å¤å›¾ç‰‡URLæ›¿æ¢é€»è¾‘ï¼Œé¿å…é‡å¤æ›¿æ¢å¯¼è‡´çš„åµŒå¥—URLé—®é¢˜
- âœ… æ”¹è¿› `replace_image_urls` æ–¹æ³•ï¼Œä½¿ç”¨ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
- âœ… æŒ‰è·¯å¾„é•¿åº¦æ’åºå¤„ç†ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…å†²çª
- âœ… å¢å¼ºå¯¹Markdownå›¾ç‰‡è¯­æ³• `![](path)` çš„æ”¯æŒ
- âœ… é˜²æ­¢å·²æ›¿æ¢URLè¢«äºŒæ¬¡å¤„ç†

### v2.1 (2024-08-07)
- ğŸ”§ **é‡è¦ä¿®å¤**: å›¾ç‰‡URLä½¿ç”¨å…¬å¼€è®¿é—®åœ°å€è€Œéåç«¯APIåœ°å€
- âœ… MinIOUploaderæ·»åŠ public_base_urlå‚æ•°
- âœ… æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ åè¿”å›å…¬å¼€å¯è®¿é—®çš„URL
- âœ… æ–‡ç« å†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„æ›¿æ¢ä¸ºå…¬å¼€URL
- âœ… é…ç½®æ–‡ä»¶æ”¯æŒç‹¬ç«‹çš„åç«¯å’Œå…¬å¼€è®¿é—®åœ°å€

### v2.0 (2024-08)
- âœ… é‡æ„ä¸ºæ¨¡å—åŒ–æ¶æ„
- âœ… æ·»åŠ æ–­ç‚¹ç»­ä¼ åŠŸèƒ½
- âœ… ä¼˜åŒ–å¹¶å‘ä¸Šä¼ æ€§èƒ½
- âœ… å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- âœ… æ”¯æŒè‡ªå®šä¹‰bucketåç§°
- âœ… æ·»åŠ ä¸Šä¼ è¿›åº¦ç›‘æ§

### v1.0 (2024-07) 
- âœ… åŸºç¡€ä¸Šä¼ åŠŸèƒ½
- âœ… MinIOé›†æˆ
- âœ… ç®€å•é”™è¯¯å¤„ç†