#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨åçˆ¬è™«æ£€æµ‹å¢å¼ºç‰ˆçˆ¬è™«çš„å‘½ä»¤è¡Œå…¥å£ã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- é€šè¿‡ `EnhancedCrawlerConfig` é…ç½®æ›´ä¿å®ˆçš„å¹¶å‘ä¸æ›´é•¿çš„å»¶è¿Ÿï¼›
- å¯é€‰ä»£ç†æ”¯æŒï¼›
- æ§åˆ¶å°è¾“å‡ºå…³é”®ç»Ÿè®¡ï¼ŒæŒ‡å¯¼é™æµæƒ…å†µä¸‹çš„å‚æ•°è°ƒä¼˜ã€‚

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # åŸºç¡€ç”¨æ³•ï¼ˆæ¨èï¼‰
    python run_anti_detect.py

    # ä»…å¤„ç†å‰ 5 ç¯‡ï¼ˆå¿«é€Ÿè”è°ƒï¼‰
    python run_anti_detect.py --limit 5

    # æä½é€Ÿç­–ç•¥ï¼ˆæœ€å®‰å…¨ï¼‰
    python run_anti_detect.py --concurrent 1 --batch 2 --article-delay 10
"""

import asyncio
import sys
import argparse
import logging
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))

from src.newsletter_system.crawler.anti_detect_crawler import AntiDetectCrawler, EnhancedCrawlerConfig
from src.newsletter_system.utils.logger import setup_logger

# è®¾ç½®æ—¥å¿—
logger = setup_logger('anti_detect_crawler', level=logging.INFO)


async def main(args):
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºé…ç½®
    config = EnhancedCrawlerConfig(
        output_dir=args.output,
        max_concurrent_articles=args.concurrent,  # é™ä½å¹¶å‘æ•°
        batch_size=args.batch,  # å‡å°æ‰¹æ¬¡å¤§å°
        api_delay=args.api_delay,
        article_delay=args.article_delay,
        enable_resume=not args.no_resume,
        use_proxy=args.use_proxy,
        proxy_url=args.proxy_url,
        smart_delay=True,
        min_delay=3.0,
        max_delay=10.0,
        batch_delay=20.0,
        rate_limit_delay=60.0
    )
    
    print("\n" + "="*60)
    print("ğŸ›¡ï¸  åçˆ¬è™«å¢å¼ºç‰ˆ Newsletter çˆ¬è™«")
    print("="*60)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"ğŸ”§ å¹¶å‘æ•°: {config.max_concurrent_articles} ç¯‡æ–‡ç« ")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"â±ï¸  APIå»¶è¿Ÿ: {config.api_delay}ç§’")
    print(f"â±ï¸  æ–‡ç« å»¶è¿Ÿ: {config.article_delay}ç§’")
    print(f"ğŸ”„ æœ€å¤§é‡è¯•: {config.max_retries}æ¬¡")
    print(f"ğŸ’¾ æ–­ç‚¹ç»­ä¼ : {'å¯ç”¨' if config.enable_resume else 'ç¦ç”¨'}")
    if config.use_proxy:
        print(f"ğŸŒ ä»£ç†: {config.proxy_url}")
    print("="*60 + "\n")
    
    try:
        async with AntiDetectCrawler(config) as crawler:
            # è·å–æ–‡ç« åˆ—è¡¨
            print("ğŸ“‹ è·å–æ–‡ç« åˆ—è¡¨...")
            articles = await crawler.get_all_articles_metadata()
            
            if not articles:
                print("âŒ æœªè·å–åˆ°æ–‡ç« ")
                return
            
            print(f"âœ… è·å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
            
            # å¦‚æœæŒ‡å®šäº†é™åˆ¶ï¼Œåªå¤„ç†éƒ¨åˆ†æ–‡ç« 
            if args.limit:
                articles = articles[:args.limit]
                print(f"ğŸ¯ é™åˆ¶å¤„ç†å‰ {args.limit} ç¯‡æ–‡ç« ")
            
            # å¤„ç†æ–‡ç« 
            print("\nğŸš€ å¼€å§‹å¤„ç†æ–‡ç« ï¼ˆä½¿ç”¨åçˆ¬ç­–ç•¥ï¼‰...")
            print("ğŸ’¡ æç¤ºï¼šå¤„ç†é€Ÿåº¦ä¼šæ¯”è¾ƒæ…¢ï¼Œè¿™æ˜¯ä¸ºäº†é¿å…è¢«æ£€æµ‹")
            
            stats = await crawler.process_articles(articles)
            
            # è¾“å‡ºç»Ÿè®¡
            print("\n" + "="*60)
            print("ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡ï¼š")
            print(f"  æ€»æ–‡ç« æ•°: {stats.get('total_articles', 0)}")
            print(f"  âœ… æˆåŠŸå¤„ç†: {stats.get('processed_articles', 0)}")
            print(f"  âŒ å¤„ç†å¤±è´¥: {stats.get('failed_articles', 0)}")
            print(f"  â­ï¸  è·³è¿‡ï¼ˆå·²å¤„ç†ï¼‰: {stats.get('skipped_articles', 0)}")
            print(f"  ğŸ“ è¾“å‡ºç›®å½•: {config.output_dir}")
            print("="*60)
            
            # å¦‚æœæœ‰å¤±è´¥çš„ï¼Œç»™å‡ºå»ºè®®
            if stats.get('failed_articles', 0) > 0:
                print("\nğŸ’¡ å»ºè®®ï¼š")
                print("  1. å¢åŠ å»¶è¿Ÿæ—¶é—´ï¼š--article-delay 10")
                print("  2. å‡å°‘å¹¶å‘æ•°ï¼š--concurrent 1")
                print("  3. ä½¿ç”¨ä»£ç†ï¼š--use-proxy --proxy-url http://proxy:port")
                print("  4. ç¨åå†è¯•ï¼ˆç½‘ç«™å¯èƒ½æš‚æ—¶é™æµï¼‰")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        logger.error(f"çˆ¬è™«é”™è¯¯: {e}", exc_info=True)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="åçˆ¬è™«å¢å¼ºç‰ˆ Newsletter çˆ¬è™«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ï¼š
  # åŸºç¡€ä½¿ç”¨ï¼ˆæ¨èï¼‰
  python run_anti_detect.py
  
  # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰
  python run_anti_detect.py --limit 5
  
  # æä½é€Ÿçˆ¬å–ï¼ˆæœ€å®‰å…¨ï¼‰
  python run_anti_detect.py --concurrent 1 --batch 2 --article-delay 10
  
  # ä½¿ç”¨ä»£ç†
  python run_anti_detect.py --use-proxy --proxy-url http://127.0.0.1:7890
  
  # ç¦ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé‡æ–°å¼€å§‹ï¼‰
  python run_anti_detect.py --no-resume
        """
    )
    
    parser.add_argument('--output', default='crawled_data', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--concurrent', type=int, default=2, help='å¹¶å‘æ–‡ç« æ•°ï¼ˆé»˜è®¤2ï¼‰')
    parser.add_argument('--batch', type=int, default=3, help='æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤3ï¼‰')
    parser.add_argument('--limit', type=int, help='é™åˆ¶å¤„ç†æ–‡ç« æ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    parser.add_argument('--api-delay', type=float, default=2.0, help='APIè¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰')
    parser.add_argument('--article-delay', type=float, default=5.0, help='æ–‡ç« å¤„ç†å»¶è¿Ÿï¼ˆç§’ï¼‰')
    # æ³¨æ„ï¼šæœ€å¤§é‡è¯•æ¬¡æ•°ç”±å†…éƒ¨ç­–ç•¥æ§åˆ¶ï¼Œå› æ­¤æ­¤å‚æ•°æš‚ä¸ç”Ÿæ•ˆ
    # parser.add_argument('--max-retries', type=int, default=5, help='æœ€å¤§é‡è¯•æ¬¡æ•°')
    parser.add_argument('--no-resume', action='store_true', help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ ')
    parser.add_argument('--use-proxy', action='store_true', help='ä½¿ç”¨ä»£ç†')
    parser.add_argument('--proxy-url', help='ä»£ç†åœ°å€ï¼ˆå¦‚ http://127.0.0.1:7890ï¼‰')
    
    args = parser.parse_args()
    
    # è¿è¡Œçˆ¬è™«
    asyncio.run(main(args))