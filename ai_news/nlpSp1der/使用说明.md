# 使用说明

本文档提供了Newsletter爬虫系统的完整使用指南，包括所有可执行命令及其功能说明。

## 目录
- [环境配置](#环境配置)
- [基础爬虫](#基础爬虫)
- [优化版爬虫](#优化版爬虫推荐)
- [反爬虫增强版](#反爬虫增强版)
- [OSS上传功能](#oss上传功能)
- [开发调试](#开发调试)
- [输出文件说明](#输出文件说明)

## 环境配置

### 安装依赖
```bash
# 安装Python依赖包
pip install -r requirements.txt

# 安装Playwright浏览器驱动（爬虫必需）
playwright install chromium
```

## 基础爬虫

基础版本爬虫提供简单的顺序处理功能，适合小规模爬取任务。

### 运行命令（稳定入口）
```bash
# 推荐使用稳定入口（已加固，避免后续改动影响）
python main.py crawl --output crawled_data

# 自定义输出目录
python main.py crawl --output my_data
```

### 功能特点
- 顺序处理文章
- 单个浏览器实例
- 基础错误处理
- 简单重试机制

## 优化版爬虫（推荐）

优化版爬虫提供并发处理、断点续传等高级功能，推荐用于生产环境。

### 运行命令
```bash
# 使用默认设置运行
python run_optimized.py

# 自定义并发设置
python run_optimized.py --max-concurrent-articles 10 --max-concurrent-images 30

# 使用配置文件
python run_optimized.py --config config.json

# 禁用断点续传（全新开始）
python run_optimized.py --no-resume

# 性能优化参数
python run_optimized.py --batch-size 20 --api-delay 0.5 --article-delay 0.2
```

### 参数说明
| 参数 | 说明 | 默认值 |
|-----|------|-------|
| `--max-concurrent-articles` | 最大并发文章处理数 | 5 |
| `--max-concurrent-images` | 最大并发图片下载数 | 20 |
| `--batch-size` | 每批处理文章数 | 12 |
| `--api-delay` | API请求间隔（秒） | 1.0 |
| `--article-delay` | 文章处理间隔（秒） | 2.0 |
| `--no-resume` | 禁用断点续传 | False |
| `--config` | 配置文件路径 | config.json |
| `--output-dir` | 输出目录 | crawled_data |

### 功能特点
- **并发处理**：可配置的并发文章处理（默认5个）
- **批量图片下载**：并行下载图片，信号量控制（默认20个并发）
- **断点续传**：自动跟踪进度，中断后可恢复
- **高级错误处理**：指数退避重试机制，详细日志记录
- **资源管理**：浏览器页面池和连接池
- **性能监控**：详细的时间和吞吐量统计

## 反爬虫增强版

专门针对反爬虫策略优化的版本，适用于有严格访问限制的网站。

### 运行命令
```bash
# 基础运行
python run_anti_detect.py

# 自定义输出目录
python run_anti_detect.py --output crawled_anti_detect

# 调整并发和批次设置
python run_anti_detect.py --concurrent 3 --batch 5

# 配置延迟参数
python run_anti_detect.py --api-delay 2.0 --article-delay 3.0

# 使用代理
python run_anti_detect.py --use-proxy --proxy-url http://proxy.example.com:8080

# 禁用断点续传
python run_anti_detect.py --no-resume
```

### 参数说明
| 参数 | 说明 | 默认值 |
|-----|------|-------|
| `--output` | 输出目录 | crawled_data |
| `--concurrent` | 并发文章数 | 3 |
| `--batch` | 批次大小 | 5 |
| `--api-delay` | API请求延迟（秒） | 1.5 |
| `--article-delay` | 文章处理延迟（秒） | 2.5 |
| `--use-proxy` | 启用代理 | False |
| `--proxy-url` | 代理服务器地址 | None |
| `--no-resume` | 禁用断点续传 | False |

### 反爬虫特性
- **智能延迟**：动态调整请求间隔（3-10秒）
- **批次延迟**：批次间额外延迟（20秒）
- **速率限制处理**：遇到限制时暂停60秒
- **浏览器指纹伪装**：随机User-Agent和浏览器特征
- **代理支持**：可配置HTTP/HTTPS代理
- **请求头优化**：模拟真实浏览器行为

## OSS上传功能

将爬取的文章和图片上传到对象存储服务（MinIO/OSS），支持断点续传和批量上传。

### 环境变量配置
```bash
# 必须设置的MinIO认证信息
export MINIO_ACCESS_KEY="your-access-key"
export MINIO_SECRET_KEY="your-secret-key"

# 可选：覆盖配置文件中的endpoint
export MINIO_ENDPOINT="http://localhost:9000"
```

### 基本上传命令
```bash
# 使用默认配置上传所有文章
python main.py upload

# 指定自定义bucket名称
python main.py upload --bucket my-custom-bucket

# 禁用断点续传，重新上传所有文件
python main.py upload --no-resume

# 指定源目录
python main.py upload --source-dir crawled_data
```

### 参数说明
| 参数 | 说明 | 默认值 |
|-----|------|-------|
| `--bucket` | 指定bucket名称 | newsletter-articles-nlp |
| `--no-resume` | 禁用断点续传 | 启用 |
| `--source-dir` | 指定源目录 | crawled_data |

### OSS配置文件
在`config.json`中配置OSS参数：

```json
{
  "oss": {
    "base_url": "http://localhost:9011",           // MinIO服务地址
    "public_base_url": "http://60.205.160.74:9000", // 公网访问地址
    "bucket_name": "newsletter-articles-nlp",      // 默认存储桶名称
    "source_id": "nlp-elvissaravia",               // 数据源标识
    "max_concurrent_uploads": 10,                  // 最大并发上传数
    "upload_timeout": 60,                          // 上传超时时间(秒)
    "retry_attempts": 3,                           // 重试次数
    "chunk_size": 8192                             // 文件块大小
  }
}
```

### 存储结构
上传后的文件在OSS中的组织结构：

```
bucket-name/
└── articles/
    └── nlp-elvissaravia/        # 数据源标识
        └── {article_id}/         # 文章ID目录
            ├── content.md        # Markdown内容
            ├── metadata.json     # 文章元数据
            └── images/           # 图片目录
                ├── cover.jpg     # 封面图片
                └── img_*.jpg     # 内容图片
```

### 功能特点
- **自动Bucket管理**：自动创建bucket并设置公共读权限
- **断点续传**：支持中断后继续上传，跳过已上传文件
- **并发上传**：默认10个并发任务，可配置调整
- **错误重试**：指数退避重试机制，最多重试3次
- **进度跟踪**：实时记录上传进度到`upload_progress.json`
- **公开访问URL**：自动替换文章中的图片路径为公开访问地址

### 上传进度监控
```bash
# 查看上传进度
cat crawled_data/upload_progress.json | python -m json.tool

# 检查上传统计
python -c "
import json
with open('crawled_data/upload_progress.json') as f:
    progress = json.load(f)
print(f'已上传: {len(progress[\"uploaded_articles\"])} 篇文章')
print(f'失败: {len(progress.get(\"failed_articles\", {}))} 篇文章')
"
```

### Python代码调用
```python
import asyncio
from src.newsletter_system.oss.oss_uploader import OSSUploader

async def upload_articles():
    # 创建上传器实例
    uploader = OSSUploader(
        endpoint='http://localhost:9000',
        access_key='your-access-key',
        secret_key='your-secret-key',
        bucket_name='my-bucket',
        source_id='nlp-elvissaravia'
    )
    
    # 上传所有文章
    await uploader.upload_all_articles('crawled_data')

# 运行上传
asyncio.run(upload_articles())
```

### 性能优化建议
- **高带宽网络**：设置`max_concurrent_uploads: 20-50`
- **低带宽网络**：设置`max_concurrent_uploads: 5-10`
- **大文件上传**：增加`upload_timeout`值
- **小文件优化**：减小`chunk_size`值

### 故障排查
| 错误类型 | 症状 | 解决方案 |
|---------|------|----------|
| 认证失败 | `AccessDenied` | 检查`MINIO_ACCESS_KEY/SECRET_KEY` |
| 网络超时 | `Connection timeout` | 增加`upload_timeout`，检查网络 |
| Bucket不存在 | `NoSuchBucket` | 程序会自动创建bucket |
| 权限不足 | `AccessDenied on PUT` | 检查用户权限设置 |
| 文件不存在 | `FileNotFound` | 检查源文件路径，重新爬取 |

## 开发调试

### 语法检查
```bash
# 检查Python语法错误
python -m py_compile src/newsletter_system/crawler/newsletter_crawler.py
python -m py_compile src/newsletter_system/crawler/optimized_crawler.py
python -m py_compile src/newsletter_system/crawler/anti_detect_crawler.py
```

### 调试运行
```bash
# 无缓冲输出，实时查看日志
python -u run.py

# 带详细日志的调试运行
python run_optimized.py --log-level DEBUG
```

### 测试脚本
```bash
# 运行爬虫测试
python src/tests/test_crawler.py

# 运行性能基准测试
python src/tests/benchmark_crawlers.py
```

## 输出文件说明

爬虫运行后会在输出目录（默认`crawled_data/`）生成以下文件结构：

```
crawled_data/
├── articles/              # 文章内容目录
│   └── {article_id}_{title}/
│       ├── content.md     # Markdown格式的文章内容
│       └── metadata.json  # 文章元数据
├── images/                # 图片目录
│   ├── covers/           # 封面图片
│   └── content/          # 文章内容图片
└── data/                 # 数据文件目录
    ├── articles_metadata.json      # 原始API响应数据
    ├── processed_articles.json     # 处理后的完整数据
    ├── recommendation_data.json    # 推荐引擎所需数据
    ├── crawler_progress.json       # 爬虫进度（断点续传）
    └── crawl_stats.json           # 爬取统计信息
```

### 数据文件说明

1. **articles_metadata.json**：从API获取的原始文章元数据
2. **processed_articles.json**：包含处理后的文章内容和本地化图片路径
3. **recommendation_data.json**：精简的推荐引擎数据，包含关键字段
4. **crawler_progress.json**：记录爬取进度，支持断点续传
5. **crawl_stats.json**：统计信息（文章数、成功率、耗时等）

## 配置文件

系统使用`config.json`进行配置管理：

```json
{
  "crawler": {
    "base_url": "https://nlp.elvissaravia.com",
    "api_endpoint": "/api/v1/archive",
    "request_delay": 2,
    "max_retries": 3,
    "timeout": 30,
    "batch_size": 12
  },
  "output": {
    "base_dir": "crawled_data",
    "articles_dir": "articles",
    "images_dir": "images",
    "data_dir": "data"
  },
  "oss": {
    "base_url": "http://localhost:9011",
    "public_base_url": "http://60.205.160.74:9000",
    "bucket_name": "newsletter-articles-nlp",
    "source_id": "nlp-elvissaravia",
    "max_concurrent_uploads": 10,
    "upload_timeout": 60,
    "retry_attempts": 3,
    "chunk_size": 8192
  }
}
```

## 注意事项

1. **速率限制**：爬虫默认遵守2秒延迟，避免对目标服务器造成压力
2. **断点续传**：优化版和反爬虫版支持断点续传，中断后可继续
3. **内存管理**：大数据集采用增量处理和及时保存，防止内存溢出
4. **浏览器管理**：使用异步上下文管理器，自动清理浏览器实例
5. **依赖检查**：所有依赖都有导入保护，缺失包不会导致程序崩溃
6. **UTF-8支持**：目录名包含中文字符，需要终端支持UTF-8编码

## 常见问题

### Q: 如何选择合适的爬虫版本？
- **基础版**：适合小规模、一次性爬取
- **优化版**：适合大规模、需要高性能的场景
- **反爬虫版**：适合有严格访问限制的网站

### Q: 爬虫中断后如何恢复？
优化版和反爬虫版支持断点续传，直接重新运行相同命令即可从中断处继续。如需全新开始，使用`--no-resume`参数。

### Q: 如何调整爬取速度？
通过调整`--api-delay`和`--article-delay`参数控制请求间隔，或使用`--max-concurrent-articles`调整并发数。

### Q: 代理设置失败怎么办？
确保代理服务器可用，格式为`http://host:port`或`https://host:port`，必要时添加认证信息。