#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google AI Blog RSS Spider
çˆ¬å–Google AI Blogçš„RSSè®¢é˜…å†…å®¹å¹¶æŒ‰ç…§æŒ‡å®šå­˜å‚¨ç»“æ„ä¿å­˜
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
import json
import logging
import os
import hashlib
import time
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
import urllib.robotparser

class GoogleAIBlogSpider:
    def __init__(self, base_path: str = None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        Args:
            base_path: æ•°æ®å­˜å‚¨åŸºç¡€è·¯å¾„ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•
        """
        self.base_path = base_path or os.getcwd()
        self.data_path = os.path.join(self.base_path, 'data')
        
        # æ³¨æ„ï¼šGoogle Researchåšå®¢çš„RSSåœ°å€
        self.base_url = 'https://research.google/blog/'
        self.rss_url = 'https://research.google/blog/rss/'
        
        # è®¾ç½®è¯·æ±‚ä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        })
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆ›å»ºç›®å½•ç»“æ„
        self.create_directory_structure()
        
        # åŠ è½½å·²æœ‰ç´¢å¼•
        self.load_article_index()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = os.path.join(self.base_path, 'logs')
        os.makedirs(log_dir, exist_ok=True)
        
        log_file = os.path.join(log_dir, f'google_ai_spider_{datetime.now().strftime("%Y%m%d")}.log')
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger("GoogleAIBlogSpider")
    
    def create_directory_structure(self):
        """åˆ›å»ºç›®å½•ç»“æ„"""
        directories = [
            os.path.join(self.data_path, 'articles'),
            os.path.join(self.data_path, 'images'),
            os.path.join(self.data_path, 'metadata'),
            os.path.join(self.data_path, 'exports', 'daily'),
            os.path.join(self.data_path, 'exports', 'weekly'),
            os.path.join(self.data_path, 'exports', 'monthly'),
            os.path.join(self.base_path, 'logs'),
            os.path.join(self.base_path, 'config')
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
        
        self.logger.info("ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")
    
    def load_article_index(self):
        """åŠ è½½æ–‡ç« ç´¢å¼•"""
        self.index_file = os.path.join(self.data_path, 'metadata', 'article_index.json')
        
        if os.path.exists(self.index_file):
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    self.article_index = json.load(f)
                self.logger.info(f"åŠ è½½å·²æœ‰ç´¢å¼•ï¼Œå…±{self.article_index.get('total_articles', 0)}ç¯‡æ–‡ç« ")
            except Exception as e:
                self.logger.error(f"åŠ è½½ç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")
                self.article_index = self.create_empty_index()
        else:
            self.article_index = self.create_empty_index()
    
    def create_empty_index(self) -> Dict:
        """åˆ›å»ºç©ºç´¢å¼•"""
        return {
            "total_articles": 0,
            "last_update": datetime.now().isoformat(),
            "articles": [],
            "categories": {},
            "monthly_stats": {}
        }
    
    def save_article_index(self):
        """ä¿å­˜æ–‡ç« ç´¢å¼•"""
        self.article_index['last_update'] = datetime.now().isoformat()
        
        try:
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(self.article_index, f, indent=2, ensure_ascii=False)
            self.logger.info("æ–‡ç« ç´¢å¼•å·²ä¿å­˜")
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")
    
    def check_robots_txt(self) -> bool:
        """æ£€æŸ¥robots.txtè§„åˆ™"""
        try:
            rp = urllib.robotparser.RobotFileParser()
            rp.set_url(f"{self.base_url}/robots.txt")
            rp.read()
            return rp.can_fetch('*', self.rss_url)
        except:
            return True  # å¦‚æœæ— æ³•è·å–robots.txtï¼Œé»˜è®¤å…è®¸
    
    def get_rss_feed(self) -> Optional[feedparser.FeedParserDict]:
        """è·å–RSSè®¢é˜…å†…å®¹"""
        try:
            self.logger.info(f"è·å–RSSè®¢é˜…: {self.rss_url}")
            
            # æ£€æŸ¥robots.txt
            if not self.check_robots_txt():
                self.logger.warning("robots.txtä¸å…è®¸è®¿é—®RSS")
                return None
            
            # ä½¿ç”¨feedparserè§£æRSS
            feed = feedparser.parse(self.rss_url)
            
            if feed.bozo:
                self.logger.warning(f"RSSè§£æè­¦å‘Š: {feed.bozo_exception}")
            
            self.logger.info(f"RSSè®¢é˜…ä¸­å‘ç°{len(feed.entries)}ç¯‡æ–‡ç« ")
            return feed
            
        except Exception as e:
            self.logger.error(f"è·å–RSSè®¢é˜…å¤±è´¥: {e}")
            return None
    
    def parse_rss_entries(self, feed: feedparser.FeedParserDict, max_entries: int = 50) -> List[Dict]:
        """è§£æRSSæ¡ç›®"""
        articles = []
        
        for entry in feed.entries[:max_entries]:
            try:
                # åŸºç¡€ä¿¡æ¯
                article_info = {
                    'title': entry.title,
                    'url': entry.link,
                    'summary': entry.get('summary', ''),
                    'publish_date': self.parse_publish_date(entry),
                    'author': self.extract_author_from_entry(entry),
                    'categories': self.extract_categories_from_entry(entry),
                    'source': 'Google Research Blog',
                    'source_type': 'official_blog',
                    'language': 'en'
                }
                
                # ç”Ÿæˆå”¯ä¸€ID
                article_info['article_id'] = self.generate_article_id(article_info)
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if not self.is_article_exists(article_info['article_id']):
                    articles.append(article_info)
                else:
                    self.logger.info(f"æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡: {article_info['title']}")
                
            except Exception as e:
                self.logger.warning(f"è§£æRSSæ¡ç›®å¤±è´¥: {e}")
                continue
        
        return articles
    
    def parse_publish_date(self, entry) -> str:
        """è§£æå‘å¸ƒæ—¥æœŸ"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                return datetime(*entry.published_parsed[:6]).isoformat()
            elif hasattr(entry, 'published'):
                return entry.published
            elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                return datetime(*entry.updated_parsed[:6]).isoformat()
            else:
                return datetime.now().isoformat()
        except:
            return datetime.now().isoformat()
    
    def extract_author_from_entry(self, entry) -> str:
        """ä»RSSæ¡ç›®æå–ä½œè€…ä¿¡æ¯"""
        try:
            if hasattr(entry, 'author'):
                return entry.author
            elif hasattr(entry, 'authors') and entry.authors:
                return ', '.join([author.name for author in entry.authors if hasattr(author, 'name')])
            else:
                return 'Google Research Team'
        except:
            return 'Google Research Team'
    
    def extract_categories_from_entry(self, entry) -> List[str]:
        """ä»RSSæ¡ç›®æå–åˆ†ç±»ä¿¡æ¯"""
        categories = []
        
        try:
            if hasattr(entry, 'tags') and entry.tags:
                categories = [tag.term for tag in entry.tags if hasattr(tag, 'term')]
            elif hasattr(entry, 'category'):
                categories = [entry.category]
        except:
            pass
        
        return categories
    
    def get_article_details(self, article_info: Dict) -> Dict:
        """è·å–æ–‡ç« è¯¦ç»†å†…å®¹"""
        url = article_info.get('url')
        if not url:
            return article_info
        
        try:
            self.logger.info(f"è·å–æ–‡ç« è¯¦æƒ…: {article_info['title']}")
            
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ–‡ç« å†…å®¹
            content = self.extract_article_content(soup)
            if content:
                article_info['content'] = content
                article_info['word_count'] = len(content.split())
                article_info['reading_time_minutes'] = max(1, article_info['word_count'] // 200)
            
            # æå–æ›´è¯¦ç»†çš„å…ƒæ•°æ®
            detailed_metadata = self.extract_detailed_metadata(soup)
            article_info.update(detailed_metadata)
            
            # æå–å›¾ç‰‡ä¿¡æ¯
            images = self.extract_images(soup, article_info['article_id'])
            if images:
                article_info['images'] = images
            
            # æŠ€æœ¯åˆ†æ
            tech_analysis = self.analyze_technical_content(content)
            article_info['technical_analysis'] = tech_analysis
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            article_info['quality_score'] = self.calculate_quality_score(article_info)
            
            # æ·»åŠ çˆ¬å–æ—¶é—´
            article_info['crawl_date'] = datetime.now().isoformat()
            
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ç« è¯¦æƒ…å¤±è´¥ {url}: {e}")
        
        return article_info
    
    def extract_article_content(self, soup: BeautifulSoup) -> str:
        """æå–æ–‡ç« æ­£æ–‡å†…å®¹"""
        # Google Research Blogçš„å†…å®¹ç»“æ„
        content_selectors = [
            'div.post-body',
            'div.entry-content',
            'article .post-content',
            'div.blog-post-content',
            'main article',
            '[role="main"] article'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # æ¸…ç†å†…å®¹
                content = self.clean_article_content(content_elem)
                if len(content) > 100:
                    return content
        
        # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šé€‰æ‹©å™¨ï¼Œå°è¯•é€šç”¨æ–¹æ³•
        article_elem = soup.find('article')
        if article_elem:
            content = self.clean_article_content(article_elem)
            if len(content) > 100:
                return content
        
        return ''
    
    def clean_article_content(self, content_elem) -> str:
        """æ¸…ç†æ–‡ç« å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for tag in content_elem.find_all(['script', 'style', 'nav', 'aside', 'footer', 'header']):
            tag.decompose()
        
        # ç§»é™¤å¹¿å‘Šå’Œåˆ†äº«æŒ‰é’®
        for tag in content_elem.find_all(class_=re.compile(r'ad|share|social|related|sidebar')):
            tag.decompose()
        
        # å¤„ç†ä»£ç å—
        for code_block in content_elem.find_all(['pre', 'code']):
            if code_block.parent.name != 'pre':  # é¿å…é‡å¤å¤„ç†
                code_text = code_block.get_text()
                code_block.string = f"`{code_text}`"
        
        # å¤„ç†é¢„æ ¼å¼åŒ–æ–‡æœ¬
        for pre_block in content_elem.find_all('pre'):
            pre_text = pre_block.get_text()
            pre_block.string = f"\\n```\\n{pre_text}\\n```\\n"
        
        # å¤„ç†é“¾æ¥
        for link in content_elem.find_all('a', href=True):
            href = link.get('href')
            text = link.get_text(strip=True)
            if href and text and href != text:
                link.string = f"[{text}]({href})"
        
        # å¤„ç†å›¾ç‰‡
        for img in content_elem.find_all('img'):
            alt = img.get('alt', '')
            src = img.get('src', '')
            if alt or src:
                img.string = f"![{alt}]({src})"
        
        # è·å–æ–‡æœ¬å†…å®¹
        text = content_elem.get_text(separator='\\n', strip=True)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        lines = [line.strip() for line in text.split('\\n') if line.strip()]
        
        return '\\n\\n'.join(lines)
    
    def extract_detailed_metadata(self, soup: BeautifulSoup) -> Dict:
        """æå–è¯¦ç»†å…ƒæ•°æ®"""
        metadata = {}
        
        # æå–å¤–éƒ¨é“¾æ¥
        external_links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href and not href.startswith('#') and 'research.google' not in href:
                link_type = 'other'
                if 'arxiv.org' in href or 'paper' in href.lower():
                    link_type = 'paper'
                elif 'github.com' in href or 'code' in href.lower():
                    link_type = 'code'
                elif 'demo' in href.lower() or 'colab' in href.lower():
                    link_type = 'demo'
                
                external_links.append({
                    'url': href,
                    'text': link.get_text(strip=True),
                    'type': link_type
                })
        
        metadata['external_links'] = external_links[:20]  # é™åˆ¶æ•°é‡
        
        return metadata
    
    def extract_images(self, soup: BeautifulSoup, article_id: str) -> List[Dict]:
        """æå–æ–‡ç« å›¾ç‰‡"""
        images = []
        
        # æŸ¥æ‰¾æ–‡ç« ä¸­çš„å›¾ç‰‡
        img_tags = soup.find_all('img')
        
        for i, img in enumerate(img_tags):
            src = img.get('src')
            if src:
                # å¤„ç†ç›¸å¯¹URL
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = urljoin(self.base_url, src)
                
                # è·³è¿‡å¾ˆå°çš„å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯å›¾æ ‡ï¼‰
                width = img.get('width')
                height = img.get('height')
                if width and height:
                    try:
                        if int(width) < 50 or int(height) < 50:
                            continue
                    except:
                        pass
                
                image_info = {
                    'url': src,
                    'alt': img.get('alt', ''),
                    'title': img.get('title', ''),
                    'width': width,
                    'height': height,
                    'local_path': ''  # æš‚æ—¶ä¸ºç©ºï¼Œåç»­å¯å®ç°å›¾ç‰‡ä¸‹è½½
                }
                
                images.append(image_info)
        
        return images
    
    def analyze_technical_content(self, content: str) -> Dict:
        """åˆ†ææŠ€æœ¯å†…å®¹"""
        if not content:
            return {}
        
        analysis = {}
        
        # æŠ€æœ¯å…³é”®è¯æ£€æµ‹
        tech_keywords = {
            'machine_learning': ['machine learning', 'ML', 'supervised learning', 'unsupervised learning'],
            'deep_learning': ['deep learning', 'neural network', 'CNN', 'RNN', 'LSTM', 'transformer'],
            'nlp': ['natural language processing', 'NLP', 'language model', 'BERT', 'GPT'],
            'computer_vision': ['computer vision', 'image recognition', 'object detection', 'segmentation'],
            'ai_research': ['artificial intelligence', 'AI', 'research', 'algorithm'],
            'tensorflow': ['TensorFlow', 'TF', 'Keras', 'TensorBoard'],
            'research': ['research', 'paper', 'study', 'experiment', 'dataset'],
            'product': ['product', 'release', 'feature', 'update', 'announcement']
        }
        
        content_lower = content.lower()
        
        for category, keywords in tech_keywords.items():
            count = sum(content_lower.count(keyword.lower()) for keyword in keywords)
            analysis[f'{category}_mentions'] = count
        
        # ç¡®å®šä¸»è¦æŠ€æœ¯é¢†åŸŸ
        if analysis:
            max_category = max(analysis.items(), key=lambda x: x[1])
            analysis['primary_tech_area'] = max_category[0].replace('_mentions', '')
        else:
            analysis['primary_tech_area'] = 'research'
        
        # è®¡ç®—æŠ€æœ¯æ·±åº¦åˆ†æ•°
        technical_indicators = [
            'algorithm', 'model', 'training', 'accuracy', 'performance',
            'dataset', 'benchmark', 'evaluation', 'architecture', 'optimization'
        ]
        
        tech_depth_score = sum(content_lower.count(indicator) for indicator in technical_indicators)
        analysis['technical_depth_score'] = min(tech_depth_score / 5, 10)  # å½’ä¸€åŒ–åˆ°0-10
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä»£ç 
        code_indicators = ['```', 'import ', 'def ', 'class ', 'function', 'code', '`']
        analysis['contains_code'] = any(indicator in content for indicator in code_indicators)
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«æ•°å­¦å…¬å¼
        math_indicators = ['equation', 'formula', 'âˆ‘', 'âˆ«', 'âˆ‚', 'matrix', '$']
        analysis['contains_math'] = any(indicator in content for indicator in math_indicators)
        
        # æå–å…³é”®è¯
        keywords = []
        for category, kws in tech_keywords.items():
            for kw in kws:
                if kw.lower() in content_lower:
                    keywords.append(kw)
        
        analysis['keywords'] = list(set(keywords))[:10]  # å»é‡å¹¶é™åˆ¶æ•°é‡
        
        return analysis
    
    def calculate_quality_score(self, article: Dict) -> float:
        """è®¡ç®—æ–‡ç« è´¨é‡åˆ†æ•°"""
        score = 7.0  # Google ResearchåŸºç¡€åˆ†æ•°
        
        # æŠ€æœ¯æ·±åº¦åŠ åˆ†
        tech_analysis = article.get('technical_analysis', {})
        tech_depth = tech_analysis.get('technical_depth_score', 0)
        score += min(tech_depth * 0.3, 2.0)
        
        # å†…å®¹é•¿åº¦åŠ åˆ†
        word_count = article.get('word_count', 0)
        if word_count > 2000:
            score += 1.5
        elif word_count > 1000:
            score += 1.0
        elif word_count > 500:
            score += 0.5
        
        # åŒ…å«ä»£ç æˆ–æ•°å­¦å…¬å¼åŠ åˆ†
        if tech_analysis.get('contains_code'):
            score += 0.5
        if tech_analysis.get('contains_math'):
            score += 0.3
        
        # å›¾ç‰‡ä¸°å¯Œåº¦
        images = article.get('images', [])
        if len(images) > 5:
            score += 0.7
        elif len(images) > 2:
            score += 0.3
        
        # å¤–éƒ¨é“¾æ¥ä¸°å¯Œåº¦
        external_links = article.get('external_links', [])
        paper_links = [link for link in external_links if link.get('type') == 'paper']
        code_links = [link for link in external_links if link.get('type') == 'code']
        
        if paper_links:
            score += 0.5
        if code_links:
            score += 0.3
        
        return min(score, 10.0)
    
    def generate_article_id(self, article: Dict) -> str:
        """ç”Ÿæˆæ–‡ç« å”¯ä¸€ID"""
        # ä»å‘å¸ƒæ—¥æœŸè·å–æ—¥æœŸéƒ¨åˆ†
        try:
            pub_date = datetime.fromisoformat(article['publish_date'].replace('Z', '+00:00'))
            date_str = pub_date.strftime('%Y%m%d')
        except:
            date_str = datetime.now().strftime('%Y%m%d')
        
        # ä½¿ç”¨æ ‡é¢˜å’ŒURLç”Ÿæˆå“ˆå¸Œ
        content = f"{article.get('title', '')}{article.get('url', '')}"
        hash_part = hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
        
        return f"google_ai_{date_str}_{hash_part}"
    
    def is_article_exists(self, article_id: str) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨"""
        for article in self.article_index.get('articles', []):
            if article.get('article_id') == article_id:
                return True
        return False
    
    def save_article(self, article: Dict) -> bool:
        """ä¿å­˜æ–‡ç« åˆ°æœ¬åœ°"""
        try:
            # è·å–æ—¥æœŸä¿¡æ¯
            pub_date = datetime.fromisoformat(article['publish_date'].replace('Z', '+00:00'))
            year = pub_date.strftime('%Y')
            month = pub_date.strftime('%m')
            
            # åˆ›å»ºå¹´æœˆç›®å½•
            article_dir = os.path.join(self.data_path, 'articles', year, month)
            os.makedirs(article_dir, exist_ok=True)
            
            article_id = article['article_id']
            
            # ä¿å­˜JSONæ–‡ä»¶
            json_file = os.path.join(article_dir, f"{article_id}.json")
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(article, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜Markdownæ–‡ä»¶
            md_file = os.path.join(article_dir, f"{article_id}.md")
            self.save_article_as_markdown(article, md_file)
            
            # æ›´æ–°ç´¢å¼•
            self.update_article_index(article, f"data/articles/{year}/{month}/{article_id}.json")
            
            self.logger.info(f"æ–‡ç« å·²ä¿å­˜: {article['title']}")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–‡ç« å¤±è´¥: {e}")
            return False
    
    def save_article_as_markdown(self, article: Dict, file_path: str):
        """å°†æ–‡ç« ä¿å­˜ä¸ºMarkdownæ ¼å¼"""
        md_content = f"""---
title: "{article.get('title', '')}"
author: "{article.get('author', '')}"
date: "{article.get('publish_date', '').split('T')[0]}"
source: "{article.get('source', '')}"
url: "{article.get('url', '')}"
categories: {json.dumps(article.get('categories', []))}
tags: {json.dumps(article.get('technical_analysis', {}).get('keywords', []))}
quality_score: {article.get('quality_score', 0)}
---

# {article.get('title', '')}

**ä½œè€…**: {article.get('author', '')}  
**å‘å¸ƒæ—¶é—´**: {article.get('publish_date', '').split('T')[0]}  
**æ¥æº**: [{article.get('source', '')}]({article.get('url', '')})  
**åˆ†ç±»**: {', '.join(article.get('categories', []))}  

## æ‘˜è¦
{article.get('summary', '')}

## æ­£æ–‡
{article.get('content', '')}

## ç›¸å…³é“¾æ¥
"""
        
        # æ·»åŠ å¤–éƒ¨é“¾æ¥
        external_links = article.get('external_links', [])
        if external_links:
            for link in external_links:
                link_type = link.get('type', 'other')
                emoji = {'paper': 'ğŸ“„', 'code': 'ğŸ’»', 'demo': 'ğŸ®'}.get(link_type, 'ğŸ”—')
                md_content += f"- {emoji} [{link.get('text', 'Link')}]({link.get('url', '')})\n"
        else:
            md_content += "æš‚æ— ç›¸å…³é“¾æ¥\n"
        
        # æ·»åŠ æŠ€æœ¯åˆ†æ
        tech_analysis = article.get('technical_analysis', {})
        md_content += f"""
## æŠ€æœ¯åˆ†æ
- **ä¸»è¦æŠ€æœ¯é¢†åŸŸ**: {tech_analysis.get('primary_tech_area', 'N/A')}
- **æŠ€æœ¯æ·±åº¦**: {tech_analysis.get('technical_depth_score', 0):.1f}/10
- **åŒ…å«ä»£ç **: {'æ˜¯' if tech_analysis.get('contains_code') else 'å¦'}
- **åŒ…å«æ•°å­¦å…¬å¼**: {'æ˜¯' if tech_analysis.get('contains_math') else 'å¦'}
- **å…³é”®è¯**: {', '.join(tech_analysis.get('keywords', []))}
"""
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
    
    def update_article_index(self, article: Dict, file_path: str):
        """æ›´æ–°æ–‡ç« ç´¢å¼•"""
        # æ·»åŠ åˆ°æ–‡ç« åˆ—è¡¨
        article_summary = {
            'article_id': article['article_id'],
            'title': article['title'],
            'date': article['publish_date'].split('T')[0],
            'categories': article.get('categories', []),
            'quality_score': article.get('quality_score', 0),
            'file_path': file_path
        }
        
        self.article_index['articles'].append(article_summary)
        self.article_index['total_articles'] = len(self.article_index['articles'])
        
        # æ›´æ–°åˆ†ç±»ç»Ÿè®¡
        for category in article.get('categories', []):
            if category in self.article_index['categories']:
                self.article_index['categories'][category] += 1
            else:
                self.article_index['categories'][category] = 1
        
        # æ›´æ–°æœˆåº¦ç»Ÿè®¡
        month_key = article['publish_date'][:7]  # YYYY-MM
        if month_key in self.article_index['monthly_stats']:
            self.article_index['monthly_stats'][month_key] += 1
        else:
            self.article_index['monthly_stats'][month_key] = 1
    
    def crawl_latest_articles(self, max_articles: int = 20) -> List[Dict]:
        """çˆ¬å–æœ€æ–°æ–‡ç« """
        self.logger.info("å¼€å§‹çˆ¬å–æœ€æ–°æ–‡ç« ")
        
        # è·å–RSSè®¢é˜…
        feed = self.get_rss_feed()
        if not feed:
            self.logger.error("æ— æ³•è·å–RSSè®¢é˜…")
            return []
        
        # è§£æRSSæ¡ç›®
        articles = self.parse_rss_entries(feed, max_articles)
        self.logger.info(f"å‘ç°{len(articles)}ç¯‡æ–°æ–‡ç« ")
        
        # è·å–è¯¦ç»†å†…å®¹å¹¶ä¿å­˜
        saved_articles = []
        for article in articles:
            try:
                # è·å–è¯¦ç»†å†…å®¹
                detailed_article = self.get_article_details(article)
                
                # ä¿å­˜æ–‡ç« 
                if self.save_article(detailed_article):
                    saved_articles.append(detailed_article)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(2)
                
            except Exception as e:
                self.logger.error(f"å¤„ç†æ–‡ç« å¤±è´¥: {e}")
                continue
        
        # ä¿å­˜æ›´æ–°çš„ç´¢å¼•
        self.save_article_index()
        
        self.logger.info(f"æˆåŠŸä¿å­˜{len(saved_articles)}ç¯‡æ–‡ç« ")
        return saved_articles

def main():
    """ä¸»å‡½æ•°"""
    spider = GoogleAIBlogSpider()
    
    # çˆ¬å–æœ€æ–°æ–‡ç« 
    articles = spider.crawl_latest_articles(max_articles=10)
    
    print(f"æˆåŠŸçˆ¬å–å¹¶ä¿å­˜äº† {len(articles)} ç¯‡æ–‡ç« ")
    
    # æ‰“å°æ–‡ç« åˆ—è¡¨
    for article in articles:
        print(f"- {article['title']} (è´¨é‡åˆ†æ•°: {article.get('quality_score', 0):.1f})")

if __name__ == "__main__":
    main()