#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Google Research Blog ä¸€é”®è¿è¡Œè„šæœ¬
æä¾›ç®€å•çš„å‘½ä»¤è¡Œç•Œé¢æ¥è¿è¡Œçˆ¬è™«
"""

import sys
import json
import time
import logging
from pathlib import Path
from datetime import datetime
from spider import GoogleResearchSpider

class CrawlerRunner:
    def __init__(self):
        self.spider = None
        self.setup_logging()
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def check_dependencies(self):
        """æ£€æŸ¥ä¾èµ–åŒ…"""
        required_packages = ['feedparser', 'requests', 'beautifulsoup4']
        missing_packages = []
        
        for package in required_packages:
            try:
                __import__(package)
            except ImportError:
                missing_packages.append(package)
        
        if missing_packages:
            print("âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…:")
            for package in missing_packages:
                print(f"   - {package}")
            print("\\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
            print("pip install -r requirements.txt")
            return False
        
        print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
        return True
    
    def show_welcome(self):
        """æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯"""
        print("=" * 60)
        print("ğŸ¤– Google Research Blog RSS çˆ¬è™«")
        print("=" * 60)
        print("ğŸ“– è‡ªåŠ¨çˆ¬å–æœ€æ–°çš„AIç ”ç©¶æ–‡ç« ")
        print("ğŸ’¾ æŒ‰æ–‡ç« åˆ†åˆ«ä¿å­˜å†…å®¹å’Œåª’ä½“æ–‡ä»¶")
        print("ğŸ”§ æ”¯æŒè‡ªå®šä¹‰é…ç½®å’Œè¿‡æ»¤")
        print("=" * 60)
    
    def show_config_info(self):
        """æ˜¾ç¤ºé…ç½®ä¿¡æ¯"""
        try:
            spider = GoogleResearchSpider()
            config = spider.config
            
            print("\\nğŸ“‹ å½“å‰é…ç½®:")
            print(f"   RSSæº: {config['rss']['url']}")
            print(f"   æ•°æ®ç›®å½•: {config['storage']['data_directory']}")
            print(f"   æœ€å¤§æ–‡ç« æ•°: {config['crawler']['max_articles_per_run']}")
            print(f"   ä¸‹è½½åª’ä½“: {'æ˜¯' if config['media']['download_enabled'] else 'å¦'}")
            print(f"   è¯·æ±‚å»¶è¿Ÿ: {config['crawler']['request_delay']} ç§’")
            
        except Exception as e:
            print(f"âŒ è¯»å–é…ç½®å¤±è´¥: {e}")
    
    def run_crawler(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            print("\\nğŸš€ å¼€å§‹çˆ¬å–...")
            start_time = time.time()
            
            # åˆ›å»ºçˆ¬è™«å®ä¾‹
            spider = GoogleResearchSpider()
            
            # æ‰§è¡Œçˆ¬å–
            articles = spider.crawl()
            
            end_time = time.time()
            duration = end_time - start_time
            
            # æ˜¾ç¤ºç»“æœ
            print("\\n" + "=" * 60)
            print("ğŸ“Š çˆ¬å–ç»“æœ")
            print("=" * 60)
            print(f"â±ï¸  è€—æ—¶: {duration:.2f} ç§’")
            print(f"ğŸ“„ æ–°å¢æ–‡ç« : {len(articles)} ç¯‡")
            print(f"ğŸ“ ä¿å­˜ç›®å½•: {spider.data_dir}")
            
            if articles:
                print("\\nğŸ“š æ–‡ç« åˆ—è¡¨:")
                for i, article in enumerate(articles, 1):
                    print(f"   {i}. {article['title']}")
                    print(f"      ğŸ“… {article.get('published_date', 'N/A')}")
                    print(f"      ğŸ“ {article['word_count']} å­—")
                    if article.get('images'):
                        print(f"      ğŸ–¼ï¸  {len(article['images'])} å¼ å›¾ç‰‡")
                    print()
            else:
                print("\\nğŸ’¡ æ²¡æœ‰å‘ç°æ–°æ–‡ç« ")
            
            return True
            
        except KeyboardInterrupt:
            print("\\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­çˆ¬å–")
            return False
        except Exception as e:
            print(f"\\nâŒ çˆ¬å–å¤±è´¥: {e}")
            self.logger.exception("çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸")
            return False
    
    def show_data_summary(self):
        """æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡"""
        try:
            data_dir = Path("crawled_data")
            if not data_dir.exists():
                print("\\nğŸ’¡ æš‚æ— çˆ¬å–æ•°æ®")
                return
            
            article_dirs = [d for d in data_dir.iterdir() if d.is_dir() and d.name.startswith('article_')]
            
            if not article_dirs:
                print("\\nğŸ’¡ æš‚æ— çˆ¬å–æ•°æ®")
                return
            
            print("\\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print(f"   ğŸ“ æ€»æ–‡ç« æ•°: {len(article_dirs)}")
            
            # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
            total_md = 0
            total_json = 0
            total_media = 0
            
            for article_dir in article_dirs:
                if (article_dir / "content.md").exists():
                    total_md += 1
                if (article_dir / "metadata.json").exists():
                    total_json += 1
                
                media_dir = article_dir / "media"
                if media_dir.exists():
                    media_files = list(media_dir.iterdir())
                    total_media += len(media_files)
            
            print(f"   ğŸ“ Markdownæ–‡ä»¶: {total_md}")
            print(f"   ğŸ“‹ å…ƒæ•°æ®æ–‡ä»¶: {total_json}")
            print(f"   ğŸ–¼ï¸  åª’ä½“æ–‡ä»¶: {total_media}")
            
            # æœ€æ–°æ–‡ç« 
            if article_dirs:
                latest_dir = max(article_dirs, key=lambda x: x.stat().st_mtime)
                metadata_file = latest_dir / "metadata.json"
                if metadata_file.exists():
                    try:
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        print(f"   ğŸ†• æœ€æ–°æ–‡ç« : {metadata.get('title', 'N/A')}")
                    except:
                        pass
            
        except Exception as e:
            print(f"âŒ è¯»å–æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
    
    def interactive_mode(self):
        """äº¤äº’æ¨¡å¼"""
        while True:
            print("\\n" + "=" * 40)
            print("ğŸ¤– Google Research Blog çˆ¬è™«")
            print("=" * 40)
            print("1. ğŸš€ å¼€å§‹çˆ¬å–")
            print("2. ğŸ“Š æŸ¥çœ‹æ•°æ®ç»Ÿè®¡")
            print("3. âš™ï¸  æŸ¥çœ‹é…ç½®")
            print("4. ğŸ”§ æ£€æŸ¥ä¾èµ–")
            print("5. âŒ é€€å‡º")
            print("=" * 40)
            
            try:
                choice = input("è¯·é€‰æ‹©æ“ä½œ (1-5): ").strip()
                
                if choice == '1':
                    self.run_crawler()
                elif choice == '2':
                    self.show_data_summary()
                elif choice == '3':
                    self.show_config_info()
                elif choice == '4':
                    self.check_dependencies()
                elif choice == '5':
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                else:
                    print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")
                    
            except KeyboardInterrupt:
                print("\\nğŸ‘‹ å†è§ï¼")
                break
            except EOFError:
                print("\\nğŸ‘‹ å†è§ï¼")
                break
    
    def run(self):
        """ä¸»è¿è¡Œæ–¹æ³•"""
        self.show_welcome()
        
        # æ£€æŸ¥ä¾èµ–
        if not self.check_dependencies():
            return False
        
        # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®šè¿è¡Œæ¨¡å¼
        if len(sys.argv) > 1:
            command = sys.argv[1].lower()
            
            if command in ['run', 'crawl', 'start']:
                self.show_config_info()
                return self.run_crawler()
            elif command in ['status', 'stats', 'summary']:
                self.show_data_summary()
                return True
            elif command in ['config', 'conf']:
                self.show_config_info()
                return True
            elif command in ['check', 'deps']:
                return self.check_dependencies()
            elif command in ['help', '-h', '--help']:
                self.show_help()
                return True
            else:
                print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
                self.show_help()
                return False
        else:
            # äº¤äº’æ¨¡å¼
            self.interactive_mode()
            return True
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        print("\\nğŸ“– ä½¿ç”¨è¯´æ˜:")
        print("   python run_crawler.py          # äº¤äº’æ¨¡å¼")
        print("   python run_crawler.py run      # ç›´æ¥è¿è¡Œçˆ¬è™«")
        print("   python run_crawler.py status   # æŸ¥çœ‹æ•°æ®ç»Ÿè®¡")
        print("   python run_crawler.py config   # æŸ¥çœ‹é…ç½®")
        print("   python run_crawler.py check    # æ£€æŸ¥ä¾èµ–")
        print("   python run_crawler.py help     # æ˜¾ç¤ºå¸®åŠ©")

def main():
    """ä¸»å‡½æ•°"""
    runner = CrawlerRunner()
    success = runner.run()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()