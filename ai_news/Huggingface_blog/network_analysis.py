#!/usr/bin/env python3
"""
Hugging Face Blog Network Analysis Tool
ä½¿ç”¨Playwrightç›‘æ§å’Œåˆ†æç½‘ç»œè¯·æ±‚
"""

import asyncio
import json
import re
from datetime import datetime
from urllib.parse import urlparse, parse_qs
from playwright.async_api import async_playwright

class NetworkAnalyzer:
    def __init__(self):
        self.requests = []
        self.responses = []
        self.api_endpoints = []
        self.xhr_requests = []
        
    def parse_request(self, request):
        """è§£æè¯·æ±‚ä¿¡æ¯"""
        url_parts = urlparse(request.url)
        
        request_info = {
            'timestamp': datetime.now().isoformat(),
            'method': request.method,
            'url': request.url,
            'domain': url_parts.netloc,
            'path': url_parts.path,
            'query_params': parse_qs(url_parts.query),
            'headers': dict(request.headers),
            'resource_type': request.resource_type,
            'post_data': request.post_data if request.post_data else None
        }
        
        return request_info
    
    async def parse_response(self, response):
        """è§£æå“åº”ä¿¡æ¯"""
        try:
            content_type = response.headers.get('content-type', '')
            
            response_info = {
                'url': response.url,
                'status': response.status,
                'headers': dict(response.headers),
                'content_type': content_type,
                'size': len(await response.body()) if response.ok else 0
            }
            
            # å¦‚æœæ˜¯JSONå“åº”ï¼Œå°è¯•è§£æå†…å®¹
            if 'application/json' in content_type and response.ok:
                try:
                    response_info['json_data'] = await response.json()
                except:
                    response_info['json_data'] = None
            
            return response_info
        except Exception as e:
            return {
                'url': response.url,
                'status': response.status,
                'error': str(e)
            }
    
    def is_api_request(self, request_info):
        """åˆ¤æ–­æ˜¯å¦ä¸ºAPIè¯·æ±‚"""
        # æ£€æŸ¥URLç‰¹å¾
        url = request_info['url'].lower()
        path = request_info['path'].lower()
        
        api_indicators = [
            '/api/',
            'application/json' in request_info['headers'].get('accept', '').lower(),
            request_info['resource_type'] in ['xhr', 'fetch'],
            'json' in path,
            'graphql' in path,
        ]
        
        return any(api_indicators)

async def analyze_huggingface_blog():
    """åˆ†æHugging Faceåšå®¢çš„ç½‘ç»œè¯·æ±‚"""
    analyzer = NetworkAnalyzer()
    
    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        page = await context.new_page()
        
        # è®¾ç½®ç½‘ç»œç›‘å¬å™¨
        def handle_request(request):
            request_info = analyzer.parse_request(request)
            analyzer.requests.append(request_info)
            
            if analyzer.is_api_request(request_info):
                analyzer.api_endpoints.append(request_info)
                print(f"ğŸ” å‘ç°APIè¯·æ±‚: {request.method} {request.url}")
        
        async def handle_response(response):
            response_info = await analyzer.parse_response(response)
            analyzer.responses.append(response_info)
            
            # å¦‚æœæ˜¯APIå“åº”ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
            for req in analyzer.api_endpoints:
                if req['url'] == response.url:
                    print(f"ğŸ“Š APIå“åº”: {response.status} - {response.url}")
                    if response_info.get('json_data'):
                        print(f"   JSONæ•°æ®ç±»å‹: {type(response_info['json_data'])}")
                        if isinstance(response_info['json_data'], dict):
                            print(f"   JSONé”®: {list(response_info['json_data'].keys())}")
        
        page.on("request", handle_request)
        page.on("response", handle_response)
        
        print("ğŸš€ å¼€å§‹åˆ†æHugging Faceåšå®¢é¡µé¢...")
        
        # 1. è®¿é—®ä¸»é¡µé¢
        print("\nğŸ“– 1. è®¿é—®åšå®¢ä¸»é¡µ...")
        await page.goto("https://huggingface.co/blog/zh", wait_until="networkidle")
        await page.wait_for_timeout(3000)
        
        # 2. ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½å¹¶å°è¯•æ‰¾åˆ°åˆ†é¡µå…ƒç´ 
        print("\nğŸ”„ 2. æŸ¥æ‰¾åˆ†é¡µå…ƒç´ ...")
        try:
            # æŸ¥æ‰¾åˆ†é¡µæŒ‰é’®æˆ–é“¾æ¥
            pagination_selectors = [
                'a[href*="page"]',
                'button[aria-label*="next"]',
                'a[aria-label*="next"]',
                '.pagination a',
                '[class*="pagination"] a',
                'a[href*="offset"]',
                'a[href*="cursor"]'
            ]
            
            pagination_found = False
            for selector in pagination_selectors:
                elements = await page.query_selector_all(selector)
                if elements:
                    print(f"   æ‰¾åˆ°åˆ†é¡µå…ƒç´ : {selector}")
                    pagination_found = True
                    break
            
            if not pagination_found:
                print("   æœªæ‰¾åˆ°æ˜æ˜¾çš„åˆ†é¡µå…ƒç´ ï¼Œæ£€æŸ¥æ»šåŠ¨åŠ è½½...")
                # å°è¯•æ»šåŠ¨çœ‹æ˜¯å¦æœ‰æ‡’åŠ è½½
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await page.wait_for_timeout(2000)
                
        except Exception as e:
            print(f"   åˆ†é¡µæ£€æŸ¥å‡ºé”™: {e}")
        
        # 3. ç‚¹å‡»è¿›å…¥ä¸€ç¯‡æ–‡ç« 
        print("\nğŸ“° 3. è¿›å…¥æ–‡ç« è¯¦æƒ…é¡µ...")
        try:
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
            article_selectors = [
                'a[href*="/blog/"]',
                'article a',
                '.blog-post a',
                'h2 a',
                'h3 a'
            ]
            
            article_link = None
            for selector in article_selectors:
                links = await page.query_selector_all(selector)
                for link in links:
                    href = await link.get_attribute('href')
                    if href and '/blog/' in href and href != '/blog/zh':
                        article_link = link
                        break
                if article_link:
                    break
            
            if article_link:
                href = await article_link.get_attribute('href')
                print(f"   ç‚¹å‡»æ–‡ç« : {href}")
                await article_link.click()
                await page.wait_for_load_state("networkidle")
                await page.wait_for_timeout(2000)
            else:
                print("   æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥")
                
        except Exception as e:
            print(f"   è¿›å…¥æ–‡ç« è¯¦æƒ…é¡µå‡ºé”™: {e}")
        
        # 4. åˆ†æConsoleæ—¥å¿—
        print("\nğŸ” 4. æ£€æŸ¥é¡µé¢æ§åˆ¶å°...")
        logs = []
        
        def handle_console(msg):
            logs.append({
                'type': msg.type,
                'text': msg.text,
                'location': msg.location
            })
        
        page.on("console", handle_console)
        await page.wait_for_timeout(1000)
        
        # 5. æ‰§è¡Œä¸€äº›JavaScriptæ¥è·å–æ›´å¤šä¿¡æ¯
        print("\nğŸ”¬ 5. æ‰§è¡Œé¡µé¢åˆ†æè„šæœ¬...")
        page_info = await page.evaluate("""
            () => {
                const info = {
                    url: window.location.href,
                    title: document.title,
                    meta_tags: [],
                    scripts: [],
                    api_related_elements: []
                };
                
                // è·å–metaæ ‡ç­¾
                document.querySelectorAll('meta').forEach(meta => {
                    info.meta_tags.push({
                        name: meta.name || meta.property,
                        content: meta.content
                    });
                });
                
                // è·å–scriptæ ‡ç­¾
                document.querySelectorAll('script[src]').forEach(script => {
                    info.scripts.push(script.src);
                });
                
                // æŸ¥æ‰¾å¯èƒ½çš„APIç›¸å…³å…ƒç´ 
                const apiElements = document.querySelectorAll('[data-api], [data-endpoint], [data-graphql]');
                apiElements.forEach(el => {
                    info.api_related_elements.push({
                        tagName: el.tagName,
                        attributes: Object.fromEntries([...el.attributes].map(attr => [attr.name, attr.value]))
                    });
                });
                
                return info;
            }
        """)
        
        await browser.close()
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        await generate_analysis_report(analyzer, page_info, logs)

async def generate_analysis_report(analyzer, page_info, logs):
    """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
    
    report = {
        'analysis_time': datetime.now().isoformat(),
        'summary': {
            'total_requests': len(analyzer.requests),
            'total_responses': len(analyzer.responses),
            'api_requests': len(analyzer.api_endpoints),
            'page_info': page_info
        },
        'api_endpoints': [],
        'network_patterns': {},
        'recommendations': []
    }
    
    print("\n" + "="*80)
    print("ğŸ“Š HUGGING FACE åšå®¢ç½‘ç»œåˆ†ææŠ¥å‘Š")
    print("="*80)
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡æ¦‚è§ˆ:")
    print(f"   æ€»è¯·æ±‚æ•°: {report['summary']['total_requests']}")
    print(f"   æ€»å“åº”æ•°: {report['summary']['total_responses']}")
    print(f"   APIè¯·æ±‚æ•°: {report['summary']['api_requests']}")
    print(f"   å½“å‰é¡µé¢: {page_info['url']}")
    
    # åˆ†æAPIç«¯ç‚¹
    print(f"\nğŸ” å‘ç°çš„APIç«¯ç‚¹:")
    if analyzer.api_endpoints:
        for api_req in analyzer.api_endpoints:
            print(f"\n   ğŸ“¡ {api_req['method']} {api_req['url']}")
            print(f"      åŸŸå: {api_req['domain']}")
            print(f"      è·¯å¾„: {api_req['path']}")
            print(f"      èµ„æºç±»å‹: {api_req['resource_type']}")
            
            if api_req['query_params']:
                print(f"      æŸ¥è¯¢å‚æ•°: {api_req['query_params']}")
            
            # æŸ¥æ‰¾å¯¹åº”çš„å“åº”
            for resp in analyzer.responses:
                if resp['url'] == api_req['url']:
                    print(f"      å“åº”çŠ¶æ€: {resp['status']}")
                    print(f"      å†…å®¹ç±»å‹: {resp.get('content_type', 'unknown')}")
                    if resp.get('json_data'):
                        print(f"      JSONæ•°æ®ç»“æ„: {type(resp['json_data'])}")
                        if isinstance(resp['json_data'], dict):
                            print(f"      JSONå­—æ®µ: {list(resp['json_data'].keys())[:10]}")
                    break
            
            report['api_endpoints'].append({
                'method': api_req['method'],
                'url': api_req['url'],
                'domain': api_req['domain'],
                'path': api_req['path'],
                'query_params': api_req['query_params'],
                'headers': api_req['headers']
            })
    else:
        print("   âš ï¸  æœªå‘ç°æ˜æ˜¾çš„APIç«¯ç‚¹")
    
    # åˆ†æåŸŸåæ¨¡å¼
    print(f"\nğŸŒ åŸŸååˆ†æ:")
    domains = {}
    for req in analyzer.requests:
        domain = req['domain']
        domains[domain] = domains.get(domain, 0) + 1
    
    for domain, count in sorted(domains.items(), key=lambda x: x[1], reverse=True):
        print(f"   {domain}: {count} ä¸ªè¯·æ±‚")
    
    # åˆ†æèµ„æºç±»å‹
    print(f"\nğŸ“ èµ„æºç±»å‹åˆ†æ:")
    resource_types = {}
    for req in analyzer.requests:
        res_type = req['resource_type']
        resource_types[res_type] = resource_types.get(res_type, 0) + 1
    
    for res_type, count in sorted(resource_types.items(), key=lambda x: x[1], reverse=True):
        print(f"   {res_type}: {count} ä¸ªè¯·æ±‚")
    
    # æŸ¥æ‰¾GraphQLç›¸å…³è¯·æ±‚
    print(f"\nğŸ” GraphQL/ç‰¹æ®ŠAPIæ£€æŸ¥:")
    graphql_found = False
    for req in analyzer.requests:
        if 'graphql' in req['url'].lower() or 'graphql' in req.get('post_data', '').lower():
            print(f"   å‘ç°GraphQLè¯·æ±‚: {req['url']}")
            graphql_found = True
    
    if not graphql_found:
        print("   æœªå‘ç°GraphQLè¯·æ±‚")
    
    # åˆ†æå¯èƒ½çš„åšå®¢APIæ¨¡å¼
    print(f"\nğŸ“ åšå®¢APIæ¨¡å¼åˆ†æ:")
    blog_patterns = []
    for req in analyzer.requests:
        url = req['url']
        if any(pattern in url.lower() for pattern in ['blog', 'post', 'article', 'content']):
            blog_patterns.append(url)
    
    if blog_patterns:
        print("   å‘ç°å¯èƒ½çš„åšå®¢ç›¸å…³è¯·æ±‚:")
        for pattern in blog_patterns[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"     {pattern}")
    else:
        print("   æœªå‘ç°æ˜æ˜¾çš„åšå®¢APIæ¨¡å¼")
    
    # ç”Ÿæˆå»ºè®®
    print(f"\nğŸ’¡ åˆ†æå»ºè®®:")
    
    if len(analyzer.api_endpoints) == 0:
        print("   1. è¯¥ç½‘ç«™å¯èƒ½ä¸»è¦ä½¿ç”¨æœåŠ¡ç«¯æ¸²æŸ“(SSR)")
        print("   2. å»ºè®®ä½¿ç”¨HTMLè§£ææ–¹å¼è·å–åšå®¢å†…å®¹")
        print("   3. å¯ä»¥å°è¯•æŸ¥çœ‹é¡µé¢æºç ä¸­çš„å†…åµŒJSONæ•°æ®")
        report['recommendations'].extend([
            "ä½¿ç”¨HTMLè§£ææ–¹å¼",
            "æ£€æŸ¥é¡µé¢å†…åµŒJSONæ•°æ®",
            "è€ƒè™‘ä½¿ç”¨Playwrightè¿›è¡ŒåŠ¨æ€å†…å®¹è·å–"
        ])
    else:
        print("   1. å‘ç°äº†APIç«¯ç‚¹ï¼Œå¯ä»¥è€ƒè™‘ç›´æ¥ä½¿ç”¨API")
        print("   2. éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•APIçš„ç¨³å®šæ€§å’Œå®Œæ•´æ€§")
        print("   3. æ£€æŸ¥APIæ˜¯å¦éœ€è¦ç‰¹æ®Šçš„è®¤è¯æˆ–é™æµå¤„ç†")
        report['recommendations'].extend([
            "ä¼˜å…ˆä½¿ç”¨å‘ç°çš„APIç«¯ç‚¹",
            "æµ‹è¯•APIç¨³å®šæ€§",
            "æ£€æŸ¥APIè®¤è¯è¦æ±‚"
        ])
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    with open('/home/shan/Huggingface_blog/network_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: /home/shan/Huggingface_blog/network_analysis_report.json")
    
    return report

if __name__ == "__main__":
    asyncio.run(analyze_huggingface_blog())