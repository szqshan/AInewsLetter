#!/usr/bin/env python3
"""
Hugging Face Blog ç½‘ç»œåˆ†æå·¥å…· - ç®€åŒ–ç‰ˆæœ¬
ä½¿ç”¨requestså’Œæµè§ˆå™¨å¼€å‘è€…å·¥å…·æ–¹æ³•åˆ†æç½‘ç»œè¯·æ±‚
"""

import json
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time

class HuggingFaceAnalyzer:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive'
        })
        self.base_url = "https://huggingface.co"
        self.api_requests = []
        self.all_requests = []
        
    def analyze_page_structure(self, url):
        """åˆ†æé¡µé¢ç»“æ„å’Œå†…åµŒæ•°æ®"""
        print(f"ğŸ” åˆ†æé¡µé¢: {url}")
        
        try:
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            analysis = {
                'url': url,
                'status_code': response.status_code,
                'headers': dict(response.headers),
                'page_title': soup.title.string if soup.title else None,
                'scripts': [],
                'json_data': [],
                'api_hints': [],
                'meta_tags': [],
                'links': []
            }
            
            # åˆ†æscriptæ ‡ç­¾
            for script in soup.find_all('script'):
                script_info = {}
                if script.get('src'):
                    script_info['type'] = 'external'
                    script_info['src'] = urljoin(url, script['src'])
                else:
                    script_info['type'] = 'inline'
                    if script.string:
                        # æŸ¥æ‰¾å†…åµŒJSONæ•°æ®
                        content = script.string.strip()
                        if content:
                            # æŸ¥æ‰¾å¯èƒ½çš„APIç«¯ç‚¹
                            api_patterns = [
                                r'[\'"]https?://[^\'"]*api[^\'"]*[\'"]',
                                r'[\'"]https?://[^\'"]*\.json[\'"]',
                                r'fetch\s*\(\s*[\'"][^\'"]+[\'"]',
                                r'axios\.[^(]+\([\'"][^\'"]+[\'"]',
                                r'\.get\s*\(\s*[\'"][^\'"]+[\'"]',
                                r'\.post\s*\(\s*[\'"][^\'"]+[\'"]'
                            ]
                            
                            for pattern in api_patterns:
                                matches = re.findall(pattern, content, re.IGNORECASE)
                                for match in matches:
                                    # æ¸…ç†åŒ¹é…çš„å­—ç¬¦ä¸²
                                    clean_match = re.sub(r'[\'\"()fetch\.getpostaxios]', '', match).strip()
                                    if clean_match and clean_match.startswith('http'):
                                        analysis['api_hints'].append(clean_match)
                            
                            # å°è¯•æ‰¾åˆ°JSONæ•°æ®
                            try:
                                # æŸ¥æ‰¾å¯èƒ½çš„JSONå¯¹è±¡
                                json_patterns = [
                                    r'\{[^{}]*"[^"]*"[^{}]*:[^{}]*\}',
                                    r'window\.__[A-Z_]+__\s*=\s*(\{.*?\});',
                                    r'data\s*:\s*(\{.*?\})',
                                ]
                                
                                for pattern in json_patterns:
                                    matches = re.findall(pattern, content, re.DOTALL)
                                    for match in matches:
                                        try:
                                            if isinstance(match, tuple):
                                                match = match[0] if match else ""
                                            parsed = json.loads(match)
                                            analysis['json_data'].append({
                                                'data': parsed,
                                                'size': len(str(match))
                                            })
                                        except:
                                            continue
                                            
                            except Exception as e:
                                pass
                                
                analysis['scripts'].append(script_info)
            
            # åˆ†æmetaæ ‡ç­¾
            for meta in soup.find_all('meta'):
                meta_info = {}
                for attr in ['name', 'property', 'content', 'http-equiv']:
                    if meta.get(attr):
                        meta_info[attr] = meta[attr]
                if meta_info:
                    analysis['meta_tags'].append(meta_info)
            
            # åˆ†æé“¾æ¥
            for link in soup.find_all('a', href=True):
                href = link['href']
                if href.startswith('/blog/') and href != '/blog/zh':
                    analysis['links'].append({
                        'href': urljoin(url, href),
                        'text': link.get_text().strip()[:100]
                    })
            
            return analysis
            
        except Exception as e:
            print(f"âŒ åˆ†æé¡µé¢æ—¶å‡ºé”™: {e}")
            return None
    
    def test_api_endpoints(self):
        """æµ‹è¯•å¯èƒ½çš„APIç«¯ç‚¹"""
        print("\nğŸ§ª æµ‹è¯•å¯èƒ½çš„APIç«¯ç‚¹...")
        
        potential_endpoints = [
            "/api/blog",
            "/api/blog/zh",
            "/api/v1/blog",
            "/api/posts",
            "/blog/api",
            "/blog/feed",
            "/blog/rss",
            "/_next/data",
            "/api/content",
            "/graphql"
        ]
        
        api_results = []
        
        for endpoint in potential_endpoints:
            full_url = urljoin(self.base_url, endpoint)
            print(f"   æµ‹è¯•: {full_url}")
            
            try:
                # å°è¯•GETè¯·æ±‚
                response = self.session.get(full_url, timeout=10)
                
                result = {
                    'url': full_url,
                    'method': 'GET',
                    'status_code': response.status_code,
                    'headers': dict(response.headers),
                    'content_type': response.headers.get('content-type', ''),
                    'size': len(response.content)
                }
                
                if response.status_code == 200:
                    print(f"   âœ… æˆåŠŸ: {response.status_code} - {response.headers.get('content-type', '')}")
                    
                    # å¦‚æœæ˜¯JSONå“åº”ï¼Œå°è¯•è§£æ
                    if 'application/json' in response.headers.get('content-type', ''):
                        try:
                            result['json_data'] = response.json()
                            print(f"      JSONæ•°æ®ç±»å‹: {type(result['json_data'])}")
                        except:
                            result['json_data'] = None
                            
                    # å¦‚æœæ˜¯HTMLå“åº”ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«åšå®¢æ•°æ®
                    elif 'text/html' in response.headers.get('content-type', ''):
                        soup = BeautifulSoup(response.text, 'html.parser')
                        if soup.find_all(['article', '.blog-post', '.post']):
                            print(f"      åŒ…å«åšå®¢ç»“æ„å…ƒç´ ")
                            result['has_blog_structure'] = True
                            
                else:
                    print(f"   âŒ å¤±è´¥: {response.status_code}")
                
                api_results.append(result)
                
            except requests.exceptions.RequestException as e:
                print(f"   âš ï¸  è¯·æ±‚é”™è¯¯: {e}")
                api_results.append({
                    'url': full_url,
                    'error': str(e)
                })
            
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        return api_results
    
    def analyze_blog_page_patterns(self):
        """åˆ†æåšå®¢é¡µé¢çš„åˆ†é¡µå’ŒåŠ è½½æ¨¡å¼"""
        print("\nğŸ“„ åˆ†æåšå®¢é¡µé¢æ¨¡å¼...")
        
        blog_url = "https://huggingface.co/blog/zh"
        analysis = self.analyze_page_structure(blog_url)
        
        if not analysis:
            return None
        
        # æŸ¥æ‰¾åˆ†é¡µç›¸å…³çš„æ¨¡å¼
        pagination_patterns = []
        
        # æ£€æŸ¥URLä¸­çš„åˆ†é¡µå‚æ•°
        for link in analysis.get('links', []):
            href = link['href']
            if any(param in href for param in ['page=', 'offset=', 'cursor=', 'skip=', 'limit=']):
                pagination_patterns.append({
                    'type': 'url_parameter',
                    'url': href,
                    'text': link['text']
                })
        
        # æ£€æŸ¥API hintsä¸­çš„åˆ†é¡µ
        for hint in analysis.get('api_hints', []):
            if any(param in hint for param in ['page', 'offset', 'cursor', 'limit']):
                pagination_patterns.append({
                    'type': 'api_hint',
                    'url': hint
                })
        
        analysis['pagination_patterns'] = pagination_patterns
        
        return analysis
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š HUGGING FACE åšå®¢ç»¼åˆç½‘ç»œåˆ†ææŠ¥å‘Š")
        print("="*80)
        
        # 1. åˆ†æä¸»é¡µé¢
        print("\n1ï¸âƒ£ ä¸»é¡µé¢åˆ†æ")
        main_analysis = self.analyze_blog_page_patterns()
        
        if main_analysis:
            print(f"   é¡µé¢æ ‡é¢˜: {main_analysis.get('page_title', 'N/A')}")
            print(f"   çŠ¶æ€ç : {main_analysis.get('status_code', 'N/A')}")
            print(f"   å‘ç°çš„é“¾æ¥æ•°é‡: {len(main_analysis.get('links', []))}")
            print(f"   å†…åµŒJSONæ•°æ®: {len(main_analysis.get('json_data', []))} ä¸ª")
            print(f"   APIæç¤º: {len(main_analysis.get('api_hints', []))} ä¸ª")
            
            if main_analysis.get('api_hints'):
                print("\n   ğŸ” å‘ç°çš„APIæç¤º:")
                for i, hint in enumerate(main_analysis['api_hints'][:5], 1):
                    print(f"      {i}. {hint}")
            
            if main_analysis.get('pagination_patterns'):
                print("\n   ğŸ“„ åˆ†é¡µæ¨¡å¼:")
                for pattern in main_analysis['pagination_patterns'][:3]:
                    print(f"      {pattern['type']}: {pattern.get('url', 'N/A')}")
        
        # 2. æµ‹è¯•APIç«¯ç‚¹
        print("\n2ï¸âƒ£ APIç«¯ç‚¹æµ‹è¯•")
        api_results = self.test_api_endpoints()
        
        successful_apis = [r for r in api_results if r.get('status_code') == 200]
        print(f"   æˆåŠŸçš„ç«¯ç‚¹: {len(successful_apis)}/{len(api_results)}")
        
        if successful_apis:
            print("\n   âœ… å¯ç”¨çš„APIç«¯ç‚¹:")
            for api in successful_apis:
                print(f"      {api['url']} - {api.get('content_type', 'unknown')}")
                if api.get('json_data'):
                    print(f"         è¿”å›JSONæ•°æ®: {type(api['json_data'])}")
        
        # 3. åˆ†æä¸€ç¯‡å…·ä½“æ–‡ç« 
        print("\n3ï¸âƒ£ æ–‡ç« è¯¦æƒ…é¡µåˆ†æ")
        if main_analysis and main_analysis.get('links'):
            first_article = main_analysis['links'][0]['href']
            print(f"   åˆ†ææ–‡ç« : {first_article}")
            
            article_analysis = self.analyze_page_structure(first_article)
            if article_analysis:
                print(f"   æ–‡ç« æ ‡é¢˜: {article_analysis.get('page_title', 'N/A')}")
                print(f"   å†…åµŒæ•°æ®: {len(article_analysis.get('json_data', []))} ä¸ª")
                print(f"   APIæç¤º: {len(article_analysis.get('api_hints', []))} ä¸ª")
        
        # 4. ç”Ÿæˆæœ€ç»ˆå»ºè®®
        print("\n4ï¸âƒ£ æ•°æ®è·å–å»ºè®®")
        
        has_working_api = len(successful_apis) > 0
        has_json_data = main_analysis and len(main_analysis.get('json_data', [])) > 0
        has_api_hints = main_analysis and len(main_analysis.get('api_hints', [])) > 0
        
        if has_working_api:
            print("   âœ… å‘ç°å¯ç”¨çš„APIç«¯ç‚¹ï¼Œå»ºè®®ä¼˜å…ˆä½¿ç”¨APIæ–¹å¼")
            print("   ğŸ“‹ APIä½¿ç”¨æ–¹æ¡ˆ:")
            for api in successful_apis:
                if api.get('json_data'):
                    print(f"      - ä½¿ç”¨ {api['url']} è·å–JSONæ•°æ®")
                    print(f"        å“åº”ç±»å‹: {type(api['json_data'])}")
        elif has_json_data:
            print("   ğŸ“„ é¡µé¢åŒ…å«å†…åµŒJSONæ•°æ®ï¼Œå»ºè®®è§£æé¡µé¢ä¸­çš„æ•°æ®")
            print("   ğŸ“‹ é¡µé¢è§£ææ–¹æ¡ˆ:")
            print("      - è¯·æ±‚HTMLé¡µé¢")
            print("      - æå–scriptæ ‡ç­¾ä¸­çš„JSONæ•°æ®")
            print("      - è§£ææ•°æ®ç»“æ„è·å–åšå®¢ä¿¡æ¯")
        elif has_api_hints:
            print("   ğŸ” å‘ç°APIæç¤ºä½†éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•")
            print("   ğŸ“‹ è¿›ä¸€æ­¥è°ƒæŸ¥æ–¹æ¡ˆ:")
            print("      - ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·ç›‘æ§å®é™…è¯·æ±‚")
            print("      - æµ‹è¯•å‘ç°çš„APIæç¤ºURL")
        else:
            print("   ğŸ“„ å»ºè®®ä½¿ç”¨ä¼ ç»ŸHTMLè§£ææ–¹å¼")
            print("   ğŸ“‹ HTMLè§£ææ–¹æ¡ˆ:")
            print("      - è¯·æ±‚åšå®¢åˆ—è¡¨é¡µé¢")
            print("      - ä½¿ç”¨BeautifulSoupè§£ææ–‡ç« é“¾æ¥")
            print("      - é€ä¸ªè¯·æ±‚æ–‡ç« è¯¦æƒ…é¡µé¢")
        
        # 5. ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_data = {
            'analysis_time': time.strftime('%Y-%m-%d %H:%M:%S'),
            'main_page_analysis': main_analysis,
            'api_test_results': api_results,
            'successful_apis': successful_apis,
            'recommendations': {
                'has_working_api': has_working_api,
                'has_json_data': has_json_data,
                'has_api_hints': has_api_hints,
                'recommended_approach': 'api' if has_working_api else ('json_parsing' if has_json_data else 'html_parsing')
            }
        }
        
        with open('/home/shan/Huggingface_blog/comprehensive_analysis_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: /home/shan/Huggingface_blog/comprehensive_analysis_report.json")
        
        return report_data

def main():
    analyzer = HuggingFaceAnalyzer()
    report = analyzer.generate_comprehensive_report()
    
    print("\nğŸ¯ æ€»ç»“:")
    recommended = report['recommendations']['recommended_approach']
    if recommended == 'api':
        print("   æ¨èä½¿ç”¨APIæ–¹å¼è·å–æ•°æ®")
    elif recommended == 'json_parsing':
        print("   æ¨èè§£æé¡µé¢å†…åµŒJSONæ•°æ®")
    else:
        print("   æ¨èä½¿ç”¨ä¼ ç»ŸHTMLè§£ææ–¹å¼")

if __name__ == "__main__":
    main()