#!/usr/bin/env python3
"""
Claude Newsroom Spider
ç®€åŒ–ç‰ˆæ–°é—»çˆ¬è™«ï¼Œä»Claudeå®˜æ–¹newsroomçˆ¬å–æ–°é—»å¹¶æœ¬åœ°å­˜å‚¨
"""

import os
import re
import json
import time
import hashlib
import requests
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import argparse

class ClaudeNewsroomSpider:
    def __init__(self, config_file="config.json"):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.base_url = "https://www.anthropic.com"
        self.newsroom_url = f"{self.base_url}/news"
        self.session = requests.Session()
        self.config = self.load_config(config_file)
        self.setup_directories()
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def load_config(self, config_file):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"é…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.get_default_config()
    
    def get_default_config(self):
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "crawler": {
                "delay": 2,
                "timeout": 30,
                "max_retries": 3,
                "max_articles": 50
            },
            "media": {
                "download_images": True,
                "image_timeout": 15,
                "max_image_size": 10485760
            },
            "storage": {
                "data_dir": "crawled_data",
                "articles_dir": "articles",
                "create_subdirs": True
            }
        }
    
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        self.data_dir = Path(self.config["storage"]["data_dir"])
        self.articles_dir = self.data_dir / self.config["storage"]["articles_dir"]
        
        self.data_dir.mkdir(exist_ok=True)
        self.articles_dir.mkdir(exist_ok=True)
    
    def get_news_links(self):
        """è·å–æ–°é—»é¡µé¢çš„æ‰€æœ‰æ–°é—»é“¾æ¥"""
        print(f"æ­£åœ¨è·å–æ–°é—»åˆ—è¡¨: {self.newsroom_url}")
        
        try:
            response = self.session.get(
                self.newsroom_url, 
                timeout=self.config["crawler"]["timeout"]
            )
            response.raise_for_status()
        except requests.RequestException as e:
            print(f"è·å–æ–°é—»åˆ—è¡¨å¤±è´¥: {e}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        news_links = []
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–°é—»é“¾æ¥
        # åŸºäºä¹‹å‰åˆ†æï¼Œæ–°é—»é“¾æ¥æ ¼å¼ä¸º /news/{slug}
        link_elements = soup.find_all('a', href=True)
        
        for link in link_elements:
            href = link.get('href', '')
            if href.startswith('/news/') and len(href) > 6:  # è¿‡æ»¤æ‰ /news æœ¬èº«
                full_url = urljoin(self.base_url, href)
                if full_url not in news_links:
                    news_links.append(full_url)
        
        print(f"æ‰¾åˆ° {len(news_links)} ç¯‡æ–°é—»")
        return news_links[:self.config["crawler"]["max_articles"]]
    
    def extract_article_content(self, url):
        """æå–å•ç¯‡æ–‡ç« å†…å®¹"""
        print(f"æ­£åœ¨çˆ¬å–: {url}")
        
        try:
            response = self.session.get(url, timeout=self.config["crawler"]["timeout"])
            response.raise_for_status()
        except requests.RequestException as e:
            print(f"è·å–æ–‡ç« å¤±è´¥: {e}")
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ–‡ç« ä¿¡æ¯
        article_data = {
            'url': url,
            'title': self.extract_title(soup),
            'category': self.extract_category(soup),
            'date': self.extract_date(soup),
            'content': self.extract_content(soup),
            'images': self.extract_images(soup),
            'crawl_time': datetime.now().isoformat(),
            'slug': self.extract_slug_from_url(url)
        }
        
        return article_data
    
    def extract_title(self, soup):
        """æå–æ–‡ç« æ ‡é¢˜"""
        # å°è¯•å¤šç§é€‰æ‹©å™¨
        selectors = ['h1', 'h1[class*="title"]', '[data-testid="title"]', '.article-title']
        
        for selector in selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                return title_elem.get_text(strip=True)
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä»é¡µé¢æ ‡é¢˜æå–
        title_tag = soup.find('title')
        if title_tag:
            title = title_tag.get_text(strip=True)
            # ç§»é™¤å¸¸è§çš„ç½‘ç«™åç¼€
            title = re.sub(r'\s*[\|\-]\s*Anthropic.*$', '', title)
            return title
        
        return "æœªçŸ¥æ ‡é¢˜"
    
    def extract_category(self, soup):
        """æå–æ–‡ç« åˆ†ç±»"""
        # æ ¹æ®é¡µé¢åˆ†æï¼Œå¯»æ‰¾å¯èƒ½çš„åˆ†ç±»æ ‡ç­¾
        selectors = [
            '[class*="category"]',
            '[class*="tag"]',
            'span:contains("Announcements")',
            'span:contains("Product")',
            'span:contains("Policy")',
            'span:contains("Research")'
        ]
        
        for selector in selectors:
            try:
                category_elem = soup.select_one(selector)
                if category_elem:
                    category = category_elem.get_text(strip=True)
                    if category and len(category) < 50:  # åˆç†çš„åˆ†ç±»é•¿åº¦
                        return category
            except:
                continue
        
        return "æœªåˆ†ç±»"
    
    def extract_date(self, soup):
        """æå–å‘å¸ƒæ—¥æœŸ"""
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼å’Œé€‰æ‹©å™¨
        selectors = [
            'time[datetime]',
            '[class*="date"]',
            '[class*="publish"]',
            'meta[property="article:published_time"]'
        ]
        
        for selector in selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                # å°è¯•ä»datetimeå±æ€§è·å–
                datetime_attr = date_elem.get('datetime') or date_elem.get('content')
                if datetime_attr:
                    return datetime_attr
                
                # ä»æ–‡æœ¬å†…å®¹è·å–
                date_text = date_elem.get_text(strip=True)
                if date_text:
                    return self.parse_date_string(date_text)
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šåœ¨é¡µé¢ä¸­æœç´¢æ—¥æœŸæ¨¡å¼
        date_patterns = [
            r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+\d{1,2},?\s+20\d{2}',
            r'\d{4}-\d{2}-\d{2}',
            r'\d{1,2}/\d{1,2}/20\d{2}'
        ]
        
        page_text = soup.get_text()
        for pattern in date_patterns:
            match = re.search(pattern, page_text)
            if match:
                return match.group(0)
        
        return datetime.now().strftime('%Y-%m-%d')
    
    def parse_date_string(self, date_str):
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        # æ¸…ç†æ—¥æœŸå­—ç¬¦ä¸²
        date_str = re.sub(r'[^\w\s,/-]', '', date_str).strip()
        
        # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
        formats = [
            '%b %d, %Y',    # Jan 01, 2024
            '%B %d, %Y',    # January 01, 2024
            '%Y-%m-%d',     # 2024-01-01
            '%m/%d/%Y',     # 01/01/2024
            '%d/%m/%Y',     # 01/01/2024
        ]
        
        for fmt in formats:
            try:
                parsed_date = datetime.strptime(date_str, fmt)
                return parsed_date.strftime('%Y-%m-%d')
            except ValueError:
                continue
        
        return date_str
    
    def extract_content(self, soup):
        """æå–æ–‡ç« æ­£æ–‡å†…å®¹"""
        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()
        
        # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            'article',
            '[class*="content"]',
            '[class*="article"]',
            'main',
            '.post-content',
            '#content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # æ¸…ç†å’Œæ ¼å¼åŒ–å†…å®¹
                content = self.clean_content(content_elem)
                if len(content) > 100:  # ç¡®ä¿å†…å®¹æœ‰è¶³å¤Ÿé•¿åº¦
                    return content
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šæå–ä¸»è¦æ®µè½
        paragraphs = soup.find_all('p')
        if paragraphs:
            content = '\n\n'.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            return content
        
        return "æ— æ³•æå–å†…å®¹"
    
    def clean_content(self, content_elem):
        """æ¸…ç†æ–‡ç« å†…å®¹"""
        # è½¬æ¢ä¸ºmarkdownæ ¼å¼
        content_lines = []
        
        for element in content_elem.descendants:
            if element.name == 'h1':
                content_lines.append(f"\n# {element.get_text(strip=True)}\n")
            elif element.name == 'h2':
                content_lines.append(f"\n## {element.get_text(strip=True)}\n")
            elif element.name == 'h3':
                content_lines.append(f"\n### {element.get_text(strip=True)}\n")
            elif element.name == 'p':
                text = element.get_text(strip=True)
                if text:
                    content_lines.append(f"{text}\n")
            elif element.name == 'a' and element.get('href'):
                text = element.get_text(strip=True)
                href = element.get('href')
                if text and href:
                    content_lines.append(f"[{text}]({href})")
        
        return '\n'.join(content_lines).strip()
    
    def extract_images(self, soup):
        """æå–æ–‡ç« ä¸­çš„å›¾ç‰‡ä¿¡æ¯"""
        images = []
        img_elements = soup.find_all('img', src=True)
        
        for img in img_elements:
            src = img.get('src')
            if not src:
                continue
            
            # è½¬æ¢ä¸ºå®Œæ•´URL
            full_url = urljoin(self.base_url, src)
            
            # è¿‡æ»¤æ‰å°å›¾æ ‡å’Œè£…é¥°æ€§å›¾ç‰‡
            if any(x in src.lower() for x in ['icon', 'logo', 'avatar']) and 'x' in src and '32' in src:
                continue
            
            # è·å–æ›´ä¸°å¯Œçš„å›¾ç‰‡ä¿¡æ¯
            alt_text = img.get('alt', '').strip()
            title_text = img.get('title', '').strip()
            
            # å°è¯•ä»å‘¨å›´æ–‡æœ¬è·å–æè¿°
            if not alt_text and not title_text:
                # æŸ¥æ‰¾å›¾ç‰‡çš„çˆ¶å…ƒç´ æˆ–å…„å¼Ÿå…ƒç´ ä¸­çš„æ–‡æœ¬
                parent = img.parent
                if parent:
                    # æŸ¥æ‰¾å›¾ç‰‡è¯´æ˜æ–‡å­—
                    caption = parent.find(['figcaption', 'caption'])
                    if caption:
                        alt_text = caption.get_text().strip()
                    else:
                        # æŸ¥æ‰¾é™„è¿‘çš„æ–‡æœ¬ï¼ˆé™åˆ¶é•¿åº¦é¿å…è·å–æ­£æ–‡ï¼‰
                        siblings = parent.find_all(text=True, recursive=False)
                        for sibling in siblings:
                            text = sibling.strip()
                            if text and len(text) < 50 and not text.startswith('#'):
                                alt_text = text
                                break
            
            image_info = {
                'url': full_url,
                'alt': alt_text,
                'title': title_text,
                'filename': self.generate_image_filename(full_url)
            }
            images.append(image_info)
        
        return images
    
    def generate_image_filename(self, url):
        """ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å"""
        parsed = urlparse(url)
        original_name = os.path.basename(parsed.path)
        
        if not original_name or '.' not in original_name:
            # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œå°è¯•ä»URLç”Ÿæˆ
            url_hash = hashlib.md5(url.encode()).hexdigest()[:8]
            return f"image_{url_hash}.jpg"
        
        return original_name
    
    def download_images(self, images, article_dir):
        """ä¸‹è½½æ–‡ç« å›¾ç‰‡"""
        if not self.config["media"]["download_images"]:
            return
        
        media_dir = article_dir / 'media'
        media_dir.mkdir(exist_ok=True)
        
        for img_info in images:
            try:
                response = self.session.get(
                    img_info['url'], 
                    timeout=self.config["media"]["image_timeout"],
                    stream=True
                )
                response.raise_for_status()
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                content_length = response.headers.get('content-length')
                if content_length and int(content_length) > self.config["media"]["max_image_size"]:
                    print(f"å›¾ç‰‡å¤ªå¤§ï¼Œè·³è¿‡: {img_info['url']}")
                    continue
                
                # ä¿å­˜å›¾ç‰‡
                image_path = media_dir / img_info['filename']
                with open(image_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                img_info['local_path'] = str(image_path)
                print(f"ä¸‹è½½å›¾ç‰‡: {img_info['filename']}")
                
            except Exception as e:
                print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_info['url']}: {e}")
                img_info['download_error'] = str(e)
    
    def extract_slug_from_url(self, url):
        """ä»URLæå–æ–‡ç« slug"""
        parsed = urlparse(url)
        parts = parsed.path.split('/')
        return parts[-1] if parts else hashlib.md5(url.encode()).hexdigest()[:8]
    
    def save_article(self, article_data):
        """ä¿å­˜æ–‡ç« åˆ°æœ¬åœ°"""
        slug = article_data['slug']
        article_dir = self.articles_dir / slug
        article_dir.mkdir(exist_ok=True)
        
        # ä¸‹è½½å›¾ç‰‡
        if article_data.get('images'):
            self.download_images(article_data['images'], article_dir)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = article_dir / 'metadata.json'
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(article_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜Markdownå†…å®¹
        content_file = article_dir / 'content.md'
        markdown_content = self.generate_markdown(article_data)
        with open(content_file, 'w', encoding='utf-8', newline='\n') as f:
            f.write(markdown_content)
        
        print(f"ä¿å­˜æ–‡ç« : {article_data['title']}")
        return article_dir
    
    def generate_markdown(self, article_data):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æ–‡ç« """
        markdown_lines = [
            f"# {article_data['title']}\n",
            f"**åˆ†ç±»**: {article_data['category']}  ",
            f"**å‘å¸ƒæ—¥æœŸ**: {article_data['date']}  ",
            f"**æ¥æº**: {article_data['url']}  ",
            f"**çˆ¬å–æ—¶é—´**: {article_data['crawl_time']}\n",
            "---\n"
        ]
        
        # å¤„ç†æ­£æ–‡å†…å®¹ï¼Œåœ¨åˆé€‚ä½ç½®æ’å…¥å›¾ç‰‡
        content = article_data['content']
        images = article_data.get('images', [])
        
        if images:
            # å°†å†…å®¹æŒ‰æ®µè½åˆ†å‰²
            paragraphs = content.split('\n\n')
            enhanced_content = []
            image_index = 0
            total_paragraphs = len(paragraphs)
            
            # è®¡ç®—å›¾ç‰‡æ’å…¥ä½ç½®
            insertion_points = []
            if total_paragraphs > 3:
                # ç¬¬ä¸€å¼ å›¾ç‰‡åœ¨ç¬¬2æ®µåï¼ˆé¿å…ç´§è·Ÿæ ‡é¢˜ï¼‰
                insertion_points.append(2)
                # å¦‚æœæœ‰å¤šå¼ å›¾ç‰‡ï¼Œåœ¨ä¸­é—´ä½ç½®æ’å…¥
                if len(images) > 1 and total_paragraphs > 6:
                    insertion_points.append(total_paragraphs // 2)
                # å¦‚æœæœ‰å¾ˆå¤šå›¾ç‰‡ï¼Œåœ¨3/4ä½ç½®æ’å…¥
                if len(images) > 2 and total_paragraphs > 10:
                    insertion_points.append(int(total_paragraphs * 0.75))
            
            for i, paragraph in enumerate(paragraphs):
                enhanced_content.append(paragraph)
                
                # åœ¨æŒ‡å®šä½ç½®æ’å…¥å›¾ç‰‡
                if i in insertion_points and image_index < len(images):
                    img = images[image_index]
                    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
                    relative_path = f"media/{img['filename']}"
                    alt_text = img.get('alt', '') or img.get('title', '') or f"{article_data['title']} - å›¾ç‰‡ {image_index + 1}"
                    enhanced_content.append(f"\n![{alt_text}]({relative_path})\n")
                    image_index += 1
            
            content = '\n\n'.join(enhanced_content)
            
            # åœ¨æ–‡ç« æœ«å°¾æ·»åŠ å‰©ä½™å›¾ç‰‡
            if image_index < len(images):
                content += "\n\n## ç›¸å…³å›¾ç‰‡\n"
                for img in images[image_index:]:
                    relative_path = f"media/{img['filename']}"
                    alt_text = img.get('alt', '') or img.get('title', '') or f"{article_data['title']} - å›¾ç‰‡"
                    content += f"\n![{alt_text}]({relative_path})\n"
        
        markdown_lines.append(content)
        return '\n'.join(markdown_lines)
    
    def is_article_exists(self, url):
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨"""
        slug = self.extract_slug_from_url(url)
        article_dir = self.articles_dir / slug
        return article_dir.exists() and (article_dir / 'content.md').exists()
    
    def crawl(self, force_update=False, max_articles=None):
        """æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""
        print("ğŸ¤– Claude Newsroom çˆ¬è™«å¯åŠ¨")
        print(f"ç›®æ ‡: {self.newsroom_url}")
        
        # è·å–æ–°é—»é“¾æ¥
        news_links = self.get_news_links()
        if not news_links:
            print("æœªæ‰¾åˆ°æ–°é—»é“¾æ¥")
            return
        
        if max_articles:
            news_links = news_links[:max_articles]
        
        success_count = 0
        error_count = 0
        
        for i, url in enumerate(news_links, 1):
            print(f"\n[{i}/{len(news_links)}] å¤„ç†æ–‡ç« ...")
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé™¤éå¼ºåˆ¶æ›´æ–°ï¼‰
            if not force_update and self.is_article_exists(url):
                print(f"æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡: {url}")
                continue
            
            try:
                # æå–æ–‡ç« å†…å®¹
                article_data = self.extract_article_content(url)
                if not article_data:
                    error_count += 1
                    continue
                
                # ä¿å­˜æ–‡ç« 
                self.save_article(article_data)
                success_count += 1
                
                # å»¶è¿Ÿä»¥é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(self.config["crawler"]["delay"])
                
            except Exception as e:
                print(f"å¤„ç†æ–‡ç« å‡ºé”™ {url}: {e}")
                error_count += 1
        
        print(f"\nâœ… çˆ¬å–å®Œæˆ!")
        print(f"æˆåŠŸ: {success_count} ç¯‡")
        print(f"å¤±è´¥: {error_count} ç¯‡")
        print(f"æ•°æ®ä¿å­˜ä½ç½®: {self.articles_dir}")

def main():
    parser = argparse.ArgumentParser(description='Claude Newsroom çˆ¬è™«')
    parser.add_argument('--config', default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--force', action='store_true', help='å¼ºåˆ¶æ›´æ–°å·²å­˜åœ¨çš„æ–‡ç« ')
    parser.add_argument('--max', type=int, help='æœ€å¤§çˆ¬å–æ–‡ç« æ•°é‡')
    
    args = parser.parse_args()
    
    spider = ClaudeNewsroomSpider(args.config)
    spider.crawl(force_update=args.force, max_articles=args.max)

if __name__ == "__main__":
    main()
