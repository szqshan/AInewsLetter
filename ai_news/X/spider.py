#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
X (Twitter) çˆ¬è™«ä¸»è„šæœ¬
åŸºäºPlaywrightçš„æ¨æ–‡çˆ¬å–å·¥å…·
"""

import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path
import hashlib
import re
import requests
from urllib.parse import urlparse

class XSpider:
    def __init__(self):
        """åˆå§‹åŒ–Xçˆ¬è™«"""
        self.base_dir = Path(__file__).parent
        self.setup_directories()
        
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        data_dir = self.base_dir / "crawled_data"
        data_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ æ•°æ®ç›®å½•å·²åˆ›å»º: {data_dir}")
        
    def generate_tweet_id(self, tweet_data):
        """ç”Ÿæˆæ¨æ–‡å”¯ä¸€æ ‡è¯†ç¬¦"""
        if 'id' in tweet_data:
            return tweet_data['id']
        # å¦‚æœæ²¡æœ‰IDï¼Œä½¿ç”¨å†…å®¹å“ˆå¸Œ
        content = tweet_data.get('text', '') + tweet_data.get('url', '')
        return hashlib.md5(content.encode()).hexdigest()[:16]
        
    def is_tweet_exists(self, tweet_id):
        """æ£€æŸ¥æ¨æ–‡æ˜¯å¦å·²å­˜åœ¨"""
        tweet_dir = self.base_dir / "crawled_data" / tweet_id
        return tweet_dir.exists()
        
    def save_tweet(self, tweet_data, tweet_id):
        """ä¿å­˜æ¨æ–‡æ•°æ®"""
        tweet_dir = self.base_dir / "crawled_data" / tweet_id
        tweet_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºmediaæ–‡ä»¶å¤¹
        media_dir = tweet_dir / "media"
        media_dir.mkdir(exist_ok=True)
        
        # ä¸‹è½½å›¾ç‰‡åˆ°mediaæ–‡ä»¶å¤¹
        downloaded_images = []
        images = tweet_data.get('images', [])
        if images:
            print(f"ğŸ“¸ å¼€å§‹ä¸‹è½½ {len(images)} å¼ å›¾ç‰‡...")
            for i, img in enumerate(images, 1):
                try:
                    img_url = img.get('url', '')
                    if img_url:
                        # è·å–æ–‡ä»¶æ‰©å±•å
                        parsed_url = urlparse(img_url)
                        ext = '.jpg'  # é»˜è®¤æ‰©å±•å
                        if '.' in parsed_url.path:
                            ext = '.' + parsed_url.path.split('.')[-1].lower()
                        elif 'format=' in img_url:
                            format_match = re.search(r'format=([a-zA-Z]+)', img_url)
                            if format_match:
                                ext = '.' + format_match.group(1).lower()
                        
                        filename = f"image_{i}{ext}"
                        filepath = media_dir / filename
                        
                        # ä¸‹è½½å›¾ç‰‡
                        response = requests.get(img_url, timeout=30)
                        response.raise_for_status()
                        
                        with open(filepath, 'wb') as f:
                            f.write(response.content)
                        
                        downloaded_images.append({
                            'url': img_url,
                            'filename': filename,
                            'alt': img.get('alt', f'æ¨æ–‡å›¾ç‰‡ {i}'),
                            'local_path': str(filepath)
                        })
                        
                        print(f"âœ… å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {filename}")
                        
                except Exception as e:
                    print(f"âŒ å›¾ç‰‡ä¸‹è½½å¤±è´¥ {img_url}: {e}")
                    downloaded_images.append({
                        'url': img_url,
                        'filename': '',
                        'alt': img.get('alt', f'æ¨æ–‡å›¾ç‰‡ {i}'),
                        'local_path': '',
                        'error': str(e)
                    })
        
        # ä¿å­˜metadata.json
        metadata_file = tweet_dir / "metadata.json"
        metadata = {
            "url": tweet_data.get('url', ''),
            "text": tweet_data.get('text', ''),
            "author": tweet_data.get('author', ''),
            "timestamp": tweet_data.get('timestamp', ''),
            "images": downloaded_images,
            "links": tweet_data.get('links', []),
            "crawl_time": datetime.now().isoformat(),
            "tweet_id": tweet_id,
            "content_hash": hashlib.md5(tweet_data.get('text', '').encode()).hexdigest()
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜content.md
        content_file = tweet_dir / "content.md"
        markdown_content = self.generate_markdown(tweet_data, downloaded_images)
        
        with open(content_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
            
        print(f"ğŸ’¾ æ¨æ–‡å·²ä¿å­˜: {tweet_id}")
        return tweet_dir
        
    def generate_markdown(self, tweet_data, downloaded_images):
        """ç”ŸæˆMarkdownæ ¼å¼å†…å®¹"""
        author_name = tweet_data.get('author', 'æœªçŸ¥ç”¨æˆ·')
        display_time = tweet_data.get('display_time', 'æœªçŸ¥æ—¶é—´')
        timestamp = tweet_data.get('timestamp', '')
        crawl_time = datetime.now().isoformat()
        
        content = f"""# {author_name} çš„æ¨æ–‡

**ä½œè€…**: {author_name}  
**å‘å¸ƒæ—¶é—´**: {display_time}  
**æ—¶é—´æˆ³**: {timestamp}  
**æ¥æº**: {tweet_data.get('url', '')}  
**çˆ¬å–æ—¶é—´**: {crawl_time}  

---

{tweet_data.get('text', '')}

"""
        
        # æ·»åŠ æœ¬åœ°å›¾ç‰‡
        if downloaded_images:
            content += "## ğŸ–¼ï¸ å›¾ç‰‡\n\n"
            for img in downloaded_images:
                if img.get('filename'):  # åªæ˜¾ç¤ºæˆåŠŸä¸‹è½½çš„å›¾ç‰‡
                    alt_text = img.get('alt', 'æ¨æ–‡å›¾ç‰‡')
                    filename = img.get('filename')
                    content += f"![{alt_text}](media/{filename})\n\n"
            
        # æ·»åŠ é“¾æ¥
        links = tweet_data.get('links', [])
        if links:
            content += "## ğŸ”— ç›¸å…³é“¾æ¥\n\n"
            for link in links:
                content += f"- [{link.get('text', 'é“¾æ¥')}]({link.get('url', '')})\n"
            content += "\n"
                
        return content
        
    def run_js_scraper(self, script_name, target_url=None):
        """è¿è¡ŒJavaScriptçˆ¬è™«è„šæœ¬"""
        script_path = self.base_dir / "src" / script_name
        
        if not script_path.exists():
            print(f"âŒ è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {script_path}")
            return None
            
        try:
            print(f"ğŸš€ æ­£åœ¨è¿è¡Œè„šæœ¬: {script_name}")
            
            # æ„å»ºå‘½ä»¤
            cmd = ["node", str(script_path)]
            if target_url:
                cmd.append(target_url)
                
            # è¿è¡Œè„šæœ¬
            result = subprocess.run(
                cmd,
                cwd=str(self.base_dir),
                capture_output=True,
                text=True,
                encoding='utf-8'
            )
            
            if result.returncode == 0:
                print(f"âœ… è„šæœ¬æ‰§è¡ŒæˆåŠŸ: {script_name}")
                return result.stdout
            else:
                print(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {script_name}")
                print(f"é”™è¯¯ä¿¡æ¯: {result.stderr}")
                return None
                
        except Exception as e:
            print(f"âŒ è¿è¡Œè„šæœ¬æ—¶å‡ºé”™: {e}")
            return None
            
    def parse_js_output(self, output):
        """è§£æJSè„šæœ¬è¾“å‡ºçš„JSONæ•°æ®"""
        try:
            # æŸ¥æ‰¾JSONæ•°æ®æ ‡è®°
            start_marker = "TWEET_DATA_START"
            end_marker = "TWEET_DATA_END"
            
            start_idx = output.find(start_marker)
            end_idx = output.find(end_marker)
            
            if start_idx == -1 or end_idx == -1:
                print("âŒ æœªæ‰¾åˆ°JSONæ•°æ®æ ‡è®°")
                return None
                
            # æå–JSONå­—ç¬¦ä¸²
            json_start = start_idx + len(start_marker)
            json_str = output[json_start:end_idx].strip()
            
            # è§£æJSON
            tweet_data = json.loads(json_str)
            return tweet_data
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"âŒ è§£æè¾“å‡ºæ—¶å‡ºé”™: {e}")
            return None
            
    def crawl_following(self, max_tweets=None):
        """çˆ¬å–Followingé¡µé¢æ¨æ–‡"""
        if max_tweets is None:
            max_tweets = 20  # é»˜è®¤çˆ¬å–20æ¡
            
        print(f"ğŸ¯ å¼€å§‹çˆ¬å–Followingé¡µé¢æ¨æ–‡ (æœ€å¤š {max_tweets} æ¡)")
        
        # è¿è¡Œå¯¹åº”çš„JSè„šæœ¬
        result = self.run_js_scraper("scrape_following_tweets.js")
            
        if result:
            print(f"âœ… Followingé¡µé¢æ¨æ–‡çˆ¬å–å®Œæˆ")
        else:
            print("âŒ JSè„šæœ¬æ‰§è¡Œå¤±è´¥")
            
        return result
        
    def run(self, target="following", max_tweets=None):
        """æ‰§è¡Œçˆ¬è™«"""
        print(f"ğŸ•·ï¸ Xçˆ¬è™«å¯åŠ¨ - ç›®æ ‡: {target}")
        
        # åªæ”¯æŒfollowingç›®æ ‡
        if target == "following":
            return self.crawl_following(max_tweets)
        else:
            print(f"âŒ æš‚ä¸æ”¯æŒç›®æ ‡: {target}")
            return None

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='X (Twitter) çˆ¬è™«')
    parser.add_argument('--target', default='following', 
                       choices=['following'],
                       help='çˆ¬å–ç›®æ ‡')
    parser.add_argument('--max', type=int, default=None,
                       help='æœ€å¤§çˆ¬å–æ•°é‡')
    
    args = parser.parse_args()
    
    try:
        spider = XSpider()
        spider.run(target=args.target, max_tweets=args.max)
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬è™«")
    except Exception as e:
        print(f"âŒ çˆ¬è™«è¿è¡Œå‡ºé”™: {e}")
        
if __name__ == "__main__":
    main()