# Newsletter爬虫项目集成指南

## 项目概述

Newsletter爬虫项目位于 `/Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp`，是一个专门用于爬取 https://nlp.elvissaravia.com/ 网站Newsletter文章的高性能异步爬虫系统。该系统已成功爬取180篇文章，包含606张图片，总耗时约7分钟。

## 项目结构

```
爬虫mlp/
├── main.py                              # 统一入口（爬取+上传）
├── config.json                          # 配置文件
├── requirements.txt                     # Python依赖
├── src/newsletter_system/               # 核心代码
│   ├── crawler/
│   │   ├── newsletter_crawler.py       # 主爬虫类（异步并发）
│   │   └── anti_detect_crawler.py      # 反爬虫版本
│   ├── oss/
│   │   ├── oss_uploader.py            # MinIO上传器
│   │   └── wrapper.py                  # OSS封装器
│   └── utils/
│       ├── logger.py                   # 日志工具
│       └── file_utils.py              # 文件处理
└── crawled_data/                       # 爬取的数据（重要）
    ├── articles/                       # 180篇文章
    │   └── {id}_{slug}/               # 如：100722694_Top-ML-Papers-of-the-Week
    │       ├── content.md             # Markdown内容
    │       ├── metadata.json          # 元数据
    │       └── images/                # 本地图片
    │           └── img_*.jpg          # 平均3-4张图片/文章
    └── data/
        ├── articles_metadata.json      # 原始API响应
        ├── processed_articles.json     # 完整处理数据（主要用这个）
        ├── recommendation_data.json    # 推荐系统用数据
        └── crawl_stats.json           # 爬取统计

```

## 数据格式说明

### 1. 文章元数据格式 (metadata.json)
每篇文章都有独立的metadata.json文件，包含以下字段：

```json
{
  "id": 100722694,                    // 唯一ID，用于去重
  "title": "🥇Top ML Papers of the Week",
  "subtitle": "The top ML Papers of the Week (Jan 30 - Feb 5)",
  "post_date": "2023-02-05T15:50:08.035Z",
  "type": "newsletter",               // 类型：newsletter/tutorial/paper
  "wordcount": 233,                   // 字数
  "canonical_url": "https://nlp.elvissaravia.com/p/top-ml-papers-of-the-week-64c",
  "slug": "top-ml-papers-of-the-week-64c",
  "reactions": {"❤": 9},             // 反应统计
  "audience": "everyone",             // everyone/only_paid
  "postTags": [],                     // 标签数组
  "cover_image": null,               // 封面图片
  "local_images": [                  // 本地化的图片列表
    {
      "original_url": "https://substackcdn.com/image/...",
      "local_path": "articles/100722694_Top-ML-Papers/images/img_0.jpg",
      "hash": "bcd2fc8be3dfbd2717416bc644c667b8",  // 用于去重
      "size": 3209
    }
  ],
  "content_hash": "1af7cd42218ec06dcac37488ec443697",  // 内容哈希，用于去重
  "processed_date": "2025-08-07T00:21:35.900847"
}
```

### 2. 主数据文件 (processed_articles.json)
包含所有180篇文章的完整数据，是上传到Elasticsearch的主要数据源：

```json
[
  {
    "id": 169333505,
    "title": "🥇Top AI Papers of the Week",
    "subtitle": "The Top AI Papers of the Week (July 21 - 27)",
    "description": "A weekly roundup of the most significant AI research papers",
    "post_date": "2025-07-27T14:52:20.585Z",
    "type": "newsletter",
    "wordcount": 2403,
    "reactions": {"❤": 35, "👍": 12},
    "postTags": [
      {
        "id": "tag-1",
        "name": "AI Papers",
        "slug": "ai-papers"
      }
    ],
    "content": "完整的Markdown内容...",
    "local_images": [...],
    "content_hash": "...",
    // ... 其他字段
  }
]
```

## 与MinIO/Elasticsearch集成

### 1. 数据流程

```
爬虫数据 → 预处理 → 上传到MinIO（可选） → 索引到Elasticsearch
```

### 2. 上传到MinIO

爬虫项目自带MinIO上传功能：

```bash
# 从爬虫项目上传
cd /Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp
python main.py upload --bucket newsletter-articles-nlp
```

配置（config.json）：
```json
{
  "oss": {
    "base_url": "http://localhost:9011",           // MinIO API
    "public_base_url": "http://60.205.160.74:9000", // 公开访问URL
    "bucket_name": "newsletter-articles-nlp",       // 默认桶名
    "source_id": "nlp-elvissaravia"                // 数据源标识
  }
}
```

### 3. 上传到Elasticsearch

从MinIO连接项目上传：

```bash
# 切换到MinIO连接项目
cd "/Users/ruanchuhao/Downloads/Codes/NewsLetters/分步骤实现MVP /Minio的连接"

# 上传爬虫数据到Elasticsearch
cd minio-file-manager/backend
python upload_crawled_articles.py ../../../爬虫mlp/crawled_data/data/processed_articles.json --verify
```

### 4. 去重机制

系统使用双重去重：
- **文章ID去重**：基于文章的唯一ID (如：100722694)
- **内容哈希去重**：SHA256(title + subtitle + content)

已爬取的180篇文章ID范围：100722694 - 163724821

## 快速使用指南

### 1. 读取已爬取的数据

```python
import json
from pathlib import Path

# 读取所有文章数据
crawler_data_path = Path("/Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp/crawled_data")
with open(crawler_data_path / "data/processed_articles.json", 'r', encoding='utf-8') as f:
    articles = json.load(f)

print(f"总文章数: {len(articles)}")  # 180篇

# 读取单篇文章
article_dir = crawler_data_path / "articles/100722694_Top-ML-Papers-of-the-Week"
with open(article_dir / "metadata.json", 'r', encoding='utf-8') as f:
    metadata = json.load(f)
with open(article_dir / "content.md", 'r', encoding='utf-8') as f:
    content = f.read()
```

### 2. 批量上传到Elasticsearch

```python
import asyncio
from pathlib import Path
import sys

# 添加MinIO项目路径
sys.path.append("/Users/ruanchuhao/Downloads/Codes/NewsLetters/分步骤实现MVP /Minio的连接/minio-file-manager/backend")
from app.services.newsletter_elasticsearch_service import newsletter_elasticsearch_service

async def upload_all():
    # 读取爬虫数据
    crawler_path = Path("/Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp/crawled_data")
    with open(crawler_path / "data/processed_articles.json", 'r') as f:
        articles = json.load(f)
    
    # 批量上传
    result = await newsletter_elasticsearch_service.bulk_index_articles(articles)
    print(f"上传结果: {result}")
    
asyncio.run(upload_all())
```

### 3. 检查重复文章

```python
async def check_duplicate(article_id: int):
    is_dup = await newsletter_elasticsearch_service.check_duplicate(
        article_id=article_id
    )
    print(f"文章 {article_id} {'已存在' if is_dup else '不存在'}")
```

## 重要文件路径

### 爬虫项目文件
- **主数据文件**: `/Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp/crawled_data/data/processed_articles.json`
- **文章目录**: `/Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp/crawled_data/articles/`
- **配置文件**: `/Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp/config.json`
- **爬虫主程序**: `/Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp/main.py`

### MinIO连接项目文件
- **上传脚本**: `minio-file-manager/backend/upload_crawled_articles.py`
- **测试脚本**: `minio-file-manager/backend/test_newsletter_upload.py`
- **ES服务**: `minio-file-manager/backend/app/services/newsletter_elasticsearch_service.py`

## 数据统计

基于crawl_stats.json的统计：
- **总文章数**: 180篇
- **成功处理**: 180篇（100%成功率）
- **失败文章**: 0篇
- **图片总数**: 606张（实际下载693张，包含重复）
- **爬取时间**: 436.67秒（约7分钟）
- **平均耗时**: 2.4秒/文章

## 常见问题

### 1. 如何重新爬取？
```bash
cd /Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp
rm -rf crawled_data  # 清除旧数据
python main.py crawl
```

### 2. 如何只上传新文章？
Elasticsearch会自动去重，直接运行上传即可：
```bash
python upload_crawled_articles.py processed_articles.json
```

### 3. 如何验证数据完整性？
```bash
# 检查文章数量
ls -d crawled_data/articles/*/ | wc -l  # 应该是180

# 检查有内容的文章
find crawled_data/articles -name "content.md" -size +0 | wc -l  # 应该是180

# 检查图片
find crawled_data/articles -path "*/images/*" -type f | wc -l  # 应该是600+
```

### 4. 图片URL如何处理？
- 爬虫已将所有图片下载到本地
- Markdown中的图片链接已更新为本地路径
- 上传到MinIO后会自动替换为公开URL

## 性能优化建议

### 爬虫配置优化
```json
{
  "crawler": {
    "max_concurrent_articles": 2,  // 不建议超过3
    "max_concurrent_images": 5,    // 可以增加到10
    "batch_size": 5,              // 5-10最佳
    "article_delay": 0.5,          // 防止触发限流
    "enable_resume": true          // 启用断点续传
  }
}
```

### Elasticsearch批量上传
```python
# 分批上传，避免内存溢出
batch_size = 50  # 每批50篇
for i in range(0, len(articles), batch_size):
    batch = articles[i:i+batch_size]
    await bulk_index_articles(batch)
```

## 技术要点

### 1. 异步并发架构
- 使用asyncio + aiohttp实现高并发
- Semaphore控制并发数
- 页面池管理Playwright实例

### 2. 去重策略
- 文章ID（主键）
- 内容哈希（SHA256）
- 图片哈希（MD5）

### 3. 错误处理
- 指数退避重试
- 断点续传
- 详细错误日志

### 4. 数据转换
- HTML → Markdown (markdownify)
- 图片本地化
- 元数据提取

## 后续开发建议

1. **增量更新**: 只爬取新发布的文章
2. **实时同步**: 定时任务自动爬取并上传
3. **数据验证**: 自动检查数据完整性
4. **性能监控**: 添加爬取速度和成功率监控
5. **多源支持**: 扩展到其他Newsletter网站

## 联系信息

- 爬虫项目路径: `/Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp`
- MinIO项目路径: `/Users/ruanchuhao/Downloads/Codes/NewsLetters/分步骤实现MVP /Minio的连接`
- 数据更新时间: 2025-08-07

---

*本文档用于快速理解和使用Newsletter爬虫系统，确保数据能够顺利集成到MinIO和Elasticsearch中。*