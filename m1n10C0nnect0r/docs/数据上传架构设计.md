# Newsletteræ•°æ®ä¸Šä¼ æ¶æ„è®¾è®¡æ–‡æ¡£

## æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿°Newsletteræ–‡ç« æ•°æ®çš„ç»Ÿä¸€ä¸Šä¼ æ¶æ„ï¼Œå®ç°ä»çˆ¬è™«è¾“å‡ºåˆ°å¤šå­˜å‚¨ç³»ç»Ÿï¼ˆMinIOã€Elasticsearchã€PostgreSQLï¼‰çš„æ•°æ®æµè½¬ã€‚ç³»ç»Ÿè®¾è®¡ä¸ºå¯æ‰©å±•çš„pipelineæ¶æ„ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†ã€å»é‡ã€é”™è¯¯æ¢å¤ç­‰åŠŸèƒ½ã€‚

## æ•°æ®æºåˆ†æ

### è¾“å…¥æ•°æ®æ ¼å¼
çˆ¬è™«è¾“å‡ºä½äºï¼š`/Users/ruanchuhao/Downloads/Codes/NewsLetters/çˆ¬è™«mlp/crawled_data/`

```
crawled_data/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed_articles.json    # ä¸»æ•°æ®æ–‡ä»¶ï¼ˆ180ç¯‡æ–‡ç« ï¼‰
â””â”€â”€ articles/
    â””â”€â”€ {id}_{slug}/               # æ¯ç¯‡æ–‡ç« çš„ç‹¬ç«‹ç›®å½•
        â”œâ”€â”€ content.md             # Markdownå†…å®¹
        â”œâ”€â”€ metadata.json          # å…ƒæ•°æ®
        â””â”€â”€ images/                # æœ¬åœ°å›¾ç‰‡
```

### æ ¸å¿ƒæ•°æ®ç»“æ„
```json
{
  "id": 169333505,                    // ä¸»é”®
  "title": "æ–‡ç« æ ‡é¢˜",
  "subtitle": "å‰¯æ ‡é¢˜",
  "content": "Markdownå†…å®¹",          // å¯èƒ½å¾ˆå¤§ï¼ˆ1-10KBï¼‰
  "post_date": "2025-07-27T14:52:20.585Z",
  "type": "newsletter",
  "wordcount": 2403,
  "canonical_url": "https://...",
  "slug": "unique-slug",
  "reactions": {"â¤": 35},
  "postTags": [{"id": "", "name": "", "slug": ""}],
  "local_images": [                   // å›¾ç‰‡åˆ—è¡¨
    {
      "original_url": "https://...",
      "local_path": "articles/.../images/img_0.jpg",
      "hash": "sha256...",
      "size": 3209
    }
  ],
  "content_hash": "sha256...",        // ç”¨äºå»é‡
  "processed_date": "2025-08-07T00:21:35"
}
```

## ä¸Šä¼ æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   çˆ¬è™«æ•°æ®è¾“å…¥   â”‚
â”‚ processed_.json â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ•°æ®é¢„å¤„ç†å™¨    â”‚
â”‚ DataPreprocessorâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚MinIO â”‚  â”‚ ES   â”‚  â”‚ PG   â”‚
â”‚ä¸Šä¼ å™¨â”‚  â”‚ç´¢å¼•å™¨â”‚  â”‚å†™å…¥å™¨â”‚
â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶è®¾è®¡

## 1. æ•°æ®é¢„å¤„ç†å™¨ (DataPreprocessor)

è´Ÿè´£æ•°æ®éªŒè¯ã€æ¸…æ´—ã€è½¬æ¢å’Œåˆ†å‘ã€‚

```python
class DataPreprocessor:
    """ç»Ÿä¸€çš„æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, source_path: str):
        self.source_path = Path(source_path)
        self.articles = []
        self.validation_errors = []
        
    def load_data(self) -> List[Dict]:
        """åŠ è½½çˆ¬è™«æ•°æ®"""
        # 1. è¯»å–processed_articles.json
        # 2. éªŒè¯æ•°æ®å®Œæ•´æ€§
        # 3. è¿”å›æ–‡ç« åˆ—è¡¨
        
    def validate_article(self, article: Dict) -> Tuple[bool, List[str]]:
        """éªŒè¯å•ç¯‡æ–‡ç« æ•°æ®"""
        errors = []
        
        # å¿…éœ€å­—æ®µæ£€æŸ¥
        required_fields = ['id', 'title', 'content_hash']
        for field in required_fields:
            if field not in article:
                errors.append(f"Missing required field: {field}")
        
        # æ•°æ®ç±»å‹æ£€æŸ¥
        if not isinstance(article.get('id'), int):
            errors.append("ID must be integer")
            
        # å†…å®¹é•¿åº¦æ£€æŸ¥
        if len(article.get('content', '')) == 0:
            errors.append("Content is empty")
            
        return len(errors) == 0, errors
    
    def prepare_for_storage(self, article: Dict) -> Dict:
        """ä¸ºä¸åŒå­˜å‚¨ç³»ç»Ÿå‡†å¤‡æ•°æ®"""
        return {
            'minio': self._prepare_for_minio(article),
            'elasticsearch': self._prepare_for_es(article),
            'postgresql': self._prepare_for_pg(article)
        }
    
    def _prepare_for_minio(self, article: Dict) -> Dict:
        """MinIOæ•°æ®å‡†å¤‡ï¼šæ–‡ä»¶è·¯å¾„å’Œå†…å®¹"""
        return {
            'bucket': 'newsletter-articles',
            'object_path': f"articles/{article['id']}_{article['slug']}/",
            'files': {
                'content.md': article['content'],
                'metadata.json': json.dumps(article, ensure_ascii=False),
                'images': article.get('local_images', [])
            }
        }
    
    def _prepare_for_es(self, article: Dict) -> Dict:
        """Elasticsearchæ•°æ®å‡†å¤‡ï¼šç´¢å¼•æ–‡æ¡£"""
        # è®¡ç®—è¯„åˆ†
        scores = self._calculate_scores(article)
        
        return {
            'id': article['id'],
            'title': article['title'],
            'subtitle': article.get('subtitle'),
            'content': article['content'],
            'content_hash': article['content_hash'],
            'post_date': article['post_date'],
            'type': article.get('type', 'newsletter'),
            'wordcount': article.get('wordcount', 0),
            'reactions': article.get('reactions', {}),
            'reaction_count': sum(article.get('reactions', {}).values()),
            'tags': article.get('postTags', []),
            **scores
        }
    
    def _prepare_for_pg(self, article: Dict) -> Dict:
        """PostgreSQLæ•°æ®å‡†å¤‡ï¼šå…³ç³»å‹æ•°æ®"""
        return {
            'articles': {  # ä¸»è¡¨
                'id': article['id'],
                'title': article['title'],
                'subtitle': article.get('subtitle'),
                'slug': article['slug'],
                'type': article.get('type'),
                'post_date': article['post_date'],
                'wordcount': article.get('wordcount'),
                'canonical_url': article['canonical_url'],
                'content_hash': article['content_hash'],
                'created_at': datetime.now()
            },
            'article_content': {  # å†…å®¹è¡¨ï¼ˆåˆ†ç¦»å¤§å­—æ®µï¼‰
                'article_id': article['id'],
                'content_markdown': article['content'],
                'description': article.get('description')
            },
            'article_tags': [  # æ ‡ç­¾å…³è”è¡¨
                {
                    'article_id': article['id'],
                    'tag_id': tag['id'],
                    'tag_name': tag['name'],
                    'tag_slug': tag['slug']
                }
                for tag in article.get('postTags', [])
            ],
            'article_reactions': {  # ååº”ç»Ÿè®¡è¡¨
                'article_id': article['id'],
                'reactions_json': json.dumps(article.get('reactions', {})),
                'total_count': sum(article.get('reactions', {}).values())
            },
            'article_images': [  # å›¾ç‰‡è¡¨
                {
                    'article_id': article['id'],
                    'image_url': img['original_url'],
                    'local_path': img['local_path'],
                    'file_hash': img['hash'],
                    'file_size': img['size'],
                    'sequence': idx
                }
                for idx, img in enumerate(article.get('local_images', []))
            ]
        }
    
    def _calculate_scores(self, article: Dict) -> Dict:
        """è®¡ç®—æ–‡ç« è¯„åˆ†"""
        # å¤ç”¨ç°æœ‰çš„è¯„åˆ†é€»è¾‘
        reaction_count = sum(article.get('reactions', {}).values())
        wordcount = article.get('wordcount', 0)
        
        # æµè¡Œåº¦è¯„åˆ†
        popularity = reaction_count * 0.3
        if 300 <= wordcount <= 2000:
            popularity += 10
        elif wordcount > 2000:
            popularity += 5
            
        # æ–°é²œåº¦è¯„åˆ†
        post_date = datetime.fromisoformat(article['post_date'].replace('Z', '+00:00'))
        days_old = (datetime.now() - post_date.replace(tzinfo=None)).days
        freshness = max(0, 100 - days_old * 0.5)
        
        # è´¨é‡è¯„åˆ†
        quality = min(100, wordcount / 50 + reaction_count * 2)
        
        # ç»¼åˆè¯„åˆ†
        combined = popularity * 0.4 + freshness * 0.3 + quality * 0.3
        
        return {
            'popularity_score': popularity,
            'freshness_score': freshness,
            'quality_score': quality,
            'combined_score': combined
        }
```

## 2. MinIOä¸Šä¼ å™¨ (MinIOUploader)

è´Ÿè´£å°†æ–‡ç« å†…å®¹å’Œå›¾ç‰‡ä¸Šä¼ åˆ°å¯¹è±¡å­˜å‚¨ã€‚

```python
class MinIOUploader:
    """MinIOä¸Šä¼ å™¨"""
    
    def __init__(self, config: Dict):
        self.endpoint = config['endpoint']
        self.bucket = config['bucket_name']
        self.public_url = config['public_base_url']
        
    async def upload_article(self, article_data: Dict) -> Dict:
        """ä¸Šä¼ å•ç¯‡æ–‡ç« åˆ°MinIO"""
        results = {
            'article_id': article_data['id'],
            'uploaded_files': [],
            'public_urls': {},
            'errors': []
        }
        
        # 1. åˆ›å»ºæ–‡ç« ç›®å½•ç»“æ„
        base_path = f"articles/{article_data['id']}_{article_data['slug']}/"
        
        # 2. ä¸Šä¼ Markdownå†…å®¹
        content_url = await self._upload_file(
            f"{base_path}content.md",
            article_data['content'].encode('utf-8'),
            'text/markdown'
        )
        results['public_urls']['content'] = content_url
        
        # 3. ä¸Šä¼ å…ƒæ•°æ®
        metadata_url = await self._upload_file(
            f"{base_path}metadata.json",
            json.dumps(article_data, ensure_ascii=False).encode('utf-8'),
            'application/json'
        )
        results['public_urls']['metadata'] = metadata_url
        
        # 4. ä¸Šä¼ å›¾ç‰‡
        for img in article_data.get('local_images', []):
            img_path = f"{base_path}images/{Path(img['local_path']).name}"
            # è¯»å–æœ¬åœ°å›¾ç‰‡æ–‡ä»¶
            local_file = Path(self.crawler_base) / img['local_path']
            if local_file.exists():
                img_url = await self._upload_file(
                    img_path,
                    local_file.read_bytes(),
                    'image/jpeg'
                )
                results['public_urls'][img['local_path']] = img_url
        
        return results
    
    async def bulk_upload(self, articles: List[Dict], max_concurrent: int = 5):
        """æ‰¹é‡ä¸Šä¼ æ–‡ç« """
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def upload_with_limit(article):
            async with semaphore:
                return await self.upload_article(article)
        
        tasks = [upload_with_limit(article) for article in articles]
        return await asyncio.gather(*tasks)
```

## 3. Elasticsearchç´¢å¼•å™¨ (ESIndexer)

è´Ÿè´£å°†æ–‡ç« ç´¢å¼•åˆ°Elasticsearchï¼Œå®ç°å…¨æ–‡æœç´¢å’Œå»é‡ã€‚

```python
class ESIndexer:
    """Elasticsearchç´¢å¼•å™¨"""
    
    def __init__(self, config: Dict):
        self.client = AsyncElasticsearch([config['host']])
        self.index = 'newsletter_articles'
        
    async def index_article(self, article_data: Dict) -> Tuple[bool, str]:
        """ç´¢å¼•å•ç¯‡æ–‡ç« """
        # 1. æ£€æŸ¥å»é‡
        if await self._is_duplicate(article_data['id'], article_data['content_hash']):
            return False, f"Article {article_data['id']} already exists"
        
        # 2. ç´¢å¼•æ–‡æ¡£
        try:
            await self.client.index(
                index=self.index,
                id=str(article_data['id']),
                body=article_data
            )
            return True, f"Article {article_data['id']} indexed successfully"
        except Exception as e:
            return False, f"Failed to index article {article_data['id']}: {str(e)}"
    
    async def _is_duplicate(self, article_id: int, content_hash: str) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨"""
        query = {
            "query": {
                "bool": {
                    "should": [
                        {"term": {"id": article_id}},
                        {"term": {"content_hash": content_hash}}
                    ]
                }
            }
        }
        response = await self.client.search(index=self.index, body=query, size=1)
        return response['hits']['total']['value'] > 0
    
    async def bulk_index(self, articles: List[Dict]) -> Dict:
        """æ‰¹é‡ç´¢å¼•æ–‡ç« """
        actions = []
        skipped = []
        
        for article in articles:
            # æ£€æŸ¥å»é‡
            if await self._is_duplicate(article['id'], article['content_hash']):
                skipped.append(article['id'])
                continue
                
            actions.append({
                "_index": self.index,
                "_id": str(article['id']),
                "_source": article
            })
        
        # æ‰¹é‡æ‰§è¡Œ
        if actions:
            success, failed = await helpers.async_bulk(self.client, actions)
        
        return {
            'total': len(articles),
            'indexed': len(actions) - len(failed) if actions else 0,
            'skipped': len(skipped),
            'failed': failed if actions else []
        }
```

## 4. PostgreSQLå†™å…¥å™¨è®¾è®¡ (PGWriter)

### æ•°æ®åº“è¡¨ç»“æ„è®¾è®¡

```sql
-- ä¸»æ–‡ç« è¡¨
CREATE TABLE articles (
    id BIGINT PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    subtitle TEXT,
    slug VARCHAR(500) UNIQUE NOT NULL,
    type VARCHAR(50) DEFAULT 'newsletter',
    post_date TIMESTAMP WITH TIME ZONE,
    wordcount INTEGER DEFAULT 0,
    canonical_url TEXT,
    content_hash VARCHAR(64) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- ç´¢å¼•
    INDEX idx_post_date (post_date DESC),
    INDEX idx_type (type),
    INDEX idx_content_hash (content_hash)
);

-- æ–‡ç« å†…å®¹è¡¨ï¼ˆåˆ†ç¦»å¤§å­—æ®µï¼‰
CREATE TABLE article_content (
    article_id BIGINT PRIMARY KEY REFERENCES articles(id) ON DELETE CASCADE,
    content_markdown TEXT,
    content_html TEXT,
    description TEXT,
    search_vector tsvector, -- å…¨æ–‡æœç´¢å‘é‡
    
    -- å…¨æ–‡æœç´¢ç´¢å¼•
    INDEX idx_search_vector USING GIN(search_vector)
);

-- æ ‡ç­¾è¡¨
CREATE TABLE tags (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(100) NOT NULL,
    slug VARCHAR(100) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æ–‡ç« -æ ‡ç­¾å…³è”è¡¨
CREATE TABLE article_tags (
    article_id BIGINT REFERENCES articles(id) ON DELETE CASCADE,
    tag_id UUID REFERENCES tags(id) ON DELETE CASCADE,
    PRIMARY KEY (article_id, tag_id)
);

-- ååº”ç»Ÿè®¡è¡¨
CREATE TABLE article_reactions (
    article_id BIGINT PRIMARY KEY REFERENCES articles(id) ON DELETE CASCADE,
    reactions_json JSONB DEFAULT '{}',
    total_count INTEGER DEFAULT 0,
    heart_count INTEGER DEFAULT 0,
    thumbs_up_count INTEGER DEFAULT 0,
    fire_count INTEGER DEFAULT 0,
    
    -- JSONBç´¢å¼•
    INDEX idx_reactions_json USING GIN(reactions_json)
);

-- å›¾ç‰‡è¡¨
CREATE TABLE article_images (
    id SERIAL PRIMARY KEY,
    article_id BIGINT REFERENCES articles(id) ON DELETE CASCADE,
    image_url TEXT,
    minio_url TEXT,
    local_path TEXT,
    file_hash VARCHAR(64),
    file_size INTEGER,
    width INTEGER,
    height INTEGER,
    sequence INTEGER DEFAULT 0,
    is_cover BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_article_images (article_id),
    INDEX idx_file_hash (file_hash)
);

-- æ–‡ç« è¯„åˆ†è¡¨
CREATE TABLE article_scores (
    article_id BIGINT PRIMARY KEY REFERENCES articles(id) ON DELETE CASCADE,
    popularity_score FLOAT DEFAULT 0,
    freshness_score FLOAT DEFAULT 0,
    quality_score FLOAT DEFAULT 0,
    combined_score FLOAT DEFAULT 0,
    calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_combined_score (combined_score DESC)
);

-- ä¸Šä¼ æ—¥å¿—è¡¨
CREATE TABLE upload_logs (
    id SERIAL PRIMARY KEY,
    article_id BIGINT,
    upload_type VARCHAR(20), -- 'minio', 'elasticsearch', 'postgresql'
    status VARCHAR(20), -- 'success', 'failed', 'skipped'
    message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_upload_logs (article_id, upload_type)
);
```

### PostgreSQLå†™å…¥å™¨å®ç°

```python
class PGWriter:
    """PostgreSQLå†™å…¥å™¨"""
    
    def __init__(self, config: Dict):
        self.conn_string = config['connection_string']
        self.pool = None
        
    async def connect(self):
        """åˆ›å»ºè¿æ¥æ± """
        self.pool = await asyncpg.create_pool(self.conn_string)
        
    async def write_article(self, article_data: Dict) -> Tuple[bool, str]:
        """å†™å…¥å•ç¯‡æ–‡ç« """
        async with self.pool.acquire() as conn:
            try:
                async with conn.transaction():
                    # 1. æ£€æŸ¥å»é‡
                    exists = await conn.fetchval(
                        "SELECT 1 FROM articles WHERE id = $1 OR content_hash = $2",
                        article_data['id'], article_data['content_hash']
                    )
                    if exists:
                        return False, f"Article {article_data['id']} already exists"
                    
                    # 2. æ’å…¥ä¸»è¡¨
                    await conn.execute("""
                        INSERT INTO articles (id, title, subtitle, slug, type, post_date, 
                                            wordcount, canonical_url, content_hash)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                    """, article_data['id'], article_data['title'], 
                        article_data.get('subtitle'), article_data['slug'],
                        article_data.get('type', 'newsletter'), 
                        article_data['post_date'], article_data.get('wordcount', 0),
                        article_data['canonical_url'], article_data['content_hash'])
                    
                    # 3. æ’å…¥å†…å®¹è¡¨
                    await conn.execute("""
                        INSERT INTO article_content (article_id, content_markdown, description)
                        VALUES ($1, $2, $3)
                    """, article_data['id'], article_data['content'], 
                        article_data.get('description'))
                    
                    # 4. å¤„ç†æ ‡ç­¾
                    for tag in article_data.get('postTags', []):
                        # æ’å…¥æˆ–è·å–æ ‡ç­¾
                        tag_id = await conn.fetchval("""
                            INSERT INTO tags (name, slug) VALUES ($1, $2)
                            ON CONFLICT (slug) DO UPDATE SET name = EXCLUDED.name
                            RETURNING id
                        """, tag['name'], tag['slug'])
                        
                        # åˆ›å»ºå…³è”
                        await conn.execute("""
                            INSERT INTO article_tags (article_id, tag_id) VALUES ($1, $2)
                        """, article_data['id'], tag_id)
                    
                    # 5. æ’å…¥ååº”ç»Ÿè®¡
                    reactions = article_data.get('reactions', {})
                    await conn.execute("""
                        INSERT INTO article_reactions 
                        (article_id, reactions_json, total_count, heart_count)
                        VALUES ($1, $2, $3, $4)
                    """, article_data['id'], json.dumps(reactions),
                        sum(reactions.values()), reactions.get('â¤', 0))
                    
                    # 6. æ’å…¥å›¾ç‰‡
                    for idx, img in enumerate(article_data.get('local_images', [])):
                        await conn.execute("""
                            INSERT INTO article_images 
                            (article_id, image_url, local_path, file_hash, file_size, sequence)
                            VALUES ($1, $2, $3, $4, $5, $6)
                        """, article_data['id'], img['original_url'], 
                            img['local_path'], img['hash'], img['size'], idx)
                    
                    # 7. æ’å…¥è¯„åˆ†
                    scores = self._calculate_scores(article_data)
                    await conn.execute("""
                        INSERT INTO article_scores 
                        (article_id, popularity_score, freshness_score, quality_score, combined_score)
                        VALUES ($1, $2, $3, $4, $5)
                    """, article_data['id'], scores['popularity_score'],
                        scores['freshness_score'], scores['quality_score'],
                        scores['combined_score'])
                    
                    return True, f"Article {article_data['id']} saved successfully"
                    
            except Exception as e:
                return False, f"Failed to save article {article_data['id']}: {str(e)}"
    
    async def bulk_write(self, articles: List[Dict]) -> Dict:
        """æ‰¹é‡å†™å…¥æ–‡ç« """
        results = {
            'total': len(articles),
            'saved': 0,
            'skipped': 0,
            'failed': []
        }
        
        for article in articles:
            success, message = await self.write_article(article)
            if success:
                results['saved'] += 1
            elif 'already exists' in message:
                results['skipped'] += 1
            else:
                results['failed'].append({
                    'id': article['id'],
                    'error': message
                })
        
        return results
```

## 5. ç»Ÿä¸€ä¸Šä¼ ç®¡ç†å™¨ (UnifiedUploadManager)

åè°ƒæ‰€æœ‰ä¸Šä¼ æ“ä½œçš„ä¸»æ§åˆ¶å™¨ã€‚

```python
class UnifiedUploadManager:
    """ç»Ÿä¸€çš„ä¸Šä¼ ç®¡ç†å™¨"""
    
    def __init__(self, crawler_data_path: str):
        self.crawler_path = Path(crawler_data_path)
        self.preprocessor = DataPreprocessor(crawler_data_path)
        self.minio_uploader = None
        self.es_indexer = None
        self.pg_writer = None
        self.upload_stats = {}
        
    async def initialize(self, config: Dict):
        """åˆå§‹åŒ–æ‰€æœ‰ä¸Šä¼ å™¨"""
        if config.get('minio'):
            self.minio_uploader = MinIOUploader(config['minio'])
            
        if config.get('elasticsearch'):
            self.es_indexer = ESIndexer(config['elasticsearch'])
            
        if config.get('postgresql'):
            self.pg_writer = PGWriter(config['postgresql'])
            await self.pg_writer.connect()
    
    async def upload_all(self, 
                         to_minio: bool = True,
                         to_es: bool = True,
                         to_pg: bool = False,
                         batch_size: int = 50) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„ä¸Šä¼ æµç¨‹"""
        
        # 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        articles = self.preprocessor.load_data()
        print(f"ğŸ“Š åŠ è½½äº† {len(articles)} ç¯‡æ–‡ç« ")
        
        # 2. æ•°æ®éªŒè¯
        valid_articles = []
        for article in articles:
            is_valid, errors = self.preprocessor.validate_article(article)
            if is_valid:
                valid_articles.append(article)
            else:
                print(f"âš ï¸ æ–‡ç«  {article.get('id')} éªŒè¯å¤±è´¥: {errors}")
        
        print(f"âœ… {len(valid_articles)} ç¯‡æ–‡ç« é€šè¿‡éªŒè¯")
        
        results = {
            'total': len(articles),
            'valid': len(valid_articles),
            'minio': None,
            'elasticsearch': None,
            'postgresql': None
        }
        
        # 3. æ‰¹é‡å¤„ç†
        for i in range(0, len(valid_articles), batch_size):
            batch = valid_articles[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(valid_articles) + batch_size - 1) // batch_size
            
            print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ç¯‡æ–‡ç« )")
            
            # å‡†å¤‡æ•°æ®
            prepared_batch = [
                self.preprocessor.prepare_for_storage(article)
                for article in batch
            ]
            
            # å¹¶å‘ä¸Šä¼ åˆ°ä¸åŒå­˜å‚¨
            tasks = []
            
            if to_minio and self.minio_uploader:
                tasks.append(self._upload_to_minio(prepared_batch))
                
            if to_es and self.es_indexer:
                tasks.append(self._upload_to_es(prepared_batch))
                
            if to_pg and self.pg_writer:
                tasks.append(self._upload_to_pg(prepared_batch))
            
            if tasks:
                batch_results = await asyncio.gather(*tasks)
                # èšåˆç»“æœ
                self._aggregate_results(results, batch_results)
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if i + batch_size < len(valid_articles):
                await asyncio.sleep(0.5)
        
        # 4. æ‰“å°æœ€ç»ˆç»Ÿè®¡
        self._print_summary(results)
        
        return results
    
    async def _upload_to_minio(self, batch: List[Dict]) -> Dict:
        """ä¸Šä¼ æ‰¹æ¬¡åˆ°MinIO"""
        minio_data = [item['minio'] for item in batch]
        return await self.minio_uploader.bulk_upload(minio_data)
    
    async def _upload_to_es(self, batch: List[Dict]) -> Dict:
        """ä¸Šä¼ æ‰¹æ¬¡åˆ°Elasticsearch"""
        es_data = [item['elasticsearch'] for item in batch]
        return await self.es_indexer.bulk_index(es_data)
    
    async def _upload_to_pg(self, batch: List[Dict]) -> Dict:
        """ä¸Šä¼ æ‰¹æ¬¡åˆ°PostgreSQL"""
        pg_data = [item['postgresql'] for item in batch]
        return await self.pg_writer.bulk_write(pg_data)
    
    def _aggregate_results(self, total_results: Dict, batch_results: List[Dict]):
        """èšåˆæ‰¹æ¬¡ç»“æœ"""
        # å®ç°ç»“æœèšåˆé€»è¾‘
        pass
    
    def _print_summary(self, results: Dict):
        """æ‰“å°ä¸Šä¼ æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ä¸Šä¼ å®Œæˆç»Ÿè®¡")
        print("=" * 60)
        print(f"æ€»æ–‡ç« æ•°: {results['total']}")
        print(f"æœ‰æ•ˆæ–‡ç« : {results['valid']}")
        
        if results['minio']:
            print(f"\nMinIOä¸Šä¼ :")
            print(f"  âœ… æˆåŠŸ: {results['minio'].get('uploaded', 0)}")
            print(f"  âš ï¸ è·³è¿‡: {results['minio'].get('skipped', 0)}")
            print(f"  âŒ å¤±è´¥: {results['minio'].get('failed', 0)}")
        
        if results['elasticsearch']:
            print(f"\nElasticsearchç´¢å¼•:")
            print(f"  âœ… æˆåŠŸ: {results['elasticsearch'].get('indexed', 0)}")
            print(f"  âš ï¸ è·³è¿‡: {results['elasticsearch'].get('skipped', 0)}")
            print(f"  âŒ å¤±è´¥: {results['elasticsearch'].get('failed', 0)}")
        
        if results['postgresql']:
            print(f"\nPostgreSQLå†™å…¥:")
            print(f"  âœ… æˆåŠŸ: {results['postgresql'].get('saved', 0)}")
            print(f"  âš ï¸ è·³è¿‡: {results['postgresql'].get('skipped', 0)}")
            print(f"  âŒ å¤±è´¥: {results['postgresql'].get('failed', 0)}")
```

## ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´ä¸Šä¼ æµç¨‹

```python
import asyncio
from pathlib import Path

async def main():
    # é…ç½®
    config = {
        'minio': {
            'endpoint': 'http://localhost:9011',
            'bucket_name': 'newsletter-articles',
            'public_base_url': 'http://60.205.160.74:9000'
        },
        'elasticsearch': {
            'host': 'http://localhost:9200',
            'index': 'newsletter_articles'
        },
        'postgresql': {
            'connection_string': 'postgresql://user:pass@localhost/newsletter_db'
        }
    }
    
    # åˆ›å»ºä¸Šä¼ ç®¡ç†å™¨
    crawler_path = '/Users/ruanchuhao/Downloads/Codes/NewsLetters/çˆ¬è™«mlp/crawled_data'
    manager = UnifiedUploadManager(crawler_path)
    
    # åˆå§‹åŒ–
    await manager.initialize(config)
    
    # æ‰§è¡Œä¸Šä¼ ï¼ˆMinIO + ESï¼ŒPGæš‚ä¸å¯ç”¨ï¼‰
    results = await manager.upload_all(
        to_minio=True,
        to_es=True,
        to_pg=False,  # PostgreSQLæš‚æœªå®ç°
        batch_size=50
    )
    
    return results

# è¿è¡Œ
if __name__ == '__main__':
    asyncio.run(main())
```

### å•ç‹¬ä¸Šä¼ åˆ°ç‰¹å®šå­˜å‚¨

```python
# åªä¸Šä¼ åˆ°Elasticsearch
results = await manager.upload_all(
    to_minio=False,
    to_es=True,
    to_pg=False
)

# åªä¸Šä¼ åˆ°MinIO
results = await manager.upload_all(
    to_minio=True,
    to_es=False,
    to_pg=False
)
```

## é”™è¯¯å¤„ç†å’Œæ¢å¤

### 1. æ–­ç‚¹ç»­ä¼ æœºåˆ¶

```python
class UploadProgress:
    """ä¸Šä¼ è¿›åº¦è·Ÿè¸ª"""
    
    def __init__(self, progress_file: str = 'upload_progress.json'):
        self.progress_file = progress_file
        self.completed = {
            'minio': set(),
            'elasticsearch': set(),
            'postgresql': set()
        }
        self.load()
    
    def mark_completed(self, storage: str, article_id: int):
        """æ ‡è®°å®Œæˆ"""
        self.completed[storage].add(article_id)
        self.save()
    
    def is_completed(self, storage: str, article_id: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ"""
        return article_id in self.completed[storage]
    
    def save(self):
        """ä¿å­˜è¿›åº¦"""
        data = {
            storage: list(ids)
            for storage, ids in self.completed.items()
        }
        with open(self.progress_file, 'w') as f:
            json.dump(data, f)
    
    def load(self):
        """åŠ è½½è¿›åº¦"""
        if Path(self.progress_file).exists():
            with open(self.progress_file, 'r') as f:
                data = json.load(f)
                for storage, ids in data.items():
                    self.completed[storage] = set(ids)
```

### 2. é‡è¯•æœºåˆ¶

```python
async def retry_with_backoff(func, max_retries: int = 3):
    """æŒ‡æ•°é€€é¿é‡è¯•"""
    for attempt in range(max_retries):
        try:
            return await func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            wait_time = 2 ** attempt
            await asyncio.sleep(wait_time)
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ‰¹å¤„ç†å¤§å°
- MinIO: 20-30ç¯‡/æ‰¹ï¼ˆè€ƒè™‘å›¾ç‰‡ä¸Šä¼ ï¼‰
- Elasticsearch: 50-100ç¯‡/æ‰¹
- PostgreSQL: 100-200ç¯‡/æ‰¹

### 2. å¹¶å‘æ§åˆ¶
```python
# ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
minio_semaphore = asyncio.Semaphore(5)    # MinIOå¹¶å‘5
es_semaphore = asyncio.Semaphore(10)      # ESå¹¶å‘10
pg_semaphore = asyncio.Semaphore(20)      # PGå¹¶å‘20
```

### 3. å†…å­˜ä¼˜åŒ–
- æµå¼è¯»å–å¤§æ–‡ä»¶
- åˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º
- åŠæ—¶é‡Šæ”¾å·²å¤„ç†æ•°æ®

## ç›‘æ§å’Œæ—¥å¿—

### ä¸Šä¼ ç›‘æ§æŒ‡æ ‡
- ä¸Šä¼ é€Ÿåº¦ï¼ˆæ–‡ç« /ç§’ï¼‰
- æˆåŠŸç‡
- é”™è¯¯åˆ†å¸ƒ
- å­˜å‚¨ç©ºé—´ä½¿ç”¨

### æ—¥å¿—è®°å½•
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('upload.log'),
        logging.StreamHandler()
    ]
)
```

## æœªæ¥æ‰©å±•

1. **å¢é‡æ›´æ–°**: åªä¸Šä¼ æ–°æ–‡ç« 
2. **æ•°æ®åŒæ­¥**: ä¿æŒä¸‰ä¸ªå­˜å‚¨ç³»ç»Ÿæ•°æ®ä¸€è‡´
3. **æ•°æ®éªŒè¯**: ä¸Šä¼ åçš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
4. **å¯è§†åŒ–ç•Œé¢**: Webç•Œé¢ç®¡ç†ä¸Šä¼ ä»»åŠ¡
5. **å®šæ—¶ä»»åŠ¡**: è‡ªåŠ¨å®šæœŸä¸Šä¼ æ–°æ•°æ®

## æ€»ç»“

æœ¬æ¶æ„è®¾è®¡å®ç°äº†ä»çˆ¬è™«æ•°æ®åˆ°å¤šå­˜å‚¨ç³»ç»Ÿçš„ç»Ÿä¸€ä¸Šä¼ ç®¡é“ï¼š

1. **æ•°æ®é¢„å¤„ç†**: éªŒè¯ã€æ¸…æ´—ã€è½¬æ¢
2. **MinIOå­˜å‚¨**: æ–‡ä»¶å’Œå›¾ç‰‡çš„å¯¹è±¡å­˜å‚¨
3. **Elasticsearch**: å…¨æ–‡æœç´¢å’Œå»é‡
4. **PostgreSQL**: ç»“æ„åŒ–å­˜å‚¨å’Œå…³ç³»æŸ¥è¯¢

ç³»ç»Ÿå…·æœ‰è‰¯å¥½çš„æ‰©å±•æ€§ã€å®¹é”™æ€§å’Œæ€§èƒ½ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚çµæ´»é…ç½®å’Œä½¿ç”¨ã€‚

---

*æ›´æ–°æ—¶é—´: 2025-08-07*
*æ–‡æ¡£ç‰ˆæœ¬: 1.0*