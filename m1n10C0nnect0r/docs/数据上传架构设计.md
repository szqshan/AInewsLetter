# Newsletter数据上传架构设计文档

## 概述

本文档描述Newsletter文章数据的统一上传架构，实现从爬虫输出到多存储系统（MinIO、Elasticsearch、PostgreSQL）的数据流转。系统设计为可扩展的pipeline架构，支持批量处理、去重、错误恢复等功能。

## 数据源分析

### 输入数据格式
爬虫输出位于：`/Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp/crawled_data/`

```
crawled_data/
├── data/
│   └── processed_articles.json    # 主数据文件（180篇文章）
└── articles/
    └── {id}_{slug}/               # 每篇文章的独立目录
        ├── content.md             # Markdown内容
        ├── metadata.json          # 元数据
        └── images/                # 本地图片
```

### 核心数据结构
```json
{
  "id": 169333505,                    // 主键
  "title": "文章标题",
  "subtitle": "副标题",
  "content": "Markdown内容",          // 可能很大（1-10KB）
  "post_date": "2025-07-27T14:52:20.585Z",
  "type": "newsletter",
  "wordcount": 2403,
  "canonical_url": "https://...",
  "slug": "unique-slug",
  "reactions": {"❤": 35},
  "postTags": [{"id": "", "name": "", "slug": ""}],
  "local_images": [                   // 图片列表
    {
      "original_url": "https://...",
      "local_path": "articles/.../images/img_0.jpg",
      "hash": "sha256...",
      "size": 3209
    }
  ],
  "content_hash": "sha256...",        // 用于去重
  "processed_date": "2025-08-07T00:21:35"
}
```

## 上传架构设计

### 整体架构图

```
┌─────────────────┐
│   爬虫数据输入   │
│ processed_.json │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  数据预处理器    │
│ DataPreprocessor│
└────────┬────────┘
         │
    ┌────┴────┐
    │         │
    ▼         ▼
┌──────┐  ┌──────┐  ┌──────┐
│MinIO │  │ ES   │  │ PG   │
│上传器│  │索引器│  │写入器│
└──────┘  └──────┘  └──────┘
```

### 核心组件设计

## 1. 数据预处理器 (DataPreprocessor)

负责数据验证、清洗、转换和分发。

```python
class DataPreprocessor:
    """统一的数据预处理器"""
    
    def __init__(self, source_path: str):
        self.source_path = Path(source_path)
        self.articles = []
        self.validation_errors = []
        
    def load_data(self) -> List[Dict]:
        """加载爬虫数据"""
        # 1. 读取processed_articles.json
        # 2. 验证数据完整性
        # 3. 返回文章列表
        
    def validate_article(self, article: Dict) -> Tuple[bool, List[str]]:
        """验证单篇文章数据"""
        errors = []
        
        # 必需字段检查
        required_fields = ['id', 'title', 'content_hash']
        for field in required_fields:
            if field not in article:
                errors.append(f"Missing required field: {field}")
        
        # 数据类型检查
        if not isinstance(article.get('id'), int):
            errors.append("ID must be integer")
            
        # 内容长度检查
        if len(article.get('content', '')) == 0:
            errors.append("Content is empty")
            
        return len(errors) == 0, errors
    
    def prepare_for_storage(self, article: Dict) -> Dict:
        """为不同存储系统准备数据"""
        return {
            'minio': self._prepare_for_minio(article),
            'elasticsearch': self._prepare_for_es(article),
            'postgresql': self._prepare_for_pg(article)
        }
    
    def _prepare_for_minio(self, article: Dict) -> Dict:
        """MinIO数据准备：文件路径和内容"""
        return {
            'bucket': 'newsletter-articles',
            'object_path': f"articles/{article['id']}_{article['slug']}/",
            'files': {
                'content.md': article['content'],
                'metadata.json': json.dumps(article, ensure_ascii=False),
                'images': article.get('local_images', [])
            }
        }
    
    def _prepare_for_es(self, article: Dict) -> Dict:
        """Elasticsearch数据准备：索引文档"""
        # 计算评分
        scores = self._calculate_scores(article)
        
        return {
            'id': article['id'],
            'title': article['title'],
            'subtitle': article.get('subtitle'),
            'content': article['content'],
            'content_hash': article['content_hash'],
            'post_date': article['post_date'],
            'type': article.get('type', 'newsletter'),
            'wordcount': article.get('wordcount', 0),
            'reactions': article.get('reactions', {}),
            'reaction_count': sum(article.get('reactions', {}).values()),
            'tags': article.get('postTags', []),
            **scores
        }
    
    def _prepare_for_pg(self, article: Dict) -> Dict:
        """PostgreSQL数据准备：关系型数据"""
        return {
            'articles': {  # 主表
                'id': article['id'],
                'title': article['title'],
                'subtitle': article.get('subtitle'),
                'slug': article['slug'],
                'type': article.get('type'),
                'post_date': article['post_date'],
                'wordcount': article.get('wordcount'),
                'canonical_url': article['canonical_url'],
                'content_hash': article['content_hash'],
                'created_at': datetime.now()
            },
            'article_content': {  # 内容表（分离大字段）
                'article_id': article['id'],
                'content_markdown': article['content'],
                'description': article.get('description')
            },
            'article_tags': [  # 标签关联表
                {
                    'article_id': article['id'],
                    'tag_id': tag['id'],
                    'tag_name': tag['name'],
                    'tag_slug': tag['slug']
                }
                for tag in article.get('postTags', [])
            ],
            'article_reactions': {  # 反应统计表
                'article_id': article['id'],
                'reactions_json': json.dumps(article.get('reactions', {})),
                'total_count': sum(article.get('reactions', {}).values())
            },
            'article_images': [  # 图片表
                {
                    'article_id': article['id'],
                    'image_url': img['original_url'],
                    'local_path': img['local_path'],
                    'file_hash': img['hash'],
                    'file_size': img['size'],
                    'sequence': idx
                }
                for idx, img in enumerate(article.get('local_images', []))
            ]
        }
    
    def _calculate_scores(self, article: Dict) -> Dict:
        """计算文章评分"""
        # 复用现有的评分逻辑
        reaction_count = sum(article.get('reactions', {}).values())
        wordcount = article.get('wordcount', 0)
        
        # 流行度评分
        popularity = reaction_count * 0.3
        if 300 <= wordcount <= 2000:
            popularity += 10
        elif wordcount > 2000:
            popularity += 5
            
        # 新鲜度评分
        post_date = datetime.fromisoformat(article['post_date'].replace('Z', '+00:00'))
        days_old = (datetime.now() - post_date.replace(tzinfo=None)).days
        freshness = max(0, 100 - days_old * 0.5)
        
        # 质量评分
        quality = min(100, wordcount / 50 + reaction_count * 2)
        
        # 综合评分
        combined = popularity * 0.4 + freshness * 0.3 + quality * 0.3
        
        return {
            'popularity_score': popularity,
            'freshness_score': freshness,
            'quality_score': quality,
            'combined_score': combined
        }
```

## 2. MinIO上传器 (MinIOUploader)

负责将文章内容和图片上传到对象存储。

```python
class MinIOUploader:
    """MinIO上传器"""
    
    def __init__(self, config: Dict):
        self.endpoint = config['endpoint']
        self.bucket = config['bucket_name']
        self.public_url = config['public_base_url']
        
    async def upload_article(self, article_data: Dict) -> Dict:
        """上传单篇文章到MinIO"""
        results = {
            'article_id': article_data['id'],
            'uploaded_files': [],
            'public_urls': {},
            'errors': []
        }
        
        # 1. 创建文章目录结构
        base_path = f"articles/{article_data['id']}_{article_data['slug']}/"
        
        # 2. 上传Markdown内容
        content_url = await self._upload_file(
            f"{base_path}content.md",
            article_data['content'].encode('utf-8'),
            'text/markdown'
        )
        results['public_urls']['content'] = content_url
        
        # 3. 上传元数据
        metadata_url = await self._upload_file(
            f"{base_path}metadata.json",
            json.dumps(article_data, ensure_ascii=False).encode('utf-8'),
            'application/json'
        )
        results['public_urls']['metadata'] = metadata_url
        
        # 4. 上传图片
        for img in article_data.get('local_images', []):
            img_path = f"{base_path}images/{Path(img['local_path']).name}"
            # 读取本地图片文件
            local_file = Path(self.crawler_base) / img['local_path']
            if local_file.exists():
                img_url = await self._upload_file(
                    img_path,
                    local_file.read_bytes(),
                    'image/jpeg'
                )
                results['public_urls'][img['local_path']] = img_url
        
        return results
    
    async def bulk_upload(self, articles: List[Dict], max_concurrent: int = 5):
        """批量上传文章"""
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def upload_with_limit(article):
            async with semaphore:
                return await self.upload_article(article)
        
        tasks = [upload_with_limit(article) for article in articles]
        return await asyncio.gather(*tasks)
```

## 3. Elasticsearch索引器 (ESIndexer)

负责将文章索引到Elasticsearch，实现全文搜索和去重。

```python
class ESIndexer:
    """Elasticsearch索引器"""
    
    def __init__(self, config: Dict):
        self.client = AsyncElasticsearch([config['host']])
        self.index = 'newsletter_articles'
        
    async def index_article(self, article_data: Dict) -> Tuple[bool, str]:
        """索引单篇文章"""
        # 1. 检查去重
        if await self._is_duplicate(article_data['id'], article_data['content_hash']):
            return False, f"Article {article_data['id']} already exists"
        
        # 2. 索引文档
        try:
            await self.client.index(
                index=self.index,
                id=str(article_data['id']),
                body=article_data
            )
            return True, f"Article {article_data['id']} indexed successfully"
        except Exception as e:
            return False, f"Failed to index article {article_data['id']}: {str(e)}"
    
    async def _is_duplicate(self, article_id: int, content_hash: str) -> bool:
        """检查文章是否已存在"""
        query = {
            "query": {
                "bool": {
                    "should": [
                        {"term": {"id": article_id}},
                        {"term": {"content_hash": content_hash}}
                    ]
                }
            }
        }
        response = await self.client.search(index=self.index, body=query, size=1)
        return response['hits']['total']['value'] > 0
    
    async def bulk_index(self, articles: List[Dict]) -> Dict:
        """批量索引文章"""
        actions = []
        skipped = []
        
        for article in articles:
            # 检查去重
            if await self._is_duplicate(article['id'], article['content_hash']):
                skipped.append(article['id'])
                continue
                
            actions.append({
                "_index": self.index,
                "_id": str(article['id']),
                "_source": article
            })
        
        # 批量执行
        if actions:
            success, failed = await helpers.async_bulk(self.client, actions)
        
        return {
            'total': len(articles),
            'indexed': len(actions) - len(failed) if actions else 0,
            'skipped': len(skipped),
            'failed': failed if actions else []
        }
```

## 4. PostgreSQL写入器设计 (PGWriter)

### 数据库表结构设计

```sql
-- 主文章表
CREATE TABLE articles (
    id BIGINT PRIMARY KEY,
    title VARCHAR(500) NOT NULL,
    subtitle TEXT,
    slug VARCHAR(500) UNIQUE NOT NULL,
    type VARCHAR(50) DEFAULT 'newsletter',
    post_date TIMESTAMP WITH TIME ZONE,
    wordcount INTEGER DEFAULT 0,
    canonical_url TEXT,
    content_hash VARCHAR(64) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- 索引
    INDEX idx_post_date (post_date DESC),
    INDEX idx_type (type),
    INDEX idx_content_hash (content_hash)
);

-- 文章内容表（分离大字段）
CREATE TABLE article_content (
    article_id BIGINT PRIMARY KEY REFERENCES articles(id) ON DELETE CASCADE,
    content_markdown TEXT,
    content_html TEXT,
    description TEXT,
    search_vector tsvector, -- 全文搜索向量
    
    -- 全文搜索索引
    INDEX idx_search_vector USING GIN(search_vector)
);

-- 标签表
CREATE TABLE tags (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(100) NOT NULL,
    slug VARCHAR(100) UNIQUE NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 文章-标签关联表
CREATE TABLE article_tags (
    article_id BIGINT REFERENCES articles(id) ON DELETE CASCADE,
    tag_id UUID REFERENCES tags(id) ON DELETE CASCADE,
    PRIMARY KEY (article_id, tag_id)
);

-- 反应统计表
CREATE TABLE article_reactions (
    article_id BIGINT PRIMARY KEY REFERENCES articles(id) ON DELETE CASCADE,
    reactions_json JSONB DEFAULT '{}',
    total_count INTEGER DEFAULT 0,
    heart_count INTEGER DEFAULT 0,
    thumbs_up_count INTEGER DEFAULT 0,
    fire_count INTEGER DEFAULT 0,
    
    -- JSONB索引
    INDEX idx_reactions_json USING GIN(reactions_json)
);

-- 图片表
CREATE TABLE article_images (
    id SERIAL PRIMARY KEY,
    article_id BIGINT REFERENCES articles(id) ON DELETE CASCADE,
    image_url TEXT,
    minio_url TEXT,
    local_path TEXT,
    file_hash VARCHAR(64),
    file_size INTEGER,
    width INTEGER,
    height INTEGER,
    sequence INTEGER DEFAULT 0,
    is_cover BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_article_images (article_id),
    INDEX idx_file_hash (file_hash)
);

-- 文章评分表
CREATE TABLE article_scores (
    article_id BIGINT PRIMARY KEY REFERENCES articles(id) ON DELETE CASCADE,
    popularity_score FLOAT DEFAULT 0,
    freshness_score FLOAT DEFAULT 0,
    quality_score FLOAT DEFAULT 0,
    combined_score FLOAT DEFAULT 0,
    calculated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_combined_score (combined_score DESC)
);

-- 上传日志表
CREATE TABLE upload_logs (
    id SERIAL PRIMARY KEY,
    article_id BIGINT,
    upload_type VARCHAR(20), -- 'minio', 'elasticsearch', 'postgresql'
    status VARCHAR(20), -- 'success', 'failed', 'skipped'
    message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_upload_logs (article_id, upload_type)
);
```

### PostgreSQL写入器实现

```python
class PGWriter:
    """PostgreSQL写入器"""
    
    def __init__(self, config: Dict):
        self.conn_string = config['connection_string']
        self.pool = None
        
    async def connect(self):
        """创建连接池"""
        self.pool = await asyncpg.create_pool(self.conn_string)
        
    async def write_article(self, article_data: Dict) -> Tuple[bool, str]:
        """写入单篇文章"""
        async with self.pool.acquire() as conn:
            try:
                async with conn.transaction():
                    # 1. 检查去重
                    exists = await conn.fetchval(
                        "SELECT 1 FROM articles WHERE id = $1 OR content_hash = $2",
                        article_data['id'], article_data['content_hash']
                    )
                    if exists:
                        return False, f"Article {article_data['id']} already exists"
                    
                    # 2. 插入主表
                    await conn.execute("""
                        INSERT INTO articles (id, title, subtitle, slug, type, post_date, 
                                            wordcount, canonical_url, content_hash)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                    """, article_data['id'], article_data['title'], 
                        article_data.get('subtitle'), article_data['slug'],
                        article_data.get('type', 'newsletter'), 
                        article_data['post_date'], article_data.get('wordcount', 0),
                        article_data['canonical_url'], article_data['content_hash'])
                    
                    # 3. 插入内容表
                    await conn.execute("""
                        INSERT INTO article_content (article_id, content_markdown, description)
                        VALUES ($1, $2, $3)
                    """, article_data['id'], article_data['content'], 
                        article_data.get('description'))
                    
                    # 4. 处理标签
                    for tag in article_data.get('postTags', []):
                        # 插入或获取标签
                        tag_id = await conn.fetchval("""
                            INSERT INTO tags (name, slug) VALUES ($1, $2)
                            ON CONFLICT (slug) DO UPDATE SET name = EXCLUDED.name
                            RETURNING id
                        """, tag['name'], tag['slug'])
                        
                        # 创建关联
                        await conn.execute("""
                            INSERT INTO article_tags (article_id, tag_id) VALUES ($1, $2)
                        """, article_data['id'], tag_id)
                    
                    # 5. 插入反应统计
                    reactions = article_data.get('reactions', {})
                    await conn.execute("""
                        INSERT INTO article_reactions 
                        (article_id, reactions_json, total_count, heart_count)
                        VALUES ($1, $2, $3, $4)
                    """, article_data['id'], json.dumps(reactions),
                        sum(reactions.values()), reactions.get('❤', 0))
                    
                    # 6. 插入图片
                    for idx, img in enumerate(article_data.get('local_images', [])):
                        await conn.execute("""
                            INSERT INTO article_images 
                            (article_id, image_url, local_path, file_hash, file_size, sequence)
                            VALUES ($1, $2, $3, $4, $5, $6)
                        """, article_data['id'], img['original_url'], 
                            img['local_path'], img['hash'], img['size'], idx)
                    
                    # 7. 插入评分
                    scores = self._calculate_scores(article_data)
                    await conn.execute("""
                        INSERT INTO article_scores 
                        (article_id, popularity_score, freshness_score, quality_score, combined_score)
                        VALUES ($1, $2, $3, $4, $5)
                    """, article_data['id'], scores['popularity_score'],
                        scores['freshness_score'], scores['quality_score'],
                        scores['combined_score'])
                    
                    return True, f"Article {article_data['id']} saved successfully"
                    
            except Exception as e:
                return False, f"Failed to save article {article_data['id']}: {str(e)}"
    
    async def bulk_write(self, articles: List[Dict]) -> Dict:
        """批量写入文章"""
        results = {
            'total': len(articles),
            'saved': 0,
            'skipped': 0,
            'failed': []
        }
        
        for article in articles:
            success, message = await self.write_article(article)
            if success:
                results['saved'] += 1
            elif 'already exists' in message:
                results['skipped'] += 1
            else:
                results['failed'].append({
                    'id': article['id'],
                    'error': message
                })
        
        return results
```

## 5. 统一上传管理器 (UnifiedUploadManager)

协调所有上传操作的主控制器。

```python
class UnifiedUploadManager:
    """统一的上传管理器"""
    
    def __init__(self, crawler_data_path: str):
        self.crawler_path = Path(crawler_data_path)
        self.preprocessor = DataPreprocessor(crawler_data_path)
        self.minio_uploader = None
        self.es_indexer = None
        self.pg_writer = None
        self.upload_stats = {}
        
    async def initialize(self, config: Dict):
        """初始化所有上传器"""
        if config.get('minio'):
            self.minio_uploader = MinIOUploader(config['minio'])
            
        if config.get('elasticsearch'):
            self.es_indexer = ESIndexer(config['elasticsearch'])
            
        if config.get('postgresql'):
            self.pg_writer = PGWriter(config['postgresql'])
            await self.pg_writer.connect()
    
    async def upload_all(self, 
                         to_minio: bool = True,
                         to_es: bool = True,
                         to_pg: bool = False,
                         batch_size: int = 50) -> Dict:
        """执行完整的上传流程"""
        
        # 1. 加载和预处理数据
        articles = self.preprocessor.load_data()
        print(f"📊 加载了 {len(articles)} 篇文章")
        
        # 2. 数据验证
        valid_articles = []
        for article in articles:
            is_valid, errors = self.preprocessor.validate_article(article)
            if is_valid:
                valid_articles.append(article)
            else:
                print(f"⚠️ 文章 {article.get('id')} 验证失败: {errors}")
        
        print(f"✅ {len(valid_articles)} 篇文章通过验证")
        
        results = {
            'total': len(articles),
            'valid': len(valid_articles),
            'minio': None,
            'elasticsearch': None,
            'postgresql': None
        }
        
        # 3. 批量处理
        for i in range(0, len(valid_articles), batch_size):
            batch = valid_articles[i:i+batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(valid_articles) + batch_size - 1) // batch_size
            
            print(f"\n📦 处理批次 {batch_num}/{total_batches} ({len(batch)} 篇文章)")
            
            # 准备数据
            prepared_batch = [
                self.preprocessor.prepare_for_storage(article)
                for article in batch
            ]
            
            # 并发上传到不同存储
            tasks = []
            
            if to_minio and self.minio_uploader:
                tasks.append(self._upload_to_minio(prepared_batch))
                
            if to_es and self.es_indexer:
                tasks.append(self._upload_to_es(prepared_batch))
                
            if to_pg and self.pg_writer:
                tasks.append(self._upload_to_pg(prepared_batch))
            
            if tasks:
                batch_results = await asyncio.gather(*tasks)
                # 聚合结果
                self._aggregate_results(results, batch_results)
            
            # 批次间延迟
            if i + batch_size < len(valid_articles):
                await asyncio.sleep(0.5)
        
        # 4. 打印最终统计
        self._print_summary(results)
        
        return results
    
    async def _upload_to_minio(self, batch: List[Dict]) -> Dict:
        """上传批次到MinIO"""
        minio_data = [item['minio'] for item in batch]
        return await self.minio_uploader.bulk_upload(minio_data)
    
    async def _upload_to_es(self, batch: List[Dict]) -> Dict:
        """上传批次到Elasticsearch"""
        es_data = [item['elasticsearch'] for item in batch]
        return await self.es_indexer.bulk_index(es_data)
    
    async def _upload_to_pg(self, batch: List[Dict]) -> Dict:
        """上传批次到PostgreSQL"""
        pg_data = [item['postgresql'] for item in batch]
        return await self.pg_writer.bulk_write(pg_data)
    
    def _aggregate_results(self, total_results: Dict, batch_results: List[Dict]):
        """聚合批次结果"""
        # 实现结果聚合逻辑
        pass
    
    def _print_summary(self, results: Dict):
        """打印上传摘要"""
        print("\n" + "=" * 60)
        print("📊 上传完成统计")
        print("=" * 60)
        print(f"总文章数: {results['total']}")
        print(f"有效文章: {results['valid']}")
        
        if results['minio']:
            print(f"\nMinIO上传:")
            print(f"  ✅ 成功: {results['minio'].get('uploaded', 0)}")
            print(f"  ⚠️ 跳过: {results['minio'].get('skipped', 0)}")
            print(f"  ❌ 失败: {results['minio'].get('failed', 0)}")
        
        if results['elasticsearch']:
            print(f"\nElasticsearch索引:")
            print(f"  ✅ 成功: {results['elasticsearch'].get('indexed', 0)}")
            print(f"  ⚠️ 跳过: {results['elasticsearch'].get('skipped', 0)}")
            print(f"  ❌ 失败: {results['elasticsearch'].get('failed', 0)}")
        
        if results['postgresql']:
            print(f"\nPostgreSQL写入:")
            print(f"  ✅ 成功: {results['postgresql'].get('saved', 0)}")
            print(f"  ⚠️ 跳过: {results['postgresql'].get('skipped', 0)}")
            print(f"  ❌ 失败: {results['postgresql'].get('failed', 0)}")
```

## 使用示例

### 完整上传流程

```python
import asyncio
from pathlib import Path

async def main():
    # 配置
    config = {
        'minio': {
            'endpoint': 'http://localhost:9011',
            'bucket_name': 'newsletter-articles',
            'public_base_url': 'http://60.205.160.74:9000'
        },
        'elasticsearch': {
            'host': 'http://localhost:9200',
            'index': 'newsletter_articles'
        },
        'postgresql': {
            'connection_string': 'postgresql://user:pass@localhost/newsletter_db'
        }
    }
    
    # 创建上传管理器
    crawler_path = '/Users/ruanchuhao/Downloads/Codes/NewsLetters/爬虫mlp/crawled_data'
    manager = UnifiedUploadManager(crawler_path)
    
    # 初始化
    await manager.initialize(config)
    
    # 执行上传（MinIO + ES，PG暂不启用）
    results = await manager.upload_all(
        to_minio=True,
        to_es=True,
        to_pg=False,  # PostgreSQL暂未实现
        batch_size=50
    )
    
    return results

# 运行
if __name__ == '__main__':
    asyncio.run(main())
```

### 单独上传到特定存储

```python
# 只上传到Elasticsearch
results = await manager.upload_all(
    to_minio=False,
    to_es=True,
    to_pg=False
)

# 只上传到MinIO
results = await manager.upload_all(
    to_minio=True,
    to_es=False,
    to_pg=False
)
```

## 错误处理和恢复

### 1. 断点续传机制

```python
class UploadProgress:
    """上传进度跟踪"""
    
    def __init__(self, progress_file: str = 'upload_progress.json'):
        self.progress_file = progress_file
        self.completed = {
            'minio': set(),
            'elasticsearch': set(),
            'postgresql': set()
        }
        self.load()
    
    def mark_completed(self, storage: str, article_id: int):
        """标记完成"""
        self.completed[storage].add(article_id)
        self.save()
    
    def is_completed(self, storage: str, article_id: int) -> bool:
        """检查是否已完成"""
        return article_id in self.completed[storage]
    
    def save(self):
        """保存进度"""
        data = {
            storage: list(ids)
            for storage, ids in self.completed.items()
        }
        with open(self.progress_file, 'w') as f:
            json.dump(data, f)
    
    def load(self):
        """加载进度"""
        if Path(self.progress_file).exists():
            with open(self.progress_file, 'r') as f:
                data = json.load(f)
                for storage, ids in data.items():
                    self.completed[storage] = set(ids)
```

### 2. 重试机制

```python
async def retry_with_backoff(func, max_retries: int = 3):
    """指数退避重试"""
    for attempt in range(max_retries):
        try:
            return await func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            wait_time = 2 ** attempt
            await asyncio.sleep(wait_time)
```

## 性能优化建议

### 1. 批处理大小
- MinIO: 20-30篇/批（考虑图片上传）
- Elasticsearch: 50-100篇/批
- PostgreSQL: 100-200篇/批

### 2. 并发控制
```python
# 使用信号量控制并发
minio_semaphore = asyncio.Semaphore(5)    # MinIO并发5
es_semaphore = asyncio.Semaphore(10)      # ES并发10
pg_semaphore = asyncio.Semaphore(20)      # PG并发20
```

### 3. 内存优化
- 流式读取大文件
- 分批处理避免内存溢出
- 及时释放已处理数据

## 监控和日志

### 上传监控指标
- 上传速度（文章/秒）
- 成功率
- 错误分布
- 存储空间使用

### 日志记录
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('upload.log'),
        logging.StreamHandler()
    ]
)
```

## 未来扩展

1. **增量更新**: 只上传新文章
2. **数据同步**: 保持三个存储系统数据一致
3. **数据验证**: 上传后的数据完整性检查
4. **可视化界面**: Web界面管理上传任务
5. **定时任务**: 自动定期上传新数据

## 总结

本架构设计实现了从爬虫数据到多存储系统的统一上传管道：

1. **数据预处理**: 验证、清洗、转换
2. **MinIO存储**: 文件和图片的对象存储
3. **Elasticsearch**: 全文搜索和去重
4. **PostgreSQL**: 结构化存储和关系查询

系统具有良好的扩展性、容错性和性能，可以根据实际需求灵活配置和使用。

---

*更新时间: 2025-08-07*
*文档版本: 1.0*